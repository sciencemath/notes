Combining Random Variables:

Distributions of Two Discrete Random Variables
Distributions of Two Continuous Random Variables
Expectation for Joint Distributions
Covariance of Random Variables
Normally Distributed Random Variables

NOTE:
sigma notation is similar to integral notation as I am not using anything like MathML example: Σ^3_j=1(j + 1) in this case 3 is above the sigma and ^, and _ separates what would be on the top and bottom respectively and next to sigma is the iterator (j + 1)

NOTE:
When integrating we sometimes have to change an upper limit from a ∞ to a variable like b because I want to evalutate the integral step by step, to show the process before substituing back into the limit (a placeholder before arriving at the final form)

NOTE:
taking a CDF for Y we need to integrate with repect to x and keep y fixed, and vise versa.

--------------------------------------------

Sigma notation
Σ^n_i=1(ai)
refers to the sum of all terms in the sequence ai where index i ranges from 1 to n
Σ^n_i=1 ai = a1 + a2 + ... + an
sigma notion can also represents sums whose terms depend on two indices that is double sums
Σ^m_i=1Σ^n_j=1(aij)
where i ranges from 1 to m and j ranges from 1 to n, "sum of a sum"
first we evaluate the inner sum by fixing the index of the outer sum (i) and incrementing only the inner index (j)
next we evaluate out sum by incrementing the outer sum index
Σ^2_i=1Σ^4_j=1(i + j)
= Σ^2_i=1[Σ^4_j=1(i + j)]
= Σ^2_i=1[(i + 1) + (i + 2) + (i + 3) + (i + 4)]
= Σ^2_i=1(4i + 10)]
= 4(1) + 10 + 4(2) + 10
= 4 + 10 + 8 + 10
= 32

example:
Σ^3_j=1Σ^2_i=1(i + j)
= Σ^3_j=1[Σ^2_i=1(i + j)]
= Σ^3_j=1[(1 + j) + (2 + j)]
= Σ^3_j=1(3 + 2j)
outer
Σ^3_j=1(3 + 2j) = (3 + 2 * 1) + (3 + 2 * 2) + (3 + 2 * 3)
= 5 + 7 + 9
= 21

example:
Σ^3_i=1Σ^3_j=2(i^2 + j)
= Σ^3_i=1[Σ^3_j=2(i^2 + j)]
= Σ^3_i=1[(i^2 + 2) + (i^2 + 3)]
= Σ^3_i=1(2i^2 + 5)
outer
Σ^3_i=1(2i^2 + 5) = (2(1)^2 + 5) + (2(2)^2 + 5) + (2(3)^2 + 5)
= 7 + 13 + 23
= 43

example:
Σ^2_j=1Σ^3_i=1(ij)
= Σ^2_j=1[Σ^3_i=1(ij)]
= Σ^2_j=1[(1j) + (2j) + (3j)]
= Σ^2_j=1(6j)
outer
Σ^2_j=1(6j) = (6(1)) + (6(2))
= 6 + 12
= 18

The sum and constant multiple rules
The constant multiple rule
we can factor out a constant multiple from a double summation
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
The sum rule
we can distribute a double summation over a sum of terms
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
The double sum of units
for single summations we have
Σ^n_j=1(1) = 1 + 1 + ... + 1 = n (n times)
double summations
Σ^m_i=1Σ^n_j=1(1) = mn

Applying the sum and constant multiple rules
find the value of the double sum
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
given that
Σ^5_i=1Σ^8_j=1(aij) = 8
Σ^5_i=1Σ^8_j=1(bij) = 15
the sum and contant multiple rules for double summations
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
applying the sum rule to the given double summation
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= Σ^5_i=1Σ^8_j=1(1) + Σ^5_i=1Σ^8_j=1(2aij) - Σ^5_i=1Σ^8_j=1(bij)
= (Σ^5_i=1Σ^8_j=1(1)) + 2Σ^5_i=1Σ^8_j=1(aij) - Σ^5_i=1Σ^8_j=1(bij)
from the info given
= (Σ^5_i=1Σ^8_j=1(1)) + 2(8) - 15
= (Σ^5_i=1Σ^8_j=1(1)) + 1
using
Σ^m_i=1Σ^n_j=1(1) = mn
(Σ^5_i=1Σ^8_j=1(1)) + 1 = 5 * 8 + 1
= 41
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= 41

example:
find the value of the double sum
Σ^50_i=1Σ^100_j=1(4(aij + bij))
given
Σ^50_i=1Σ^100_j=1(aij) = 40
Σ^50_i=1Σ^100_j=1(bij) = 60
= 4Σ^50_i=1Σ^100_j=1(aij) + 4Σ^50_i=1Σ^100_j=1(bij)
= 4 * 40 + 4 * 60
= 160 + 240
= 400

example:
Σ^5_i=1Σ^10_j=1(1 - aij - bij)
Σ^5_i=1Σ^10_j=1(aij) = 10
Σ^5_i=1Σ^10_j=1(bij) = 20
= (Σ^5_i=1Σ^10_j=1(1)) - 30
= 5 * 10 - 30
= 50 - 30
= 20

The product and swap rules
the product rule
Σ^m_i=1Σ^n_j=1(aibj) = Σ^m_i=1(ai)Σ^n_j=1(bj)
the swap rule
Σ^m_i=1Σ^n_j=1(aij) = Σ^n_j=1Σ^m_i=1(aij)

Appling the product rule and swap rules
find the value of the double sum
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
given
Σ^15_i=1(ai) = 40
Σ^18_j=1(bj) = -8
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
= Σ^15_i=1(ai - 2)Σ^18_j=1(bj + 1)
= (Σ^15_i=1(ai) - Σ^15_i=1(2)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (Σ^15_i=1(ai) - 2Σ^15_i=1(1)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (40 - 2 * 15) * (-8 + 18)
= 10 * 10
= 100

example:
Σ^10_i=1Σ^20_j=1(ai - 1)(bj + 1)
given
Σ^10_i=1(ai) = 12
Σ^20_j=1(bj) = 5
= (Σ^10_i=1(ai - 1)Σ^20_j=1(bj + 1))
= (Σ^10_i=1(ai) - Σ^10_i=1(1)) * (Σ^20_j=1(bj) + Σ^20_j=1(1))
= (12 - 10) * (5 + 20)
= (2) * (25)
= 50

example:
Σ^30_i=1Σ^30_j=1(2ai(bj - 2))
given
Σ^30_i=1(ai) = 42
Σ^30_j=1(bj) = 70
= 2Σ^30_i=1(ai)Σ^30_j=1(bj - 2)
= (2Σ^30_i=1(ai)) * (Σ^30_j=1(bj) - 2Σ^30_j=1(1))
= (2 * 42) * (70 - 2 * 30)
= (84) * (70 - 60)
= (84) * (10)
= 840

===================================================

We often want to know if there is a relationship between two random variables. For this reason we wish to formulate the idea of a joint probability distribution
X represents the number of heads obtained when a fair coin is flipped. The support of X denoted S_X consists of all possible values of X is given by S_X = {0,1}
Y represents the result of rolling a fair tetrahedral die then the support of Y denoted S_Y is S_Y = {1,2,3,4}
The joint support of X and Y denoted S consists of all possible pairs (x, y) such that x is a possible outcome for X and y is a possible outcome for Y
S = S_X x S_Y = {
	(0,1), (0,2), (0,3), (0,4)
	(1,1), (1,2), (1,3), (1,4)
}
S_X x S_Y is the cartesian product of S_X and S_Y
flipping of the coin and rolling of the die are independent then for any x ∈ X and any y ∈ Y
P(X = x and Y = y) = 1/2 * 1/4 = 1/8
the joint probability mass function of X and Y denoted f(x,y) can be represented by the table
f(x,y)|y=1|y=2|y=3|y=4|
x = 0 |1/8|1/8|1/8|1/8|
x = 1 |1/8|1/8|1/8|1/8|
similar to the case of single random variables
f(x,y) = P(X = x and Y = y)
P(X = x,Y = y) as short hand

Bivariate and multivariate distributions
X and Y are discrete random variables with supports S_X and S_Y
f(x,y) = P(X = x,Y = y)
to be a valid joint probability mass function with joint support S = S_X x S_Y it must satisfy following conditions
0 <= f(x,y) <= 1 for all (x,y) ∈ S
Σ_(x,y)∈S f(x,y) = 1
P((X,Y) ∈ A) = Σ_(x,y)∈A f(x,y) where A is a subset of S
some intuition
the consition 0 <= f(x,y) <= 1 for all (x,y) ∈ S states that any possible outcome for (X,Y) must have a probability between 0 and 1
The Σ_(x,y)∈S f(x,y) = 1 states that the sum of all probabilities over all possible values of X and Y must add up to 1
The third condition states that we compute the probability of an event A by adding up the probabilites associated with A
The joint probability table
f(x,y) | Y = y1 | Y = y2 | ... | Y = yn |
x = x1 |f(x1,y1)|f(x1,y2)| ... |f(x1,yn)|
x = x2 |f(x2,y1)|f(x2,y2)| ... |f(x2,yn)|
 ...   |   ...  |   ...  | ... |   ...  |
x = xk |f(xk,y1)|f(xk,y2)| ... |f(xk,yn)|
two random variables the joint distribution is sometimes called a bivariate distribution, any number of random variables is a multivariate distribution

finding the joint distribution of two discrete random variables
   |Y=0|  Y=1 |
X=0|a/2|   a  |
X=1| a |5/2(a)|
the joint probability mass function f(x,y) for the discrete random variables X and Y is given above
The support of X is S_X = {0,1} the support of Y is S_Y = {0,1} and therefore the joint support S is
S = S_X x S_Y = {(0,0),(0,1),(1,0),(1,1)}
f(0,0) + f(0,1) + f(0,1) + f(1,1) = 1
a/2 + a + a + 5/2(a) = 1
5a = 1
a = 1/5

example:
   |Y=1|Y=2|
X=0| 2a| 3a|
X=1| 5a| 2a|
S = S_X x S_Y = {(0,1),(0,2),(1,1),(1,2)}
2a + 3a + 5a + 2a = 1
12a = 1
a = 1/12

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
which of the following conditions are true?
1. 0 <= f(x,y) <= 1 for all (x,y) in S
2. Σ_(x,y)∈S f(x,y) = 1
3. f(x,y) is a valid joint probability mass function
all 3 are valid, and since 1 and 2 we've seen makes 3 true.

Calculating a joint probability from a table
   |Y=1 |Y=3 |
X=2|0.05|0.25|
X=4|0.2 |0.05|
X=6|0.3 |0.15|
compute P((X,Y) ∈ {(2,3),(6,1)})
= P((X,Y) = (2,3)) + P((X,Y) = (6,1))
= f(2,3) + f(6,1)
= 0.25 + 0.3
= 0.55

example:
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
calculate P(X=0,Y=1)
= f(0,1) = 1/2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P((X,Y) ∈ {(1,4),(1,6),(2,6)})
= 0.25 + 0.25 + 0.2
= 0.7

Calculating a joint probability containing inequalities from a table
   |Y=0 |Y=2 |Y=4
X=0|0.15|0.1 |0.1
X=1|0.05|0.1 |0.05
X=2|0.05|0.25|0.15
Compute P(X*Y >= 4)
= P((X,Y) ∈ {(1,4),(2,2),(2,4)})
= 0.05 + 0.25 + 0.15
= 0.45

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X = 2 and Y <= 4)
= P((X,Y) ∈ {(2,2),(2,4)})
= 0.05 + 0.15
= 0.2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X < 2 or Y = 6)
= P((X,Y) ∈ {(1,2),(1,4),(1,6),(2,6)})
= 0.1 + 0.25 + 0.25 + 0.2
= 0.8

===================================================

The joint PMF f(x,y) tells us all possible events (X,Y) and the probabilities associated with each event, however we wish to determine the PMF for X only how can this be deduced from the joint PMF?
consider the joint PMF
   |Y=1 |Y=2 |
X=1|0.05|0.15|
X=2|0.7 |0.1 |
The support of X is S_X = {1,2} lets now deduce the probabilites associated with each value of X in S_X
According to the law of total probability we can deduce P(X = 1) by summing the values in the first row of the table
P(X = 1) = P(X = 1,Y = 1) + P(X = 1,Y = 2)
= 0.05 + 0.15
= 0.2
summing the values in second row in table
P(X = 2) = P(X = 2,Y = 1) + P(X = 2,Y = 2)
= 0.7 + 0.1
= 0.8
PMF for X
x     | 1 | 2 |
f_X(x)|0.2|0.8|
f_X(x) was deduced from the joint probability mass function, we call f_X(x) the marginal mass function for X

Deducing the marginal mass function for Y
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
the marginal mass function in the right margin of the table
P(X = 1) = f_X(1) = 0.2
P(X = 2) = f_X(2) = 0.8
similary we can compute the marginal mass function of Y denoted f_Y(y) by summing the columns adding these totals to our table
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
f_Y|0.75|0.25| 1 |
P(Y = 1) = f_Y(1) = 0.75
P(Y = 2) = f_Y(2) = 0.25
marginal mass function f_Y(y) of Y
x     | 1  |  2 |
f_Y(y)|0.75|0.25|

A formal definition of the margin mass function
The maginal mass function of X denoted f_X(x)
f_X(x) = P(X = x) = Σ_y∈SY f(x,y)
The marginal mass function of Y denoted f_Y(y)
f_Y(y) = P(Y = x) = Σ_x∈SX f(x,y)
To compute P(X = x) for some particular x we sum all possible values of Y in the row corresponding to X = x
To compute P(Y = y) for some particular y we sum all possible values of X in the column corresponding to Y = y
Marginal mass functions are sometimes called marginal probability mass functions, marginal PMFs, or marginal distributions

Finding a marginal probability
   |Y=0 |Y=1 |
X=0|1/6 |5/12|
X=1|1/4 |1/6 |
P(X = x) = f_X(x) = Σ_y f(x,y)
P(Y = y) = f_Y(y) = Σ_x f(x,y)
The marginal distribution for X corresponds to the row totals and the marginal distribution for Y corresponds to the column totals.
   |Y=0 |Y=1 |f_X |
X=0|1/6 |5/12|7/12|
X=1|1/4 |1/6 |5/12|
f_Y|5/12|7/12|  1 |
P(X = 0) = f_X(0)
= f(0,0) + f(0,1)
= 1/6 + 5/12
= 7/12

example:
find P(X = 0)
   |Y=1 |Y=2 |
X=0|0.4 |0.2 |
X=1|0.15|0.25|
= 0.4 + 0.2
= 0.6

example:
find f_Y(0)
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
= 1/4 + 1/8
= 3/8

(rest of examples are simple table lookups and summations no new knowledge to note)

===================================================

Determining whether two random variables are independent
   |Y=0 |Y=1 |
X=0|0.24|0.06|
X=1|0.56|0.14|
marginal mass functions
x     |0  |1  |
f_X(x)|0.3|0.7|
---------------
y     |0  |1  |
f_Y(y)|0.8|0.2|
which of the following statements are true?
1. P(X = 0,Y = 0) = P(X = 0)*P(Y = 0)
2. P(X = x,Y = y) = P(X = x)*P(Y = y) for all possible x and y
3. X and Y are independent
Two discrete random variables X and Y with joint probability mass function f(x,y) are independent iff
f(x,y) = f_X(x)*f_Y(y)
Statements 1,2, and 3 is true. 1 is true because
P(X = 0,Y = 0) = f(0,0) = 0.24
P(X = 0) * P(Y = 0) = f_X(0) * f_Y(0) = 0.3 * 0.8 = 0.24
2 is true because
f(1,0) = 0.56 = 0.7 * 0.8 = f_X(1) * f_Y(0)
f(0,1) = 0.06 = 0.3 * 0.2 = f_X(0) * f_Y(1)
f(1,1) = 0.14 = 0.7 * 0.2 = f_X(1) * f_Y(1)
P(X = x,Y = y) = P(X = x) * P(Y = y) for all possible x and y
statement 3 is true because of 1 and 2 is true X and Y must be independent

example:
   |Y=0 |Y=1|
X=0|0.05|0.2|
X=1|0.15|0.6|
computing the marginal probability mass functions for X and Y
   |Y=0 |Y=1| f_X|
X=0|0.05|0.2|0.25|
X=1|0.15|0.6|0.75|
f_Y| 0.2|0.8|  1 |
check if these statements are true
this first statement is true:
P(X = 1,Y = 1) = P(X = 1) * P(Y = 1)
0.75 * 0.8 = 0.6
this is also true
P(X = x,Y = y) = P(X = x) * P(Y = y)
making X and Y independent

example:
   |Y=0 | Y=1|
X=0|0.05|0.15|
X=1|0.25|0.55|
computing the marginal probability mass functions for X and Y
   |Y=0 | Y=1| f_X|
X=0|0.05|0.15| 0.2|
X=1|0.25|0.55| 0.8|
f_Y| 0.3| 0.7|  1 |
check if these statements are true
this first statement is true:
P(X = 0,Y = 1) = P(X = 0) * P(Y = 1)
0.2 * 0.7 = 0.14 != 0.15
so this would also be false
P(X = x,Y = y) = P(X = x) * P(Y = y)
and X and Y are not independent

example:
   |Y=2| Y=4|
X=1|0.2| 0.2|
X=3|0.1|0.25|
X=5|0.2|0.05|
computing the marginal probability mass functions for X and Y
   |Y=2| Y=4| f_X|
X=1|0.2| 0.2| 0.4|
X=3|0.1|0.25|0.35|
X=5|0.2|0.05|0.25|
f_Y|0.5| 0.5|  1 |
check if these statements are true
this is true:
P(X = 1,Y = 2) = P(X = 1) * P(Y = 2)
0.4 * 0.5 = 0.2
this is false (only have to check one instance)!
P(X = x,Y = y) = P(X = x) * P(Y = y)
0.5 * 0.35 = 0.175 != 0.1
making X and Y dependent

example:
   |Y=1| Y=3| f_X|
X=2| * |0.27|  * |
X=4| * |0.63|  ? |
f_Y|0.1| 0.9|  1 |
find P(X = 4)
normally to cross reference we * by totals, we don't
have all totals but we can find it by the info given to us
by dividing total with the cross reference cell
0.63/0.9 = 0.7
P(X = 4) = 0.7

===================================================

Conditional distributions for discrete random variables
the multiplication law for conditional probability for two events A and B
P(A|B) = P(A ∩ B)/P(B)
suppose X and Y are discrete random variables the conditional probability X = x given that Y = y follows immediately from the multiplication law
P(X = x|Y = y) = P(X = x,Y - y)/P(Y = y) = f(x,y)/f_Y(y)
F(x,y) is the joint PMF of X and Y, f_Y(y) is the marginal mass function
To avoid getting a zero in the denominator the formula assumes that P(Y = y) = f_Y(y) != 0
In the case where P(Y = y) = 0 the corresponding probability for X does not exist it makes no sense to ask about the event of X = x occuring given that Y = y has occurred if the event Y = y is impossible
similarly the conditional probability Y = y given X = x
P(Y = y|X = x) = P(X = x,Y = y)/P(X = x) = f(x,y)/f_X(x)
where f_X(x) is the marginal mass function of X this formula assumes that P(X = x) != 0

Computing a conditional probability
   | Y=0| Y=1|
X=0| 0.5|0.05|
X=1|0.15| 0.3|
compute P(Y = 0|X = 1) given that the discrete random variables X and Y have the joint probability function f(x,y)
P(Y = 0|X = 1) = P(X = 1,Y = 0)/P(X = 1) = f(1,0)/f_X(1)
f_X(1) = f(1,0) + f(1,1)
= 0.15 + 0.3
= 0.45
substituting
f_X(1) = 0.45 and f(1,0) = 0.15 in our formula
P(Y = 0|X = 1) = f(1,0)/f_X(1)
= 0.15/0.45
= 1/3

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(X = 2|Y = 3)
P_Y(3) = 0.25 + 0.05 + 0.15
= 0.45
f(2,3)/P_Y(3)
= 0.25/0.45
~= 0.56

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(Y = 3|X = 2)
P_X(2) = 0.05 + 0.25 = 0.3
f(2,3)/f_X(2)
= 0.25/0.3
~= 0.83

Computing a conditional probability over an interval
   | Y=0| Y=1|Y=2 |Y=3|
X=2| 0.2|   0|0.05|0.1|
X=4| 0.1|0.05|0.15|  0|
X=6| 0.2|0.05|   0|0.1|
Compute P(0 < Y < 3|X = 0)
= P((0 < Y < 3), X = 0)/P(X = 0)
= P(X = 0, (0 < Y < 3))/P(X = 0)
= P(X = 0,Y = 0) + P(X = 0,Y = 2)/P(X = 0)
= (f(0,1) + f(0,2))/f_X(0)
f_X(0) = 0.2 + 0 + 0.05 + 0.1
= 0.35
= 0 + 0.05/0.35
= 0.05/0.35
~= 0.14

example:
    | Y=0| Y=2| Y=4|
X=0 |0.15| 0.1| 0.1|
X=4 |0.05| 0.1|0.05|
X=8 | 0  | 0.2|0.15|
X=12|0.05|0.05|   0|
Compute P(4 <= X <= 8|Y = 0)
= (f(4,0) + (8,0))/f_Y(0)
= 0.05 + 0
f_Y(0) = 0.15 + 0.05 + 0 + 0.05 = 0.25
(0.05 + 0)/0.25
~= 0.2

example:
    | Y=0| Y=2| Y=4|Y=6|
X=1 |1/18|5/18| 1/9|2/9|
X=2 | 1/9|1/18|   0|1/6|
Compute P(2 <= Y <= 6|X = 2)
f_X(2) = 1/9 + 1/18 + 0 + 1/6
= 2/18 + 1/18 + 3/18
= 6/18
= 1/3
f(2,2) + f(2,4) + f(2,6)
= 1/18 + 0 + 1/6
= 4/18
= 2/9
= ((2/9)/(1/3)) 
= 2/9 * 3/1
= 6/9 = 2/3

Conditional probability mass functions
We can now define two more probability mass functions
The conditional probability mass function of X given that Y = y denoted f_X|Y(x|y)
f_X|Y(x|y)
= P(X = x,Y = y) = P(X = x,Y = y)/P(Y = y) 
= f(x,y)/f_Y(y)
f(x,y) is the joint probability mass function of X and Y and f_Y(y) is the marginal mass function of Y
The conditional probability mass function of Y given that X = x denoted f_Y|X(y|x)
f_Y|X(y|x)
= P(Y = y,X = x) = P(X = x,Y = y)/P(X = x)
= f(x,y)/f_X(x)
f(x,y) is a joint probability mass function of X and Y and f_X(x) in the marginal mass function of X
The conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y and independent
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)
X and Y are independent, knowing the value of one random variable does not affect the probability distribution of the other.

Computing the value of a conditional probability mass function at a point
    | Y=0| Y=1| Y=2|
X=1 | 1/6| 1/9| 1/6|
X=2 |1/18|1/18| 1/6|
X=3 |1/9 | 1/9|1/18|
Compute f_X|Y(2|2)
= f(x,y)/f_Y(2)
f_Y(2) = 1/6 + 1/6 + 1/18
= 3/18 + 3/18 + 1/18
= 7/18
f(2,2) = 1/6
= (1/6)/(7/18)
= 18/42
= 3/7

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Comput f_X|Y(1|0)
f_Y(0) = 0.3 + 0.1 = 0.4
f(1,0) = 0.1
0.1/0.4 = 0.25

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Compute f_Y|X(1|0)
f_X(0) = 0.3 + 0.2 + 0.1 = 0.6
f(0,1) = 0.2
0.2/0.6
~= 0.33

Computing conditional probability mass function
   | Y=1| Y=3| Y=5|
X=0| 0.1|   0| 0.2|
X=2| 0.3| 0.1|   0|
X=4| 0.1|   0| 0.2|
find f_X|Y(x|1)
= f(x,1)/f_Y(1)
x ∈ {0,2,4}
P(X = 0|Y = 1) = f(0,1)/f_Y(1) = 0.1/0.5 = 1/5
P(X = 2|Y = 1) = f(2,1)/f_Y(1) = 0.3/0.5 = 3/5
P(X = 4|Y = 1) = f(4,1)/f_Y(1) = 0.1/0.5 = 1/5
x         |0  |2  |4  |
f_X|Y(x|1)|1/5|3/5|1/5|
-----------------------

(rest of the example were faster to answer and calculate in my head then creating tables here for notes)

===================================================

The joint cumulative distribution function (or joint CDF)
F(x,y) = P(X <= x,Y <= y) = Σ_xi<=xΣ_yi<=y(f(xi,yi))
the sets S_X and S_Y are called the supports of X and Y
joint CDF is analogous to the cumulative distribution function of a single random variable
joint PMF:
   | Y=0| Y=1| Y=2|
X=0| 0.1| 0.2| 0.1|
X=1| 0.3| 0.2| 0.1|
the supports of X and Y are S_X = {0,1} and S_Y = {0,1,2}
lets compute F(1,1)
F(1,1) = P(X <= 1, Y <= 1)
f(0,0) + f(0,1) + f(1,0) + f(1,1)
= 0.1 + 0.2 + 0.3 + 0.2
= 0.8

Evaluating a joint CDF
   |Y=-2|Y=-1| Y=0|Y=2|
X=1| 0.1| 0.3|0.05|0.1|
X=3| 0.1|0.05| 0.1|0.2|
find F(2,1)
F(2,1) = P(X <= 2, Y <= 1)
f(1,-2) + f(1,-1) + f(1,0)
= 0.1 + 0.3 + 0.05
= 0.45

example:
   | Y=2| Y=4| Y=6|
X=1| 0.1|0.25|0.25|
X=2|0.05|0.15| 0.2|
find F(2,4)
F(2,4) = P(X <= 2, Y <= 4)
f(1,2) + f(1,4) + f(2,2) + f(2,4)
0.1 + 0.25 + 0.05 + 0.15
= 0.55

example:
   | Y=0| Y=1|
X=2| 0.2|0.25|
X=4|0.16|0.15|
X=6|0.04|0.15|
find F(5,1/2)
F(5,1/2) = P(X <= 5, Y <= 1/2)
f(2,0) + f(4,0)
0.2 + 0.16
= 0.36

Some properties of the joint CDF
0 <= F(x,y) <= 1
the marginal CDF of X is given by F_X(x) = F(x, ∞)
the marginal CDF of Y is given by F_Y(y) = F(∞, y)
F(∞, ∞) = 1
F(-∞, y) = F(x, -∞) = 0
if X and Y are independent random variables then F(x,y) = F_X(x)*F_Y(y)

Interpreting the joint CDF Geometrically
(think of f(x,y) as our top right corner)
S_X = {1,2,3,4}, S_Y = {2,3,4}
find an expression for the corresponding joint CDF when x > 4, y < 2
We can think of each point where f(x,y) != 0 as a point mass in the plane where the probability mass at each point equals the probability associated with that point
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis
all our points are on S = S_X x S_Y = {1,2,3,4} x {2,3,4} our region lies x > 4 and y < 2 there are no points that lie in our region
f(x,y) = 0

example:
S_X = {-2,0,2}, S_Y = {0,1,2,3}
when x < -3, y > 3
f(x,y) = 0
the region is outside any of our supports

example:
when x > 4, y > 3
S_X = {1,2,3,4}, S_Y = {0,1,2,3}
f(x,y) = 1
region includes all points

Finding the cumulative probabilities given values from the joint CDF
F(5,4) = 0.9, F(5,2) = 0.2, F(3,4) = 0.5, F(3,2) = 0.1
S_X = {1,3,5,7}, S_Y = {1,2,3,4,5}
Find P(3 < X <= 5, 2 < Y <= 4)
For a given joint PMF f(x,y) we can think of each point where f(x,y) != 0 as a point mass in the plane, where the probability mass at each point equals the probability associated with that point.
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis.
S = S_X x S_Y = {1,3,5,7} x {1,2,3,4,5}
if we want to find a sub-region within our support say at (5,4) 
subtract the infinite rectangles that have their top-right at (5,2) and (3,4)
we need to add back the infinite rectangle that has its top-right corner at (3,2) this is because we subtracted this region twice in step 2
Find P(3 < X <= 5, 2 < Y <= 4)
top-right corner at (5,4):
= P(X <= 5, Y <= 4)
top-right corner at (5,2) - top-right corner at (3,4)
- P(X <= 5, Y <= 2) - P(X <= 3, Y <= 4)
top-right corner at (3,2)
+ P(X <= 3, Y <= 2)
= F(5,4) - F(5,2) - F(3,4) + F(3,2)
= 0.9 - 0.2 - 0.5 + 0.1
= 0.3

example:
joint CDF of X and Y be F(x,y)
F(9,10) = 0.65, F(6,10) = 0.31, F(3,10) = 0.12
supports are
S_X = {3,6,9,12}, S_Y = {2,4,6,8,10,12}
Find P(3 < X <= 9,Y <= 10)
top-right corner at (9,10) - top-right corner at (3,10):
= P(X <= 9, Y <= 10) - P(X <= 3, Y <= 10)
= F(9,10) - F(3,10)
= 0.65 - 0.12
= 0.53

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.72, F(10,6) = 0.3, F(8,9) = 0.15, F(8,6) = 0.08
supports of X and Y
S_X {2,4,6,8,10,12}, S_Y = {3,6,9,12}
find P(8 < X <= 10, 6 < Y <= 9)
top-right corner at (10,9)
P(X <= 10, Y <= 9)
top-right corner at (10,6) - top-right corner at (8,9)
- P(X <= 10, Y <= 6) - P(X <= 8, Y <= 9)
top-right corner at (8,6)
+ P(X <= 8, Y <= 6)
= F(10,9) - F(10,6) - F(8,9) + F(8,6)
= 0.72 - 0.3 - 0.15 + 0.08
= 0.35

Further finding cumulative probabilities given values from the joint CDF
the joint CDF of X and Y be F(x,y)
F(8,3) = 0.9, F(6,3) = 0.7, F(4,3) = 0.4
S_X = {2,4,6,8}, S_Y = {1,3,5}
P(X > 6, Y <= 4)
top-right corner at (8,3) - top-right corner at (6,3)
P(X <= 8, Y <= 3) - P(X <= 6, Y <= 3)
= F(8,3) - F(6,3)
= 0.9 - 0.7
= 0.2

example:
joint CDF
F(6,5) = 3/5, F(6,3) = 2/5, F(6,2) = 1/10
S_X = {2,6,10}, S_Y = {0,1,2,3,4,5}
find P(X < 7, Y > 3)
top-right corner at (6,5) - top-right corner at (6,3)
P(X <= 6,Y <= 5) - P(X <= 6,Y <= 3)
F(6,5) - F(6,3)
= 3/5 - 2/5
= 1/5

example:
joint CDF
F(7,1) = 9/16, F(7,-2) = 1/4, F(3,1) = 1/8, F(3,-2) = 1/16
S_X = {1,3,5,7,9}, S_Y = {-2,-1,0,1,2}
P(5 <= X <= 8,-1 <= Y <= 1)
9/16 - 4/16 - 2/16 + 1/16
4/16 = 1/4

===================================================

Joint distributions for continuous random variables
A function f(x,y) is a joint probability density function (or joint PDF) for the two continuous random variables X and Y if the following is satisfied
f(x,y) >= 0 for all (x,y) ∈ R^2
states f must be nonnegative
∫∫_R^2 f(x,y)dxdy = 1
the sum (integral) of f over possible X and Y values must add up to 1
P((X,Y) ∈ A) = ∫∫_A f(x,y)dxdy where A is a subset of R^2 
compute the probability of an event A by summing (integrating) f over all regions associated with A
analog of those for the joint PMF of two discrete random variables
they are two dimensional analog for the PDF f(x) of a (single) continuous random variable X

Verifying that a function is a valid joint PDF
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified
to check the second condition we need to integrate
∫∫_R^2 f(x,y) dxdy = ∫∫_D f(x,y) dxdy
= ∫^1_0 ∫^1_0 3/2(x^2 + y^2) dydx
= 3/2 ∫^1_0 ∫^1_0 x^2 + y^2 dydx
= 3/2 ∫^1_0 [x^2y + y^3/3]|_0-1 dx
= 3/2 ∫^1_0 (x^2 + 1/3) dx
= 3/2[x^3/3 + x/3]|_0-1
= 3/2(1/3 + 1/3)
= 1
therefore second condition is satisfied
f(x,y) is a valid joint probability density function

Joint PDFs for continuous random variables
f(x,y) = {
	csin(x + y), 0 < x < pi/2, 0 < y < pi/2
	0,           otherwise
}
verify that f(x,y) is a valid joint PDF
c > 0 we have f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified 
∫^pi/2_0[∫^pi/2_0 csin(x + y) dy]dx
= c ∫^pi/2_0[∫^pi/2_0 sin(x + y) dy]dx
= c ∫^pi/2_0 [-cos(x + y)]|_0-pi/2 dx
= c ∫^pi/2_0 (-cos(x + pi/2) + cos(x)) dx
= c ∫^pi/2_0 cos(x) - cos(x + pi/2) dx
integrate with respect to x
= c[sin(x) - sin(x + pi/2)]|_0-pi/2
= c[(sin(pi/2) - sin(pi)) - (sin(0) - sin(pi/2))]
= c[(1 - 0) - (0 - 1)]
= c[1 - (-1)]
= 2c
hence
2c = 1  =>  c = 1/2

example:
joint PDF:
f(x,y) = {
	c, 0 < x < 2, 1 < y < 2
	0, otherwise
}
c is a positive real number, find the value of c
∫^2_0 ∫^2_1 c dydx
= ∫^2_0 ∫^2_1 cy dydx
= c ∫^2_0 [y]|_1-2 dx
= c ∫^2_0 1 dx
= c[x]|_0-2
= c(2 - 0)
= 2c
2c = 1
c = 1/2

Joint PDFs for continuous random variables (non-rectangular domains)
joint PDF:
f(x,y) = {
	c/(1+x+y)^3, 0 <= x < y
	0, 			 otherwise
}
where c is a positive real number, find the value of c
c > 0, f(x,y) for all (x,y) ∈ R^2 the first condition is satisfied
∫^∞_0 [∫^∞_x c/(1 + x + y)^3 dy] dx
= c ∫^∞_0 [∫^∞_x (1 + x + y)^-3 dy] dx
= c ∫^∞_0 [-1/2(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 [(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 0 - (1 + 2x)^-2 dx
= -c/2 ∫^∞_0 -(1 + 2x)^-2 dx
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
integrate with respect to x
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
= c/2[-1/2(1 + 2x)^-1]|_0-∞
= -c/4[1/1 + 2x]|0-∞
= -c/4[0 - 1]
= c/4
c/4 = 1  => c = 4

example:
joint PDF
f(x,y) = {
	cxy, 0 <= x <= y < 1
	0,   otherwise
}
find value of c
∫^1_0 ∫^1_x cxy dydx
= ∫^1_0 [cxy^2/2]|_x-1 dx
= ∫^1_0 (cx/2 - cx^3/2) dx
integrate with respect to x
= [cx^2/4 - cx^4/8]|_0-1
= c/4 - c/8
= c/8
c/8 = 1  =>  c = 8

example:
joint PDF
f(x,y) = {
	cx^2y, x^2 <= y <= 1
	0,     otherwise
}
find value of c
∫^1_-1 ∫^1_x^2 cx^2y dydx
= ∫^1_-1 [cx^2 * y^2/2]|_x^2-1 dx
= ∫^1_-1 cx^2((1 - x^4)/2)]|_x^2-1 dx
respect to x
= [c(x^3/6 - x^7/14)]|_-1-1
= 4c/21
4c/21 = 1  => c = 21/4

Finding probabilities using joint PDFs
f(x,y) is a joint probability density function for two continuous random variables X and Y
P((X,Y) ∈ A) = ∫∫_A f(x,y) dxdy
if A is the event defined as
A = {(X,Y) | a <= X <= b, c <= Y <= d}
the corresponding probability can be computed as
P(a <= X <= b, c <= Y <= d) = ∫^d_c ∫^b_a f(x,y) dxdy
= ∫^b_a ∫^d_c dydx
analogously to the case of a single continuous random variable the probability that (X,Y) takes on any particular value (x,y) is zero
P(X = x,Y = y) = 0

Computing a probability using a joint PDF
f(x,y) = {
	1/6, 0 <= x <= 3, 0 <= y <= 2
	0,   otherwise
}
find P(X > 1, Y > 1)
= 1/6 ∫∫_A dxdy
= 1/6Area(A)
= 1/6 * (3 - 1) * (2 - 1)
= 1/6 * 2 * 1
= 1/3

example:
PDF:
f(x,y) = {
	1, 0 <= x <= 1, 0 <= y <= 1
	0, otherwise
}
find P(X > Y)
= 0.5

example:
PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
find P(X > 1/2, Y > 1/2)
∫^1_1/2 ∫^1_1/2 4xy dxdy
∫^1_1/2 [∫^1_1/2 4xy dx] dy
∫^1_1/2 [2x^2y]|_1/2-1 dy
∫^1_1/2 3y/2 dy
= [3y^2/4]|_1/2-1
= 9/16

===================================================

Marginal distributions for continuous random variables
Suppose that X and Y are continuous random variables with joint probability density function f(x,y)
the marginal density function of X
f_X(x) = ∫^∞_-∞ f(x,y) dy
the marginal density function of Y
f_Y(y) = ∫^∞_-∞ f(x,y) dx
marginal density function can be known as marginal probability density function or marginal PDF
To convert from discrete to continuous we turn sums into integrals
when we compute f_X in the discrete case, we sum over all possible values of Y in the continuous case we integrate with repect to y
vise-versa for f_Y

example:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
lets find the expression for the MDF f_X(x) of X
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 x^2 + y^2 dy
= 3/2[x^2y + y^3/3]|_0-1
= 3/2((x^2*1 1^3/3) - (0))
= 3/2[x^2 + 1/3]
= 1/2(3x^2 + 1)
f_X(x) = {
	1/2(3x^2 + 1), 0 <= x <= 1
	0,             otherwise
}

finding a marginal distribution from a joint distribution
joint PDF
f(x,y) = {
	15/7(x^2y^4), 1 <= x <= 2, 0 <= y <= 1
	0,            otherwise
}
find expression for the marginal PDF f_Y(y)
the marginal density function of Y for 0 <= y <= 1
f_Y(y) ∫^2_1 15/7(x^2y^4) dx
15/7(y^4)[x^3/3]|_1-2
= 5/7(y^4)[x^3]|_1-2
= 5/7(y^4)(2^3 - 1^3)
= 5y^4
full expression
f_Y(y) = {
	5y^4, 0 <= y <= 1
	0,	  otherwise
}

example:
X and Y be two continuous random variables
joint PDF:
f(x,y) = {
	3/2(y^2), 0 <= x <= 2, 0 <= y <= 1,
	0,        otherwise
}
find the expression for the marginal PDF f_X(x)
∫^1_0 3/2(y^2) dy
= 3/2[y^3/3]|_0-1
= 1/2[y^3]|_0-1
= 1/2(1^3 - 0)
= 1/2
f_X(x) = {
	1/2, 0 <= x <= 2,
	0,   otherwise
}

example:
joint PDF:
f(x,y) = {
	8/3(x^3y), 0 <= x <= 1, 1 <= y <= 2
	0,   	   otherwise
}
find the marginal PDF f_Y(y)
∫^1_0 8/3(x^3y) dx
= 8/3y[x^4/4]|_0-1
= 2/3(y)[x^4]
= 2/3(y)(1^4 - 0)
= 2/3y

Finding a marginal probability from a joint PDF
f(x,y) = {
	e^-(4x+y/4), x >= 0, y >= 0
	0,           otherwise
}
find P(-4 <= Y <= 4)
the marginal density function of Y for y >= 0
f_Y(y) ∫^∞_0 e^-(4x+y/4) dx
= [-1/4e^-(4x+y/4)]|_0-∞
= -1/4(0 - e^-(0+y/4))
= 1/4e^(-y/4)
full expression for f_y(y)
f_y(y) = {
	1/4e^(-y/4), y >= 0
	0,         otherwise
}
the required probability is
P(-4 <= Y <= 4) = ∫^4_-4 f_Y(y) dy
= ∫^4_0 1/4e^(-y/4) dy
= [-e^(-y/4)]|_0-4
= (-e^(-4/4) + e^0)
= 1 - 1/e

example:
joint PDF:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,				otherwise
}
P(0.2 < X < 1)
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 (x^2 + y^2) dy
= [3/2(x^2y) + 3/6(y^3)]|_0-1
= [3/2(x^2y) + 1/2(y^3)]|_0-1
= [3/2(x^2y) + y^3/2]|_0-1
= (3/2(x^2) + 1/2) - 0
= 3/2(x^2) + 1/2
full expression for f_X(x)
f_X(x) = {
	3/2(x^2) + 1/2, 0 <= x <= 1
	0,              otherwise
}
∫^1_0.2 (3/2(x^2) + 1/2) dx
= [x^3/2 + 1/2x]|_0.2-1
= [1/2 + 1/2] - [(0.2)^3/2 + 0.2/2]
= 1 - 0.104
= 0.896

example:
joint PDF:
f(x,y) = {
	1/6(x + y), 0 <= x <= 1, 0 <= y <= 3
	0,			otherwise
}
P(Y <= 1)
∫^1_0 1/6(x + y) dx 
= [x^2/12 + yx/6]|_0-1
= (1/12 + y/6) - 0
full expression for f_X(x)
f_Y(y) = {
	1/12 + y/6, 0 <= y <= 3
	0,          otherwise
}
required probability
P(Y <= 1) = ∫^1_-∞ f_Y(y) dy
∫^1_0 (1/12 + y/6) dy
= [y/12 + y^2/12]|_0-1
= (1/12 + 1/12) - 0
= 1/6

Finding a marginal distribution from a joint distribution non-rectangular domains
joint PDF:
f(x,y) = 5ysqrt(x), y^2 <= x <= 1, y >= 0
find the expression for the marginal PDF f_Y(y)
0 <= y <= 1
f_Y(y) = ∫^1_y^2 5ysqrt(x) dx
= 5y ∫^1_y^2 sqrt(x) dx
= 5y[2/3(x^3/2)]|_y^2-1
= 10y/3[x^(3/2)]|_y^2-1
= 10y/3[1^(3/2) - (y^2)^(3/2)]
= 10y/3[1 - (y^3)]
= 10(y - y^4)/3
full expression for f_Y(y)
f_Y(y) = {
	10(y - y^4)/3, 0 <= y <= 1
	0, 			   otherwise
}

example:
joint PDF:
f(x,y) = 1/2xy 0 <= x <= y <= 2
find the expression for the marginal PDF f_X(x)
∫^2_x 1/2xy dy
= [xy^2/4]|_x-2
= (x - x^3/4)
f_X(x) = {
	x - x^3/4, 0 <= x <= 2
	0,         otherwise
}

example:
joint PDF:
f(x,y) = 1/(pi)(x^2), y^2 + 1 <= x
find the expression for the marginal PDF f_Y(y)
∫^∞_(y^2 + 1) 1/(pi)(x^2) dx
1/pi ∫^∞_(y^2 + 1) x^-2 dx
[-1/(pi)(x)]|_(y^2 + 1)-∞
= 0 - (-1/(pi)(y^2 + 1))
f_Y(y) = 1/pi(y^2 + 1)

Marginal cumulative distribution functions
joint probability density function
f(x,y) = {
	3/8(x + y)^2, -1 <= x <= 1, -1 <= y <= 1
	0,            otherwise
}
find the marginal cumulative distribution function (CDF) F_Y(y) of Y
the marginal function of Y for -1 <= y <= 1
f_Y(y) = ∫^1_-1 3/8(x + y)^2 dx
= 3/8[1/3(x + y)^3]|_-1-1
= 1/8[(x + y)^3]|_-1-1
= 1/8[(1 + y)^3 - (-1 + y)^3]
= 1/8[(1 + y)^3 - (y - 1)^3]
= 1/8[(y^3 + 3y^2 + 3y + 1) - (y^3 - 3y^2 + 3y - 1)]
(cancel out terms by distributing -)
= 1/8[2(3y^2 + 1)]
= 1/4(3y^2 + 1)
the marginal CDF of Y for -1 <= y <= 1
F_Y(y) = ∫^y_-∞ fx(u) du
= ∫^y_-1 1/4(3u^2 + 1) du
= 1/4 ∫^y_-1 (3u^2 + 1) du
= 1/4[u^3 + u]|_-1-y
= 1/4[y^3 + y - (-1)^3 - (-1)]
= 1/4[y^3 + y + 2]
= (y^3 + y + 2)/4
full expression for the marginal CDF of Y is
F_Y(y) = {
	0, 		          y < -1
	(y^3 + y + 2)/4, -1 <= y <= 1
	1,                y > 1
}

example:
joint PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
the marginal CDF F_X(x) of X
F_X(x) = {
	0, 	  x < 0
	g(x), 0 <= x <= 1
	1, 	  x > 1
}
what is the function g(x)
∫^1_0 4xy dy
= 4x[y^2/2]|_0-1
= 2x[y^2]|_0-1
= 2x(1^2 - 0)
= 2x
the marginal CDF of X for 0 <= x <= 1
∫^1_0 2u du
= [u^2]|_0-1
= x^2 - 0
= x^2
full expression F_X(x)
F_X(x) = {
	0,   x < 0
	x^2, 0 <= x <= 1
	1,   x > 1
}
g(x) = x^2

example:
joint PDF:
f(x,y) = {
	2y/(x + 1)^2 x >= 0, 0 <= y <= 1
	0,			 otherwise
}
the marginal CDF F_Y(y) of Y
F_Y(y) = {
	0,    y < 0
	g(y), 0 <= y <= 1
	1,    y > 1 
}
what is the function g(y)
∫^∞_0 2y/(x + 1)^2 dx
= 2y ∫^b_0 [(x + 1)^-1] dx
= 2y[-1/(x + 1)]|_0-b
= 2y((-1/b + 1) + (1/0 + 1))
= 2y(1 - 1/(b + 1))
(when b approaches infinity 1/(b + 1) approaches 0 simplifies)
= 2y
∫^y_0 2u du
= [u^2]|_0-y
= y^2 - 0
= y^2
full expression F_Y(y)
F_Y(y) = {
	0,   y < 0
	y^2, 0 <= y <= 1
	1,   y > 1
}
g(y) = y^2

===================================================

Independence of continuous random variables
Let X and Y be continuous random variables X and Y are independent if for all A ⊆ R and B ⊆ R
P(X ∈ A,Y ∈ B) = P(X ∈ A) * P(Y ∈ B)
X and Y are independent iff
f(x,y) = f_X(x) * f_Y(y)
(we've seen this before)
direct analogy with the case of independence for discrete random variables.

Finding the joint PDF of two independent random variables using marginal PDFs
X and Y are two independent continuous random variables whose MDF are
f_X(x) = {
	2/9(x), 0 <= x <= 3
	0,      otherwise
}
f_Y(y) = {
	1/4(y^3), 0 <= y <= 2
	0,        otherwise
}
find the joint probability density function f(x,y)
since our random variables are independent
f(x,y) = f_X(x) * f_Y(y)
= 2/9(x) * 1/4(y^3)
= 2/36(xy^2)
= 1/18(xy^2)
where (x,y) ∈ [0,3] x [0,2] the full expression f(x,y)
f(x,y) = {
	1/18(xy^3), 0 <= x <= 3, 0 <= y <= 2
	0,          otherwise
}

example:
X and Y and their marginal density functions:
f_X(x) = {
	4x^3, 0 <= x <= 1
	0,    otherwise
}
f_Y(y) = {
	2/3y, 1 <= y <= 2
	0,    otherwise
}
find the joint probability density function f(x,y)
X and Y are independent so 
f(x,y) = f_X(x) * f_Y(y)
4x^3 * 2/3y
= 8/3x^3y
where (x,y) ∈ [0,1] x [1,2] the full expression f(x,y)
f(x,y) = {
	8/3(x^3y), 0 <= x <= 1, 1 <= y <= 2
	0,         otherwise
}

example:
X and Y are two continuous random variables whose marginal density functions are
f_X(x) = {
	3x^2, 0 <= x <= 1
	0,    otherwise
}
f_Y(y) = {
	4y^3, 0 <= y <= 1
	0,    otherwise
}
find the joint probability density function f(x,y)
hint: We don't know if X and Y are independent
if we don't know if X and Y and independent then its impossible to determine using only the given information

Finding a joint probability for some independent random variables using marginal PDFs
Let X and Y be two independent continuous random variables whose marginal density functions
f_X(x) = {
	10/3x^2, 2 <= x <= 5
	0,       otherwise
}
f_Y(y) = {
	1/4(y), 1 <= y <= 3
	0,      otherwise
}
find P(X >= 3,Y <= 2)
f(x,y) = f_X(x) * f_Y(y)
= 10/3x^2 * 1/4(y)
= 5y/6x^2
where (x,y) ∈ [2,5] x [1,3] the full expression f(x,y)
f(x,y) = {
	5y/6x^2, 2 <= x <= 5, 1 <= y <= 3
	0,       otherwise
}
required probability
P(X >= 3,Y <= 2)
= ∫^5_3 ∫^2_1 5y/6x^2 dy dx
= ∫^5_3 [∫^2_1 5y/6x^2 dy] dx
= ∫^5_3 5/6x^2 [y^2/2]|_1-2 dx
= ∫^5_3 5y/6x^2 (3/2) dx
= ∫^5_3 5/4x^2 dx
= [-5/4x]|_3-5
= -5/4(1/5 - 1/3)
= 1/6

example:
X and Y are two independent continuous random variables MDF
f_X(x) = {
	2x, 0 <= x <= 1
	0,  otherwise
}
f_Y(y) = {
	3y, 0 <= y <= sqrt(2/3)
	0,  otherwise
}
find P(X <= 0.5,Y <= 1)
f(x,y) = f_X(x) * f_Y(y)
2x * 3y = 6xy
where (x,y) ∈ [0,1] x [0,sqrt(2/3)] the full expression f(x,y)
f(x,y) = {
	6xy, 0 <= x <= 1, 0 <= y <= sqrt(2/3)
	0,   otherwise
}
required probability
= ∫^0.5_0 [∫^1_0 6xy dy] dx
= ∫^0.5_0 6x[y]|_0-1 dx
= ∫^0.5_0 6x[y^2/2]|_0-1 dx
= ∫^0.5_0 6x(1/3) dx
= ∫^0.5_0 3x(1) dx
= ∫^0.5_0 3x dx
= [3x^2/2]|_0-0.5
= ((3(0.5)^2/2) - (0))
= 0.375

example:
MDF:
f_X(x) = {
	3/2(sqrt(x)), 0 <= x <= 1
	0,  		  otherwise
}
f_Y(y) = {
	2/3(y), 1 <= y <= 2
	0,      otherwise
}
find P(0 <= X <= 0.4, Y <= 2)
3/2(sqrt(x)) * 2/3(y)
= ysqrt(x)
where (x,y) ∈ [0,1] x [1,2] the full expression f(x,y)
f(x,y) = {
	ysqrt(x), 0 <= x <= 1, 1 <= y <= 2
	0,		  otherwise
}
required probability
= ∫^0.4_0 [∫^2_1 ysqrt(x) dy] dx
= ∫^0.4_0 sqrt(x)[y^2/2]|_1-2 dx
= ∫^0.4_0 sqrt(x)(4/2 - 1/2) dx
= ∫^0.4_0 sqrt(x)(3/2) dx
= 3/2[x^1/2]|_0-0.4
= 3/2[2/3(x^(3/2))]|_0-0.4
= 3/2(2/3(0.4)^(3/2) - 2/3(0)^(3/2))
(0.4^(3/2) = 0.4^1 * 0.4^1/2 = 0.4 * sqrt(0.4))
(sqrt(0.4) ~= 0.6325)
(0.4 * 0.253)
= 2/3 * 0.253 ~= 0.506/3 ~= 0.16867
= 3/2(0.16867)
~= 0.253

Determining whether two continuous random variables are independent
The joint PDF for the continuous random variables X and Y
f(x,y) = {
	1/4(xe^-y/2), 0 <= x <= 2, y >= 0
	0,            otherwise
}
which statements are true?
1. f_Y(y) = 1/2(e^-y/2) for y ∈ [0, ∞)
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
statement 1 is true, computing the MDF for Y
f_Y(y) = ∫^2_0 1/4(xe^-y/2) dx
= 1/4(e^-y/2) [x^2/2]|_0-2
= 1/2(e^-y/2)
where y ∈ [0, ∞)
statement 2 is true, computing the marginal density function for X
f_X(x) = ∫^∞_0 1/4xe^-y/2 dy
= 1/4(x) ∫^b_0 e^-y/2 dy
= 1/4(x) [-2e^-y/2]|_0-b
= 1/4(x) (-2e^-b/2 + 2)
(-2e^-b/2 b -> ∞ this becomes 0)
we are left with
= x/4(2)
= x/2
when x ∈ [0,2] and y ∈ [0, ∞)
1/4(xe^-y/2) <- f(x,y)
= x/2 <- f_X(x)  *  1/2(e^-y/2) <- f_Y(y)
Statement 3 is also true, because of 2 X and Y are independent

example:
joint PDF:
f(x,y) = {
	2/3(xy^3), 2 <= x <= 4, 0 <= y <= 1
	0,         otherwise
}
Which statements are true?
1. f_X(x) = 2x for x ∈ [2,4]
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
f_X(x) = ∫^1_0 2/3(xy^3) dy
= 1/3(x)[y^4/2]|_0-1
= 1/3(x)(1/2)
= 1/6(x)
statement 1 is false because 2x is not x/6
f_Y(y) = ∫^4_2 2/3(xy^3) dx
= 2/3(y^3) [x^2/2]|_2-4
= 2/3(y^3) (16/2 - 4/2)
= 2/3(y^3) (16/2 - 4/2)
= 2/3(y^3)(6)
= 4y^3
Statement 3 is true, since 2 is true. X and Y are independent
f(x,y) = f_X(x) * f_Y(y)
2/3(xy^3) = 1/6(x) * 4y^3

example:
joint PDF:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
1. f_X(x) = 3/2(x^2) + 1/2 for x ∈ [0,1]
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
f_X(x) = ∫^1_0 3/2(x^2 + y^2) dy
= 3/2(x^2) ∫^1_0 3/2[y^2] dy
= 3/2(x^2) + 3/2[y^3/3]|_0-1
= 3/2(x^2) + 3/2(1/3)
= 3/2(x^2) + 1/2
statement 1 is true
f_Y(y) = ∫^1_0 3/2(x^2 + y^2) dx
= 3/2[x^3/3 + xy^2]|_0-1
= 3/2(y^2) + 1/2
statement 2 is false
f(x,y) 		   !=       f_X(x) 	  *     f_Y(y)
3/2(x^2 + y^2) != (3/2(x^2) + 1/2)(3/2(y^2) + 1/2)
statement 3 is false, since statment 2 is not true X and Y are not independent therefore correct answer is 1 only

theorem:
Suppose that the joint support of two continuous random variables X and Y with a joint probability density function f(x,y) is rectangular if
f(x,y) = g(x) * h(x)
for some two functions g and h (not necessarily PDF) then X and Y are independent, the rectangular joint support S does not need to be finite.
For example random variables X and Y with the following joint PDF:
f(x,y) = {
	1/4(xe^-y/2), 0 <= x <= 2, y >= 0
	0,			  otherwise
}
the joint support [0,2] x [0, ∞) is rectangular, we can write f(x,y) = g(x) * h(x) where
g(x) = 1/4x,  h(y) = e^-y/2
X and Y are independent

===================================================
**
For continuous random variables X and Y the conditional probability density function of X is given Y = y is
f_X|Y(x|y) = f(x,y)/f_Y(y), f_Y(y) != 0
and the CPD function Y given X = x is
f_Y|X(y|x) = f(x,y)/f_X(x), f_X(x) != 0
(just like discrete random variables)
To compute probabilities we work directly with these conditional PDFs to calculate the conditional probability of X given Y we use
P(X ∈ A|Y = y) = ∫_A f_X|Y(x|y) dx
the conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y are independent then
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)

Continuous random variables X and Y which the CPD function f_Y|X(y|x) and the MDF f_X(x)
f_Y|X(y|x) = {
	3(x^2 + y^2)/(3x^2 + 1), 0 <= x <= 1, 0 <= y <= 1
	0, 					     otherwise
}
f_X(x) = {
	(3x^2 + 1)/2, 0 <= x <= 1
	0,            otherwise
}
find the expression of the joint probability density function f(x,y) for 0 <= x <= 1, 0 <= y <= 1
conditional probability density function of Y given X = x for two continuous random variables X and Y
f_Y|X(y|x) = f(x,y)/f_X(x)
f(x,y) is the joint PDF for X and Y
f_X(x) is the MDF for X
from the equation 
f(x,y) = f_Y|X(y|x) * f_X(x)
0 <= x <= 1, 0 <= y <= 1
= 3(x^2 + y^2)/(3x^2 + 1) * (3x^2 + 1)/2
= 3(x^2 + y^2)/2
for other values of x and y the function f_Y|X(y|x) equals 0 the product will be equal to 0 as well.
the PDF:
f(x,y) = {
	3(x^2 + y^2)/2, (3x^2 + 1)/2, 0 <= x <= 1
	0,				otherwise
}

example:
joint probability density function f(x,y) and the marginal density function f_X(x)
f(x,y) = {
	3x, 0 < x < 1, 0 < y < x
	0,  otherwise
}
f_X(x) = {
	3x^2, 0 < x < 1
	0,    otherwise
}
find the expression of the conditional probability density function f_Y|X(y|x) for 0 < x < 1, 0 < y < x
f_Y|X(y|x) = f(x,y)/f_X(x)
= (3x)/3(x^2)
= 1/x
the conditional probability density function
f_Y|X(y|x) = {
	1/x, 0 < x < 1, 0 < y < x
	0,   otherwise
}

example:
The conditional probability density function f_X|Y(x|y) and the marginal density function f_Y(y)
f_X|Y(x|y) = {
	2x/y^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}
f_Y(y) = {
	5y^4, 0 < y < 1
	0,    otherwise
}
find the expression of the joint probability density function f(x,y) for 0 < x < y, 0 < y < 1
f(x,y) = f_X|Y(x|y) * f_Y(y)
= 2x/y^2 * 5y^4
= 10xy^2
joint probability density function
f(x,y) = {
	10xy^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}

Finding a conditional PDF
X and Y be continuous random variables with the joint probability density function
f(x,y) = {
	x + y, 0 <= x <= 1, 0 <= y <= 1
	0,     otherwise
}
find the expression of the conditional probability density function f_X|Y(x|y) for 0 <= x <= 1, 0 <= y <= 1
using the formula for the marginal density function for Y
f_Y(y) = ∫^1_0 (x + y) dx
= [x^2/2 + xy]|_0-1
= 1/2 + y - 0
= (2y + 1)/2
f_Y(y) = {
	(2y + 1)/2, 0 <= y <= 1
	0,          otherwise
}
The conditional probability density function of X given Y = y for two continuous random variables X and Y
f_X|Y(x|y) = f(x,y)/f_Y(y)
f(x,y) is the joint probability density function for X and Y
f_Y(y) is the marginal density function for Y
0 <= x <= 1, 0 <= y <= 1
f_X|Y(x|y) = f(x,y)/f_Y(y)
= x + y/((2y + 1)/2)
= 2(x + y)/2y + 1
for other values of x and y the function f(x,y) equals 0 as a result f_X|Y(x|y) will be equal to 0
The conditional probability density function
f_X|Y(x|y) = {
	2(x + y)/2y + 1, 0 <= x <= 1, 0 <= y <= 1
	0,               otherwise
}

example:
joint probability density function
f(x,y) = {
	2y(2x + 3y)/5, 1 <= x <= 2, 0 <= y <= 1
	0,             otherwise
}
find the expression of the conditional probability density function f_X|Y(x|y) for 1 <= x <= 2, 0 <= y <= 1
f_Y(y) = ∫^1_2 2y(2x + 3y)/5 dx
= ∫^1_2 (4xy/5 + 6y^2/5) dx
= [2x^2y/5 + 6xy^2]|_1-2
= (8y/5 + 12y^2/5) - (2y/5 - 6y^2/5)
= 6y(y + 1)/5
full expression for f_Y(y)
f_Y(y) = {
	6y(y + 1)/5, 0 <= y <= 1
	0,           otherwise
}
f_X|Y(x|y) = f(x,y)/f_Y(y)
f(x,y) is the joint probability density function for X and Y
F_Y(y) is the marginal density function for Y
1 <= x <= 2, 0 <= y <= 1
= (2y(2x + 3y)/5)/(6y(y + 1)/5)
= 2x + 3y/3(y + 1)
for other values of x and y the function f(x,y) equals 0 f_X|Y(x|y) will be equal to 0 as well
conditional probability density function
f_X|Y(x|y) = {
	2x + 3y/3(y + 1), 1 <= x <= 2, 0 <= y <= 1
	0,                otherwise
}

example:
joint probability function
f(x,y) = {
	1, 0 < x < 1, x - 1 < y < 1 - x
	0, otherwise
}
find the expression of the conditional probability density function f_Y|X(y|x) for 0 < x < 1, x - 1 < y < 1 - x
f_X(x) = ∫^(1-x)_(x-1) 1 dy
= [y]|_(x-1)-(1-x)
= ((1-x) - (x-1))
= 2 - 2x
full expression for f_X(x)
f_X(x) = {
	2 - 2x, 0 < x < 1
	0, 	    otherwise
}
f_Y|X(y|x) = f(x,y)/f_X(x)
f(x,y) is the joint probability density function for X and Y
f_X(x) is the marginal density function for X
0 < x < 1, x - 1 < y < 1 - x
f_Y|X(y|x) = f(x,y)/f_X(x)
= 1/2 - 2x
Conditional probability density function
f_Y|X(y|x) = {
	1/2 - 2x, 0 < x < 1, x - 1 < y < 1 - x
	0,        otherwise
}

Computing a conditional probability
X and Y are two continuous random variables with the joint probability density function
f(x,y) = {
	8x^2y, 0 < x <= 1, 0 <= y <= sqrt(x)
	0,     otherwise
}
find P(Y < 1/2|X = 1)
recall
P(Y ∈ A|X = x) = ∫_A f_Y|X(y|x) dy
where f_Y|X(y|x) is the conditional probability density function
marginal density function for X
= ∫^sqrt(x)_0 8x^2y dy
= 8x^2 ∫^sqrt(x)_0 y dy
= 8x^2 [y^2/2]|_0-sqrt(x)
= 4x^2(sqrt(2))^2 - 0
= 4x^3
full expression for f_X(x)
f_X(x) = {
	4x^3, 0 < x <= 1,
	0,    otherwise
}
conditional probability density function for two continuous random variables X and Y
f_Y|X(y|x) = f(x,y)/f_X(x)
like always f(x,y) joint probability density function
f_X(x) is the marginal density function for X
0 < x <= 1, 0 <= y <= sqrt(x)
= 8x^2y/4x^3
= 2y/x
the conditional probability density function
f_Y|X(y|x) = {
	2y/x, 0 < x <= 1, 0 <= y <= sqrt(x)
	0,    otherwise
}
computing the probability
P(Y < 1/2|X = 1) = ∫^1/2_0 f_Y|X(y|x = 1) dy
= ∫^1/2_0 2y/1 dy
= ∫^1/2_0 2y dy
= [y^2]|_0-1/2
= 1/4 - 0
= 1/4

example:
Conditional probability density function
f_X|Y(x|y) = {
	2x/y^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}
find P(1/4 < X < 1/2|Y = 5/8)
= ∫^1/2_1/4 f_X|Y(x|y = 5/8) dx
= ∫^1/2_1/4 2x/(5/8)^2 dx
= 64/25 ∫^1/2_1/4 2x dx
= 64/25 [x^2]|_1/4-1/2
= 64/25(1/4 - 1/16)
= 64/25(3/16)
= 12/25

example:
X and Y be two continuous random variables with the following joint probability density function
f(x,y) = {
	24xy, 0 < x < 1, 0 < y < 1 - x
	0,    otherwise
}
find P(Y < X|X = 1/3)
P(Y ∈ A|X = x) = ∫_A f_Y|X(y|x) dy
where f_Y|X(y|x) is the conditional probability density function
marginal density function for X
= ∫^(1-x)_0 24xy dy
= 12x[y^2]|_0-(1-x)
= 12x(1 - x)^2
= 12x(x - 1)^2
f_X(x) = {
	12x(x - 1)^2, 0 < y < 1 - x,
	0,    		  otherwise
}
f(x,y)/f_X(x)
= 24xy/12x(x - 1)^2
= 2y/(x - 1)^2
conditional probability density function
f_Y|X(y|x) = {
	2y/(x - 1)^2, 0 < x < 1, 0 < y < 1 - x
	0,            otherwise
}
we compute probability
P(Y < X|X = 1/3) = P(Y < 1/3|X = 1/3)
= ∫^1/3_0 f_Y|X(y|x = 1/3) dy
= ∫^1/3_0 2y/(1/3 - 1)^2 dy
= ∫^1/3_0 2y/(1/3 - 3/3)^2 dy
= ∫^1/3_0 2y/(-2/3)^2 dy
= ∫^1/3_0 2y/(4/9) dy
= ∫^1/3_0 2y * 9/4 dy
= ∫^1/3_0 9y/2 dy
= 9/2 ∫^1/3_0 y dy
= 9/2 [y^2/2]|_0-1/3
= 9/2(1/18 - 0)
= 1/4

===================================================

The joint cumulative distribution function (joint CDF)
F(x,y) = P(X <= x,Y <= y)
Joint CDF gives the probability that pair (X,Y) will lie inside the inifinite rectangular region Δ that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axes
0 <= F(x,y) <= 1
the marginal CDF of X is given by F_X(x) = F(x, ∞)
the marginal CDF of Y is given by F_Y(y) = F(∞, y)
F(∞,∞) = 1
F(-∞,y) = F(x,-∞) = 0
if X and Y are independent random variables, F(x,y) = F_X(x) * F_Y(y)
if X and Y are continuous random variables with joint probability density function f(x,y) we have the following relationship between the joint PDF and joint CDF
F(x,y) = ∫^y_-∞ ∫^x_-∞ f(u,v) dudv
over the rectangle [a,b]x[c,d] then the joint CDF is
F(x,y) = ∫^d_c ∫^b_a f(u,v) dudv

Computing the value of the joint CDF at a point
X and Y be continuous random variables with the joint PDF
f(x,y) = {
	3/5(sqrt(x)y), (x,y) ∈ D
	0,              otherwise
}
if F(x,y) is the joint CDF of X and Y find F(2,2/5)
using the definition of the joint CDF
F(2, 5/2) = P(X < 2, Y < 5/2) = ∫^5/2_-∞ ∫^2_-∞ f(u,v) dudv
we're only interested in the region inside the Δ where f(x,y) is non-zero
D ∩ Δ = {(x,y) : 0 <= x <= 1, 2 <= y <= 5/2}
∫^5/2_2 ∫^1_0 3/5(sqrt(u)v) dudv
= ∫^5/2_2 ∫^1_0 3/5(u^(1/2)v) dudv
= ∫^5/2_2 [3/5 * 2/3(u^(3/2)v)]|_0-1 dv
= ∫^5/2_2 [2/5u^(3/2)v]|_0-1 dv
= ∫^5/2_2 2/5(v) dv
= [v^2/5]|_2-5/2
= 5/4 - 4/5
= 9/20

example:
joint probability density function
f(x,y) = {
	x + y, (x,y) ∈ D
	0,     otherwise
}
if F(x,y) is the joint CDF of X and Y find F(1/2,1/5)
F(1/2, 1/5) = P(X < 1/2, Y < 1/5) = ∫^1/5_-∞ ∫^1/2_-∞ f(u,v) dudv
D ∩ Δ = {(x,y) : 0 <= x <= 1/2, 0 <= y <= 1/5}
= ∫^1/5_0 ∫^1/2_0 (u + v) dudv
= ∫^1/5_0 [u^2/2 + uv]|_0-1/2 dv
= ∫^1/5_0 (1/8 + v/2) dv
= [v/8 + v^2/4]|_0-1/5
= [1/40 + 1/100] - 0
= 7/200

example:
joint probability density function
f(x,y) = {
	x + y, (x,y) ∈ D
	0,     otherwise
}
if F(x,y) is the joint CDF of X and Y find F(3/2, 1/2)
F(3/2, 1/2) = P(X < 3/2, Y < 1/2) = ∫^1/2_-∞ ∫^3/2_-∞ f(u,v) dudv
D ∩ Δ = {(x,y) : 0 <= x <= 1, 0 <= y <= 1/2}
= ∫^1/2_0 ∫^1_0 (u + v) dudv
= ∫^1/2_0 [u^2/2 + uv]|_0-1 dv
= ∫^1/2_0 (1/2 + v) dv
= [v/2 + v^2/2]|_0-1/2
= 1/4 + 1/8
= 2/8 + 1/8
= 3/8

Finding a probability geometrically using a CDF
the joint CDF of X and Y be F(x,y)
F(5,5) = 0.9, F(5,2) = 0.3, F(0,5) = 0.45, F(0,2) = 0.15
find P(0 <= X <= 5, 2 <= Y <= 5)
geometrically this region can be obtained by
1. take the infinite rectangle that has its top-right corner at (5,5)
2. subtract the infinite rectangles that have their top-right corners at (5,2) and (0,5)
3. add back the infinite rectangle that has its top-right corner at (0,2) this is because we subtracted this region in step 2
P(0 <= X <= 5, 2 <= Y <= 5)
top-right corner: at (5,5) - at (5,2) - at (0,5)
P(X <= 5,Y <= 5) - P(X <= 5,Y <= 2) - P(X <= 0,Y <= 5)
at (0,2)
+ P(X <= 0,Y <= 2)
= F(5,5) - F(5,2) - F(0,5) + F(0,2)
= 0.3

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.7, F(9,5) = 0.4, F(10,5) = 0.2
find P(X <= 10, 5 <= Y <= 9)
F(10,9) - F(10,5) (same x axis)
= 0.7 - 0.2
= 0.5

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.7, F(10,5) = 0.2, F(8,9) = 0.3, F(8,5) = 0.1
find P(8 <= X <= 10, 5 <= Y <= 9)
F(10,9) - F(10,5) - F(8,9) + F(8,5)
= 0.7 - 0.2 - 0.3 + 0.1
= 0.3

Finding Part of a Joint CDF from a joint PDF simple cases
joint probability density function
f(x,y) = {
	1/4(xy), 0 <= x <= 1, 0 <= y <= 4
	0,       otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 4
only interested in the region inside Δ where f(x,y) is nonzero
D ∩ Δ = D
our region covers the entire region
∫^y_-∞ ∫^x_-∞ f(u,v) dudv = ∫∫_D f(u,v) dudv = 1

example:
joint probability density function
f(x,y) = {
	1/3(x^2y^2), 0 <= x <= 1, 0 <= y <= 3
	0,           otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 3
D ∩ Δ = D (picked a point where x > 1 and y > 3)
so our entire region gets covered

example:
joint probability density function
f(x,y) = {
	1/3(x^2y^2), 0 <= x <= 1, 0 <= y <= 3
	0,           otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 3
D ∩ Δ = D (picked a point where x < 0 and 1 < y < 3)
so our region lies below and to the left of our point (f(x,y))
D ∩ Δ = 0

finding part of a joint CDF from a joint PDF
joint probability density function
f(x,y) = {
	1/3(x + y), 0 <= x <= 2, 0 <= y <= 1
	0,          otherwise
}
find the expression corresponding joint CDF 0 <= x <= 2 and y > 1
∫^1_0 ∫^x_0 1/3(u + v) dudv
= 1/3 ∫^1_0 [u^2/2 + uv]|_0-x dv
= 1/3 ∫^1_0 [x^2/2 + xv] dv
= 1/3[x^2v/2 + xv^2/2]|_0-1
= 1/3(x^2/2 + x/2 - 0)
= 1/6(x^2 + x)

example:
joint probability density function
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
find the expression for the corresponding joint CDF when 0 <= x <= 1, 0 <= y <= 1
∫^y_0 ∫^x_0 4uv dudv
= ∫^y_0 [4vu^2/2]|_0-x dv
= ∫^y_0 4vx^2/2 dv
= [2v^2x^2/2]|_0-y
= 2y^2x^2/2
= y^2x^2

example:
joint probability density function
f(x,y) = {
	1/2sin(x + y), 0 <= x <= pi/2, 0 <= y <= pi/2
}
find the expression for the joint CDF x > pi/2 and 0 <= t <= pi/2
D ∩ Δ = {(u,v) : 0 <= u <= pi/2, 0 <= v <= y}
∫^y_0 ∫^pi/2_0 1/2sin(u + v) dudv
= 1/2 ∫^y_0 [-cos(u + v)]|_0-pi/2 dv
= 1/2 ∫^y_0 (cos(v) - cos(pi/2 + v)) dv
= 1/2 [sin(v) - sin(pi/2 + v)]|_0-y
= 1/2 (sin(y) - sin(pi/2 + y)) - 1/2(sin(0) - sin(pi/2 + 0))
= 1/2(sin(y) - sin(pi/2 + y) + 1)

=================================================

Recovering a marginal CDF
how to recover the marginal CDFs of X and Y from the joint CDF
the joint CDF of two continuous random variables X and Y is
F(x,y) = {
	(1 - e^-x)(1 - e^2y), x >= 0, y >= 0
	0,                    otherwise
}
lets calculate F_Y(y) the marginal CDF of Y using the property
F_Y(y) = F(∞,y) lim_x->∞ F(x,y)
therefore y >= 0
F_Y(y) = lim_x->∞ [(1 - e^-x)(1 - e^-2y)]
= (1 - e^-2y) lim_x->∞ (1 - 1/e^x)
= (1 - e^-2y) lim_x->∞ (1 - 0)
= 1 - e^-2y
the full expression for the marginal CDF
F_Y(y) = {
	1 - e^-2y, y >= 0
	0,         otherwise
}

Finding a marginal CDF using a joint CDF
The joint CDF of two continuous random variables X and Y for (x,y) memberof [0,1] x [0,2]
F(x,y) = xy/8(2x^2 + y)
find F_X(x) the marginal CDF of X for 0 <= x <= 1
the marginal CDF of X is given by
F_X(x) = F(x, ∞) = lim_y->∞ F(x,y)
for 0 <= x <= 1
F_X(x) = F(x, ∞)
= F(x,2)
= x(2)/8(2x^2 + (2))
= 4x/8(x^2 + 1)
= 1/2(x^3 + x)

example:
the joint CDF
F(x,y) {
	0, 			    x < 0 or y < 0
	(1 - e^-3x)y^2, x >= 0, 0 <= y <= 1
	F_X(x),         x >= 0, y > 1
}
find F_Y(y) the marginal CDF of Y for y >= 0
for y >= 0
y^2 lim_x->∞ (1 - e^-3x)
= y^2 lim_x->∞ (1 - 0)
= y^2

example:
the joint CDF for (x,y) [0,1] x [0,2]
F(x,y) = 1/20x^2y(4x + 3y)
find F_X(x) the marginal CDF of X for 0 <= x <= 1
1/20x^2(2)(4x + 3(2))
= 1/20x^2(2)(4x + 6)
= 1/20x^2(8x + 12)
= 1/5(2x^3 + 3x^2)

Finding part of a joint CDF from marginal CDFs for continuous random variables
the marginal CDFs
F_X(x) = {
	1 - 1/x, x >= 1
	0,       otherwise
}
F_Y(y) = {
	1 - 1/sqrt(y^3), y >= 1
	0,               otherwise
}
for x >= 1 and y >= 1 the joint CDF
= (1 - 1/x)(1 - 1/sqrt(y^3))
= 1 - 1/x - 1/sqrt(y^3) + 1/xsqrt(y^3)
full expression for the joint CDF
F(x,y) = {
	1 - 1/x - 1/sqrt(y^3) + 1/xsqrt(y^3), x >= 1, y >= 1
	0,								      otherwise
}

example:
The marginal CDFs
F_X(x) = {
	1 - e^-2x, x >= 0
	0,         otherwise
}
F_Y(y) = {
	1 - e^-3y, y >= 0
	0,         otherwise
}
find the expression for the joint CDF when x >= 0 and y >= 0
(1 - e^-2x)(1 - e^-3y)
= 1 - e^-2x - e^-3y + e^-2x-3y
F(x,y) = {
	1 - e^-2x - e^-3y + e^-2x-3y, x >= 0, y >= 0
	0,							  otherwise
}

example:
The marginal CDFs
F_X(x) = {
	1 - 1/x^2, x >= 1
	0,         otherwise
}
F_Y(y) = {
	1 - 1/y, y >= 1
	0,       otherwise
}
find the expression for the joint CDF when x >= 1 and y >= 1
(1 - 1/x^2)(1 - 1/y)
= 1 - 1/x^2 - 1/y + 1/x^2y
F(x,y) = {
	1 - 1/x^2 - 1/y + 1/x^2y, x >= 1, y >= 1
	0,                        otherwise
}

Recovering the joint PMF from the joint CDF
the joint CDF
F(x,y) = {
	0,             x < 0 or y < 0
	x^2(1 - e^-y), 0 <= x <= 1, y >= 0
	F_Y(y),        x > 1, y >= 0
}
find the joint PMF f(x,y) for 0 <= x <= 1, y >= 0
since the partial derivatives of F(x,y) are defined the joint probability density function f(x,y)
f(x,y) = ∂^2/∂x∂y F(x,y)
for 0 <= x <= 1 and y >= 0
= ∂/∂x(∂/∂y(x^2(1 - e^-y)))
= ∂/∂x(x^2∂/∂y(1 - e^-y))
= ∂/∂x(x^2e^-y)
= e^-y * ∂/∂x(x^2)
= e^-y * 2x
= 2xe^-y
f(x,y) = {
	2xe^-y, 0 <= x <= 1, y >= 0
	0,      otherwise
}

example:
joint CDF:
F(x,y) = {
	(1 - e^-x)(1 - e^-y), x >= 0, y >= 0
	0,					  otherwise
}
find the joint PMF f(x,y) for x >= 0, y >= 0
∂/∂x(∂/∂y(1 - e^-x)(1 - e^-y))
∂/∂x((1 - e^-x)(e^-y))
e^-x-y
f(x,y) = {
	e^-x-y, x >= 0, y >= 0
	0,      otherwise
}

example:
joint CDF:
F(x,y) = {
	0,	                  x < 1 or y < 1
	4(x^2 - 1)(y - 1)/9y, 1 <= x <= 2, 1 <= y <= 4
	F_X(x),				  1 <= x <= 2, y > 4
	F_Y(y),				  x > 2, 1 <= y <= 4
	1,					  x > 2, y > 4
}
where F_X and F_Y are the marginal CDFs of C and Y. Find the joint PMF f(x,y) for 1 <= x <= 2 and 1 <= y <= 4
∂/∂x(∂/∂y(4(x^2 - 1)(y - 1)/9y))
= ∂/∂x(4(x^2 - 1) * ∂/∂y(1/9 - 1/9y))
= ∂/∂x(4(x^2 - 1) * 1/9y^2)
= 4/9y^2 * ∂/∂x(x^2  -1)
= 8x/9y^2
f(x,y) = {
	8x/9y^2, 1 <= x <= 2, 1 <= y <= 4
	0,       otherwise
}

===================================================

For any random variables X and Y and constants a and b
E[aX + bY] = aE[X] + bE[Y]
The expected value of a sum of scaled random variables equals the sum of the scaled expected values
E[2X + 5Y] = 2E[X] + 5E[Y]
property also works with substraction
E[2X - 5Y] = 2E[X] - 5E[Y]
we can extend this idea to any number of random variables X1,X2...,Xn and real constants a1,a2,...an
E[a1X1 + a2X2 + ... + anXn] = a1E[X1] + a2E[X2] + ... + anE[Xn]
E[Σ^n_i=1(aiXi)] = Σ^n_i=1(aiE[Xi])

Finding the expected value of a sum of random variables
Compute E[5X - 2Y] if E[X] = 1 and E[Y] = 2
E[aX + bY] = aE[X] + bE[Y]
E[5X - 2Y] = 5E[X] - 2E[Y]
= 5(1) - 2(2)
= 1

example:
E[X] = 4, E[Y] = 5 find E[5X + 4Y]
= 5(4) + 4(5)
= 40

example:
find E[5X + 4Y]
x   | 0 | 1 |  2 |  3 | 4 |
f(x)|0.1|0.2|0.25|0.15|0.3|
---------------------------
y   |  2 | 4 |  6 |
f(y)|0.25|0.5|0.25|
-------------------
X = 0 * 0.1 + 1 * 0.2 + 2 * 0.25 + 3 * 0.15 + 4 * 0.3 = 2.35
y = 2 * 0.25 + 4 * 0.5 + 6 * 0.25 = 4
5(2.35) + 4(4)
= 27.75

Finding the expected value of a sum of random variables in context
Mathias has two dice one of them is fair and the other is not the rolls of the unfair die follow the PMF f(y) what is the expected sum of the outcomes of 2 throws of the fair die and 4 throws of the unfair one?
y   | 1 |  2 | 3 | 4 | 5 |  6 |
f(y)|1/4|1/16|1/4|1/8|1/4|1/16|
-------------------------------
Xi be the ith score from the fair die and Yj be the jth score from unfair die.
E[Σ^2_i=1(Xi) + Σ^4_j=1(Yj)]
= Σ^2_i=1(E[Xi]) + Σ^4_j=1(E[Yj])
= 2E[X] + 4E[Y]
E[Xi] = E[X] and E[Yj] = E[Y] for all 1 <= i <= 2, 1 <= j <= 4
The expected values of X and Y:
E[X] = 1*1/6 + 2*1/6 + 3*1/6 + 4*1/6 + 5*1/6 + 6*1/6 = 7/2
E[Y] = 1*1/4 + 2*1/16 + 3*1/4 + 4*1/8 + 5*1/4 + 6*1/16 = 13/4
E[2X + 4Y] = 2E[X] + 4E[Y] = 2*7/2 + 4*13/4 = 20

example:
Two machines produce same car the random variable X represents the number of rejects per day of first machine random variable Y are rejects per day on other machine, these are probability distributions
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
expected number of rejects per day
= 2

example:
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
5 type A (X) and 3 type B (Y) 
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
1.3(5) + 0.7(3)
= 8.6

Expected product of two independent random variables
E[X * Y] = E[X] * E[Y]
E[X] = 2 and E[Y] = 3
= 2 * 3 = 6
its important to check that the variables are independent if the random variables X1,X2,...Xn are mutually independent
E[X1*X2...Xn] = E[X1] * E[X2] * ... * E[Xn]
E[Π^n_i=1(Xi)] = Π^n_i=1(Xi)
(Π is the product operator known as capital Pi)

Finding the expected value of a product of random variables
The probability distributions of the independent random variables X and Y
x   | -4 | -2 | 2 | 4 |
f(x)| 1/6| 1/3|1/3|1/6|
-----------------------
y   | 1 | 2 | 3 |
f(y)|1/4|1/4|1/2|
-----------------
find E[XY]
E[X] = -4*1/6 - 2*1/3 + 2*1/3 + 4*1/6 = 0
E[Y] = 1*1/4 + 2*1/4 + 3*1/2 = 9/4
E[X * Y] = E[X] * E[Y] = 0 * 9/4 = 0

example:
E[X] = 4, E[Y] = 5, find E[XY]
E[X] * E[Y] = 20

example:
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
find E[XY]
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
1.3 * 0.7 = 0.91

===================================================

Variance of sums of independent random variables
Var[X + Y] = Var[X] + Var[Y]
Var[X - Y] = Var[X] + Var[Y]
When computing the variance of a difference we add the variances

Calculating the variance of a sum or difference of independent random variables
For two independent random variables X and Y we have Var[X] = 3 and Var[Y] = 7 find Var[X - Y]
Var[X - Y] = 3 + 7 = 10

example:
Var[X] = 12 and Var[Y] = 5, find Var[X + Y]
= 17

example:
Var[X] = 12 and Var[Y] = 5, find Var[X - Y]
= 17

calculating the variance of a sum of scaled random variables
Var[X] = 10 and Var[Y] = 12, find Var[X - 2Y]
if X and Y are independent variables, and a and b are constants then
Var[aX + bY] = a^2Var[X] + b^2Var[Y]
substituting we get
Var[X - 2Y] =  (1)^2 * 10 + (-2)^2*12 = 58

example:
Var[X] = 12 and Var[Y] = 5, find Var[2X + 5Y]
= (2)^2*12 + (5)^2*5
= 173

example:
Var[X] = 11 and Var[Y] = 15, find Var[2X - 3Y]
= (2)^2*11 + (3)^2*15
= 179

calculate a combined variance given some raw moments
E[X] = 5, E[Y] = 4 and second raw moments E[X^2] = 30 and E[Y^2] = 18 find the variance of 12X - 11Y
Var[X] = E[X^2] - E[X]^2
= 30 - 5^2
= 5
Var[Y] = E[Y^2] - E[Y]^2
= 18 - 4^2
= 2
Var[12X - 11Y] = 12^2Var[X] + (-11)^2Var[Y]
= 144(5) + 121(2)
= 962

example:
E[X] = 2, E[Y] = 4, second raw moments E[X^2] = 5, E[Y^2] = 23 find the variance of 5X + 7Y
Var[X] = 5 - 2^2 = 1
Var[Y] = 23 - 4^2 = 23 - 16 = 7
5^2(1) + 7^2(7)
25 + 343 = 368

example:
PMF
x   | 1 | 2 | 3 |
f(x)|0.2|0.3|0.5|
-----------------
y   |  1 |  2 |
f(y)|0.25|0.75|
---------------
find Var[3X + 2Y]
E[X] = 0.2 + 2*0.3 + 3*0.5 = 2.3
E[Y] = 0.25 + 2*0.75 = 1.75
E[X^2] = 1*0.2 + 4*0.3 + 9*0.5 = 5.9
Var[X] = E[X^2] - E[X]^2
= 5.9 - 2.3^2
= 0.61
E[Y^2] = 1*0.25 + 4*0.75
= 3.25
Var[Y] = E[Y^2] - E[Y]^2
= 3.25 - 1.75^2
= 0.1875
Var[3X - 2Y] = 3^2Var[X] + 2^2Var[Y]
= 3^2(0.61) + 2^2(0.1875)
= 6.24

===================================================

To compute the expected value of a discrete random variable X given a joint mass function f(x,y) we must first find its marginal mass function f_X once the marginal mass function is known we can compute E[X] using
E[X] = Σ^n_i=1(f_X(xi))
joint probability mass function f(x,y)
f  |Y=1 | Y=2|
X=1|0.05|0.15|
X=2| 0.7| 0.1|
--------------
the marginal distribution for X corresponds to the row totals and the marginal distributino for Y corresponds to the column totals, we're only interested in expected value of X

f  |Y=1 | Y=2| f_X|
X=1|0.05|0.15|0.20|
X=2| 0.7| 0.1|0.80|
-------------------
x     |  1 |  2 |
f_X(x)|0.20|0.80|
-----------------
expected value of X:
E[X] = 1*0.2 + 2*0.8
= 0.2 + 1.6
E[X] = 1.8

example:
joint probability distribution, find E[Y]
f  |Y=0 | Y=1| Y=2|
X=1|0.05| 0.1|0.25|
X=2|0.25|0.15| 0.2|
-------------------
y     | 0 |  1 |  2 |
f_Y(y)|0.3|0.25|0.45|
---------------------
E[Y] = 0*0.3 + 1*0.25 + 2*0.45
= 1.15

example:
joint probability distribution, find E[5X - 2Y]
f  |Y=0 | Y=1| Y=2|
X=1|0.05| 0.1|0.25|
X=2|0.25|0.15| 0.2|
-------------------
y     | 0 |  1 |  2 |
f_Y(y)|0.3|0.25|0.45|
---------------------
x     | 0 | 1 |
f_X(x)|0.4|0.6|
---------------
E[Y] = 0*0.3 + 1*0.25 + 2*0.45
= 1.15
E[X] = 0.6
5(0.6) - 2(1.15)
= 0.7

Continuous random variable
to compute the expected value of a continuous random variable X given a joint density function f(x,y) we must first find its marginal mass function f_X once the marginal mass function is known we can then compute E[X] using
E[X] = ∫^∞_-∞ x * f_X(x) dx

finding the expected value of a continuous random variable from a joint distribution
joint PDF:
f(x,y) = {
	4/x^3y^3, x >= 1, y >= 1
	0,        otherwise
}
find the expected value of Y
The marginal density for Y when y >= 1
f_Y(y) ∫^∞_1 4/x^3y^3 dx
= 4/y^3 ∫^b_1 1/x^3 dx
= 4/y^3 [-1/2x^2]|_1-b
= 4/y^3 (1/2 - 1/2b^2)
= 4/y^3 (1/2 - 0)
= 2/y^3
full expression for f_Y(y)
f_Y(y) = {
	2/y^3, y >= 1
	0,     otherwise
}
expected value of Y
E[Y] = ∫^∞_-∞ y * f_Y(y) dy
= ∫^∞_1 y * 2/y^3 dy
= 2 ∫^b_1 1/y^2 dy
= 2 [-1/y]|_1-b
= 2(1 - 1/b)
= 2

example:
joint probability density function
f(x,y) = {
	2/pi(sin(y)), 0 <= x <= pi/2, 0 <= y <= pi/2
	0,	          otherwise
}
find the expected value of X
marginal density function for X
f_X(x) ∫^pi/2_0 2/pi(sin(y)) dy
2/pi ∫^pi/2_0 sin(y) dy
2/pi [-cos(y)]|_0-pi/2
2/pi (-cos(pi/2) - (-cos(0)))
2/pi (0 - (-1))
= 2/pi
f_X(x) = {
	2/pi, 0 <= x <= pi/2
	0,    otherwise
}
E[X] = ∫^pi/2_0 x * 2/pi dx
= 2/pi [x^2/2]|_0-pi/2
= 2/pi (pi^2/8)
= 2pi^2/8pi
= pi^2/4pi
= pi/4

example:
joint PDF
f(x,y) = {
	3/2(x^2y), 0 <= x <= 1, 0 <= y <= 2
	0,		   otherwise
}
E[X] = 3/4, find E[4X + 6Y]
f_y(y) ∫^1_0 3/2(x^2y) dx
= 3/2 ∫^1_0 (x^2y) dx
= 3/2y [x^3/3]|_0-1
= 3/2y(1/3)
= 3/6(y)
= 1/2(y)
E[Y] = ∫^2_0 y * 1/2(y) dy
= ∫^2_0 y^2/2 dy
= [y^3/6]|_0-2
= 8/6 
= 4/3
E[4X + 6Y] = 4(3/4) + 6(4/3)
3 + 8 = 11

===================================================

For discrete random variables X and Y the conditional expected value of X given Y = y is
E[X|Y = y] = Σ_x(xf_X|Y(x|y))
where f_X|Y(x|y) is the conditional probability mass function of X given Y
This definition is similar to that of E[X] however we now use the conditional PMF f_X|Y(x|y) instead of the marginal PMF f_X(x) in the summation
The conditional expectation E[X|Y = y] is (in general) a function of y. If the distribution of X varies with Y then the expected value of X for a given Y must vary with the value of Y
If y is replaced with a number then E[X|Y = y] returns a single value
If X and Y are independent then E[X|Y = y] = E[X] for all y. If the outcome of Y has no influence on the outcome of X then the distribution of X and its mean is also unaffected by Y
something the notation μ_X|y to denote E[X|Y = y]
same for E[Y|X = x]
= Σ_y(yf_Y|X(y|x))

Calculating a conditional expected value given conditional mass
y         | 1 | 2 |
f_Y|X(y|2)|2/3|1/3|
-------------------
find the E[Y|X = 2] given that the conditional probability mass function f_Y|X(y|2) is shown in the table above
E[Y|X = 2] = Σ_y(yf_Y|X(y|2))
= 1 * 2/3 + 2 * 1/3
= 4/3

example:
x         | 2 | 3 |
f_X|Y(x|1)|4/5|1/5|
-------------------
find E[X|Y = 1] given that the conditional probability mass function f_X|Y(x|1)
= 2 * 4/5 + 3 * 1/5
= 11/5

example:
f_Y|X(y|5) = {
	0.2, y = 1
	0.8, y = 3
	0,   otherwise
}
find E[Y|X = 5] given that the conditional probability mass function f_Y|X(y|5)
E[Y|X = 5] = 0.2 + 0.8 * 3
= 2.6

Calculating a conditional expectation using row totals
f  | Y=1| Y=2|
X=1| 1/9| 2/9|
X=2|8/27|5/27|
X=3|2/27|1/91|
--------------
find E[Y|X = 2]
in order to find f_Y|X(y|x) lets first find the marginal distribution for X corresponding row totals
f  | Y=1| Y=2|f_X(x)|
X=1| 1/9| 2/9|   1/3|
X=2|8/27|5/27| 13/27|
X=3|2/27|1/91|  5/27|
---------------------
The conditional probability mass function of Y given that X = x is
f_Y|X(y|x) = f(x,y)/f_X(x)
dividing each joint probability by the corresponding row total we obtain the conditional probability mass function of Y given that X = x
f_Y|X(y|x)| Y=1| Y=2|f_X(x)|
X=1       | 1/3| 2/3|     1|
X=2       |8/13|5/13|     1|
X=3       | 2/5| 3/5|     1|
----------------------------
E[Y|X = 2] = 1 * 8/13 + 2 * 5/13
= 18/13

example:
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/8|1/4|1/8|   1/2|
X=1|1/4|1/8|1/8|   1/2|
-----------------------
find E[Y|X = 0] given the joint probability mass function
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/4|1/2|1/4|     1|
X=1|1/2|1/4|1/4|     1|
-----------------------
= 0*1/4 + 1*1/2 + 2*1/4
= 1

example:
f  |Y=0| Y=2| Y=4|
X=0|1/9|3/18|1/18|
X=2|1/3| 1/9| 2/9|
------------------
find E[Y|X = 2] givem the joint probability mass function
f  |Y=0| Y=2| Y=4|f_X(x)|
X=0|1/9|3/18|1/18|   1/3|
X=2|1/3| 1/9| 2/9|   2/3|
-------------------------
f  |Y=0| Y=2| Y=4|f_X(x)|
X=0|1/9|3/18|1/18|   1/3|
X=2|1/3| 1/9| 2/9|   2/3|
-------------------------
3/6 3/18 6/18 (multiplying row 2 by the recipical of total)
(0)9/18 + (2)3/18 + (4)6/18 = 6/18 + 24/18
= 30/18 = 15/9
= 5/3

Calculating a conditional expectation using column totals
f  |Y=1|Y=2|
X=2|1/9|2/9|
X=4|1/3|1/9|
X=6|1/9|1/9|
------------
find E[X|Y = 1] given the joint probability mass function
In order to find f_X|Y(x|y) lets first find the marginal distribution for Y corresponding to the column totals
f     |Y=1|Y=2|
X=2   |1/9|2/9|
X=4   |1/3|1/9|
X=6   |1/9|1/9|
f_Y(y)|5/9|4/9|
---------------
f_X|Y(x|y) = f(x,y)/f_Y(y)
same processes as the rows we divide each joint probability by the corresponding column total we obtain the conditional probability mass function of X given Y = y
f_X|Y(x|y)|Y=1|Y=2|
X=2       |1/5|1/2|
X=4       |3/5|1/4|
X=6       |1/5|1/4|
          |  1|  1|
-------------------
= 2*1/5 + 4*3/5 + 6*1/5
= 20/5
= 4

example:
f  |Y=0|Y=1|Y=2|
X=0|1/8|1/4|1/8|
X=1|1/4|1/8|1/8|
----------------
find E[X|Y = 1] given the joint probability mass function
f     |Y=0|Y=1|Y=2|
X=0   |1/8|1/4|1/8|
X=1   |1/4|1/8|1/8|
f_Y(y)|3/8|3/8|1/4|
-------------------
= (0)16/24 + (1)8/24 (focusing on second column only)
= 1/3

example:
f  |Y=4|Y=5|Y=6|
X=0|1/9|1/3|1/9|
X=3|2/9|1/9|1/9|
----------------
find E[X|Y = 6] given the joint probability mass function
f     |Y=4|Y=5|Y=6|
X=0   |1/9|1/3|1/9|
X=3   |2/9|1/9|1/9|
f_Y(y)|3/9|4/9|2/9|
-------------------
9/18 9/18 = (1)1/2 + (3)1/2
= 4/2 = 2

===================================================

X and Y are discrete random variables, where the conditional expected value of X given Y = y is given by μ_X|y
E[X|Y = y] = μ_X|y
The conditional variance of X given Y = y
Var[X|Y] = E[(X - μ_X|y)^2|Y = y]
This is similar to the definition of Var[X] in that it represents the average (squared) distance of the random variable X from the mean. The difference here is that the expected values are now dependent on the outcome of Y
similarly to the case of Var[X] this often isn't the easiest formula to use in practice, it can be shown that the definition of conditional variance is equivalent to
Var[X|Y = y] = E[X^2|Y = y] - μ^2_X|y
= [Σ_x(x^2f_X|Y(x|y))] - μ^2_X|y
where f_X|Y(x|y) is the conditional PMF of X given Y these formulas are analogous to those for Var[X]
similar Y given X = x
Var[Y|X] = E[(Y - μ^2_Y|x)^2|X = x]
= E[Y^2|X = x] - μ^2_Y|x
= [Σ_y(y^2f_Y|X(y|x))] - μ^2_Y|x

Calculating a conditional variance given a conditional mass function
     y    | 1 | 2 | 4 |
f_Y|X(y|3)|1/3|1/6|1/2|
-----------------------
X and Y are discrete random variables. Find Var[Y|X = 3] the corresponding conditional expected value is μ_Y|3 = 8/3
Var[Y|X = 3] = [Σ_y(y^2f_Y|X(y|3))] - μ^2_Y|3
= [(1)^2*1/3 + (2)^2*1/6 + (4)^2*1/2] - (8/3)^2
= 1/3 + 2/3 + 8 - 64/9
= 17/9

example:
f_X|Y(x|3) = {
	1/3, x = 12
	2/3, x = 18
	0,   otherwise
}
find the conditional variance Var[X|Y = 3] given that the conditional probability mass function f_X|Y(x|3) is shown above the corresponding conditional expected value is μ_X|3 = 16
= [(12)^2*1/3 + (18)^2*2/3]
= [48 + 216] - 256
= 8

example:
     y    | 0 | 1 | 2 |
f_Y|X(y|1)|0.1|0.4|0.5|
-----------------------
find Var[Y|X = 1] given the conditional probability mass function f_Y|X(y|1) conditional expected value is μ_Y|1 = 1.4
= [0.4 + 2^2*0.5] - 1.96
= 0.44

calculating a conditional variance using row totals
f  | Y=1| Y=2| Y=3|
X=1|1/10|1/10| 1/5|
X=2|3/10| 1/5|1/10|
-------------------
find Var[Y|X = 1] given the joint probability mass function f(x,y) corresponding conditional expected value is μ_Y|1 = 9/4
f  | Y=1| Y=2| Y=3|f_X(x)|
X=1|1/10|1/10| 1/5|   2/5|
X=2|3/10| 1/5|1/10|   3/5|
--------------------------
the conditional probability mass function of Y given that X = x
f_Y|X(y|x) = f(x,y)/f_X(x)
dividing each joint probability by the row total we obtain the conditional probability mass function of Y given that X = x
f_Y|X(y|x)|Y=1| Y=2|Y=3|f_X(x)|
X=1       |1/4| 1/4|1/2|     1|
X=2       |1/2| 1/3|1/6|     1|
-------------------------------
Var[Y|X = 1] = [Σ_y(y^2f_Y|X(y|1))] - μ^2_Y|1
= [1^2*1/4 + 2^2*1/4 + 3^2*1/2] - (9/4)^2
= 1/4 + 1 + 9/2 - 81/16
= 11/16

example:
f  |Y=0|Y=1|Y=2|
X=0|1/8|1/4|1/8|
X=1|1/4|1/8|1/8|
----------------
find Var[Y|X = 0] given the joint probability mass function f(x,y), conditional expected value is μ_Y|0 = 1
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/8|1/4|1/8|   1/2|
X=1|1/4|1/8|1/8|   1/2|
-----------------------
2/8 + 4/8 + 2/8 (first row)
4/8 + 8/8 = 12/8 - 1
= 6/4 = 3/2 - 2/2
= 1/2

example:
find Var[Y|X = 1] given the joint probability mass function f(x,y), corresponding conditional expected value is μ_Y|1 = 6/5
f  |Y=1|Y=2|f_X(x)|
X=1|1/2|1/8|   5/8|
X=2|1/8|1/4|   3/8|
-------------------
32/40 + 8/40
= 16/20 + 4/20
= 8/10 + 2/10
= 4/5 + 1/5
= 4/5 + 4*1/5 = 8/5 - (6/5)^2
= 40/25 - 36/25 = 4/25

Calculate a conditional variance using column totals
f  | Y=2| Y=4|
X=1| 1/2| 1/4|
X=2|1/16|3/16|
--------------
find Var[X|Y = 2] given the joint probability mass function f(x,y), conditional expected value μ_X|2 = 10/9
find f_X|Y(x|y) lets first find the marginal distribution of Y corresponding to the column totals
f     | Y=2| Y=4|
X=1   | 1/2| 1/4|
X=2   |1/16|3/16|
f_Y(y)|9/16|7/16|
-----------------
the conditional probability mass function of X given that Y = y
f_X|Y(x|y) = f(x,y)/f_Y(y)
dividing each joint probability by the corresponding column total we get the conditional probability mass function of X given Y = y
f_X|Y(x,y)|Y=2|Y=4|
X=1       |8/9|4/7|
X=2       |1/9|3/7|
          |  1|  1|
-------------------
= [1^2*8/9 + 2^2*1/9] - (10/9)^2
= 8/9 + 4/9 - 100/81
= 8/81

example:
f  |Y=0|Y=1|
X=0|1/2|1/8|
X=1|1/4|1/8|
------------
find Var[X|Y = 0] given the joint PMF the expected value is μ_X|0 = 1/3
f     |Y=0|Y=1|
X=0   |1/2|1/8|
X=1   |1/4|1/8|
f_Y(y)|3/4|2/8|
---------------
= [0^2*4/6 + 1^2*4/12] - (1/3)^2
1/3 - 1/9
3/9 - 1/9 = 2/9

example:
f     | Y=0|  Y=1|
X=0   |1/10|  1/5|
X=3   | 1/5|  1/4|
X=9   |3/20| 1/10|
f_Y(y)|9/20|11/20|
------------------
find Var[X|Y = 0] given the joint PMF, expected value is μ_X|0 = 13/3
[0 + 3^2*20/45 + 9^2*60/180] - 169/9
180/45 + 4860/180 - 169/9
4 + 27 - 169/9
279/9 - 169/9
110/9

===================================================

For continuous random variables X and Y the conditional expectation of X given Y = y is defined as
E[X|Y = y] = ∫_R xf_X|Y(x|y) dx
where f_X|Y(x|y) is the conditional probability density function of X given Y
This definition is similar to that of E[X] we now use the conditional PDF f_X|Y(x|y) instead of the marginal PDF f_X(x) in our integral
When we select a particular value of y, the conditional expected value of X given Y = y returns a real number if y is not specified i.e. E[X|Y = y] depends on y we have that E[X|Y = y] gives us a function of y
if X and Y are independent then E[X|Y = y] = E[X] for all y
Sometimes the notation we use is μ_X|y to denote E[X|Y = y]
we have an analogous definition for E[Y|X = x]:
E[Y|X = x] = ∫_R yf_Y|X(y|x) dy
f_Y|X(y|x) is the conditional probability density function of Y given X

A worked example
conditional expectation E[X|Y = 1] given that the conditional probability density function f_X|Y(x|1) is given by
f_X|Y(x|1) = {
	2(x - 1), 1 <= x <= 2
	0,        otherwise
}
X given Y = y
∫^∞_-∞ xf_X|Y(x|1) dx
= ∫^2_1 x * 2(x - 1) dx
= ∫^2_1 2x(x - 1) dx
= ∫^2_1 2x^2 - 2x dx
= [2x^2/3 - x^2]|_1-2
= ((2(2)^2/3 - (2)^2) - (2(1)^2/3 - (1)^2))
= (16/3 - 4) - (2/3 - 1)
= (16/3 - 12/3) - (2/3 - 3/3)
= 4/3 - (-1/3)
= 5/3

Calculating a conditional expectation from a conditional PDF
f_Y|X(y|1) = {
	3(1 + y^2)/4, 0 <= y <= 1
	0,            otherwise
}
X and Y are continuous find the conditional expected value E[Y|X = 1] given that the conditional probability density function f_Y|X(y|1)
= ∫^1_0 y * 3(1 + y^2)/4 dy
= 3/4 ∫^1_0 (y + y^3) dy
= 3/4 [y^2/2 + y^4/4]|_0-1
= 3/4(1/2 + 1/4 - 0)
= 9/16

example:
conditional probability density function
f_X|Y(x|1) = {
	x/4, 1 <= x <= 3
	0,   otherwise
}
find the conditional expected value E[X|Y = 2]
= ∫^3_1 x * x/4 dx
= 1/4 ∫^3_1 x^2 dx
= 1/4 [x^3/3]|_1-3
= 1/4 (27/3 - 1/3)
= 1/4 (26/3)
= 26/12
= 13/6

example:
conditional probability density function
f_Y|X(y|2) = {
	32y/3, 1/4 <= y <= 1/2
	0,     otherwise
}
find the conditional expected value E[Y|X = 2]
= ∫^1/2_1/4 y * 32y/3 dy
= 32/3 ∫^1/2_1/4 y^2 dy
= 32/3 [y^3/3]|_1/4-1/2
= 32/3(1/24 - 1/192)
= 32/9(1/8 - 1/64)
= 32/9(8/64 - 1/64)
= 32/9(7/64) = 224/64
= 7/18

Calculate conditional expectations given joint and marginal PDFs
joint probability density function and marginal probability density function
f(x,y) = {
	y - x, -1 <= x <= 0, 0 <= y <= 1
	0,     otherwise
}
f_Y(y) = {
	1/2 + y, 0 <= y <= 1
	0,       otherwise
}
fing the conditional expected value E[X|Y = 1/2]
The conditional density function of X given Y = y
f_X|Y(x|y) = f(x,y)/f_Y(y)
= (y - x)/(1/2 + y)
= 2(y - x)/1 + 2y
where -1 <= x <= 0, 0 <= y <= 1 outside this domain we have f_X|Y(x|y)
f_X|Y(x|1/2) = 2((1/2) - x)/(1 + 2(1/2))
= 1 - 2x/2
= 1/2 - x
using the conditional expectation
∫^0_-1 x (1/2 - x) dx
= ∫^0_-1 (x/2 - x^2) dx
= [x^2/4 - x^3/3]|_-1-0
= 0 - (1/4 + 1/3)
= -7/12

example:
(similar to last example)
Their joint probability density function and the marginal probability density
f(x,y) = {
	x + y, 0 <= x <= 1, 0 <= y <= 1
	0,     otherwise
}
f_Y(y) = {
	1/2 + y, 0 <= y <= 1
	0,       otherwise
}
find the conditional expected value E[X|Y = 1/2]
f_X|Y(x|y) = f(x,y)/f_Y(y)
= (x + y)/(1/2 + y)
= 2(x + y)/1 + 2y
f_X|Y(x|1/2) = 2((1/2) + x)/(1 + 2(1/2))
= 1 + 2x/2
= 1/2 + x
using the conditional expectation
∫^1_0 x (1/2 + x) dx
= ∫^1_0 (x/2 + x^2) dx
= [x^2/4 + x^3/3]|_0-1
= (1/4 + 1/3) - (0)
= 3/12 + 4/12
= 7/12

example:
joint probability density function and the marginal probability density function
f(x,y) = {
	(y^2 - x^2)/2, 0 <= x <= 1, 1 <= y <= 2
	0,			   otherwise
}
f_X(x) = {
	7 - 3x^2/6, 0 < x < 1
	0,          otherwise
}
find the conditional expected value E[Y|X = 1]
f_Y|X(y|x) = f(x,y)/f_X(x)
= (y^2 - x^2)/2
= 7 - 3X^2/6 = 7 - x^2/2
= 6(y^2 - x^2)/2(7 - 3x^2)
= 3(y^2 - x^2)/(7 - 3x^2)
f_Y|X(y|1) = 3(y^2 - 1)/4
using the conditional expectation
∫^2_1 y 3(y^2 - 1)/4 dy
= ∫^2_1 (3y^3 - 3y)/4 dy
= [3y^4/16 - 3y^2/8]|_1-2
= ((48/16) - (12/8) - (3/16 - 3/8))
= ((48/16) - (24/16) - (3/16 - 6/16))
= (24/16) - (-(3/16))
= 24/16 + 3/16
= 27/16

Conditional expectation as a function
joint probability density function and the marginal probability density function
f(x,y) = {
	x + y/4, 0 <= y <= x, 0 < x < 2
	0,       otherwise
}
f_X(x) = {
	3x^2/8, 0 < x < 2
	0,      otherwise
}
find the expression for the conditional expected value E[Y|X = x] for 0 < x < 2
The conditional density function of Y given X = x
f_Y|X(y|x) = f(x,y)/f_X(x)
= ((x + y)/4)/(3x^2/8)
= 2(x + y)/3x^2
where 0 <= y <= x, 0 < x < 2 outside this domain f_Y|X(y|x) = 0
the definition of the conditional expectation E[Y|X = x]
= ∫^x_0 y * 2(x + y)/3x^2 dy
= 2/3x^2 ∫^x_0 (xy + y^2) dy
= 2/3x^2 [xy^2/2 + y^3/3]|_0-x
= 2/3x^2((x^3/2 + x^3/3) - (0))
= 2/3x^2 * 5x^3/6
= 1/3x^2 * 5x^2 * x/3
= 5x/9

example:
joint probability density function and the marginal probability density function
f(x,y) = {
	xy/4, 0 <= x <= 2, 0 < y < 2
	0,    otherwise
}
f_Y(y) = {
	y/2, 0 < y < 2
	0,   otherwise
}
find the expression for the conditional expected value E[X|Y = y] for 0 < y < 2
f_X|Y(x|y) = f(x,y)/f_Y(y)
= (xy/4)/y/2
= 2(xy)/4y
= x/2
where 0 <= x <= 2, 0 < y < 2
= ∫^2_0 x * x/2 dx
= ∫^2_0 x^2/2 dx
= [x^3/6]|_0-2
= 8/6
= 4/3

example:
joint probability density function and the marginal probability density function
f(x,y) = {
	3/2, x^2 <= y <= 1, 0 < x < 1
	0,   otherwise
}
f_X(x) = {
	3/2(1 - x^2), 0 < x < 1
	0,            otherwise
}
find the expression for the conditional expected value E[Y|X = x] for 0 < x < 1
(3/2)/(3/2(1 - x^2))
= 6/6(1 - x^2)
= 1/(1 - x^2)
where x^2 <= y <= 1, 0 < x < 1
= ∫^1_x^2 1/(1 - x^2) dy
= 1/(1 - x^2) ∫^1_x^2 y dy
= 1/(1 - x^2) [y^2/2]|_(x^2)-1 dy
= 1/(1 - x^2) [1/2 - x^4/2]
= 1/(1 - x^2)(1 - x^4/2)
= 1(1 + x^2)/2
= 1 + x^2/2

===================================================

The conditional expected value of X given Y = y
E[X|Y = y] = μ_X|y
The conditional variance of X given Y = y
Var[X|Y] = E[(X - μ_X|y)^2|Y = y] 
the discrete case can be written as
Var[X|Y = y] = E[X^2|Y = y] - μ^2_X|y
where
E[X^2|Y = y] = ∫^∞_-∞ x^2 f_X|Y(x|y) dx
f_X|Y(x|y) is the conditional probability mass function of X given Y = y analogous to Var[X] for continuous X
when a particular y is selected the conditional variance of X given Y = y returns a real number, if y is not specified then Var[X|Y = y] gives us a function of y
theres an analogous definition for the conditional variance of Y given X = x
Var[Y|X = x] = E[(Y - μ_Y|x)^2|X = x]
= E[Y^2|X = x] - μ^2_Y|x
where
μ_Y|x = E[Y|X = x] is the conditional expected value of Y given X
E[Y^2|X = x] = ∫^∞_-∞ y^2 f_Y|X(y|x) dy
f_Y|X(y|x) is the conditional probability mass function of Y give X = x

Suppose that X and Y are continuous random variables it is known that the conditional probability density function f_Y|X(y|1)
f_Y|X(y|1) = {
	4y^3, 0 <= y <= 1
	0,    otherwise
}
μ_Y|1 = 4/5
what is the conditional Var[Y|X = 1]
∫^1_0 y^2 * 4y^3 dx
= ∫^1_0 4y^5 dy
= [2y^6/3]|_0-1
= 2/3
Var[Y|X = 1] = E[Y^2|X = 1] - μ^2_Y|1
= 2/3 - (4/5)^2
= 2/3 - 16/25
= 2/75

find the conditional variance Var[X|Y = -2] given that the conditional probability density function f_X|Y(x|-2)
f_X|Y(x|-2) = {
	3x^2, 0 <= x <= 1
	0,    otherwise
}
the corresponding expected value is μ_X|-2 = 3/4
= ∫^1_0 x^2 * 3x^2 dx
= ∫^1_0 3x^4 dx
= [3x^5/5]|_0-1
= 3/5 - 0
= 3/5
Var[X|Y = -2] = E[X^2|Y = -2] - μ^2_X|-2
= 3/5 - (3/4)^2
= 3/5 - 9/16
= 3/80

example:
find the conditional variance Var[X|Y = 2] given that the conditional probability density function f_X|Y(x|2)
f_X|Y(x|2) = {
	x/4, 1 <= x <= 3
	0,   otherwise
}
expected value is μ_X|2 = 13/6
= ∫^3_1 x^2 * x/4 dx
= ∫^3_1 x^3/4 dx
= [x^4/16]|_1-3
= 81/16 - 1/16
= 80/16
= 5
(13/6)^2
= 180/36 - 169/36
11/36

example:
find the conditional variance Var[Y|X = 2] given that the conditional probability density function f_Y|X(y|2) the expected value: μ_Y|2 = 0
f_Y|X(y|2) = {
	3/2(y2), -1 <= y <= 1
	0,       otherwise
}
= ∫^1_-1 y^2 * 3/2(y^2) dy
= 3/2 ∫^1_-1 (y^4) dy
= 3/2 [y^5/5]|_-1-1
= 3/2(1/5 - (-1/5))
= 3/2(2/5)
= 6/10
= 3/5 - (0)^2

Calculating a conditional variance given a conditional PDF using improper integrals
f_Y|X(y|3) = {
	3/y^4, y >= 1
	0,     otherwise
}
find the conditional variance Var[Y|X = 3] and the expected value is μ_Y|3 = 3/2
∫^∞_1 y^2 * 3/y^4 dy
= ∫^∞_1 3/y^2 dy
= ∫^∞_1 3y^-2 dy
= [3 * y^-1/-1]|_1-∞
= [-3/y]|_1-∞
= 0 - (-3)
= 3
Var[Y|X = 3] = E[Y^2|X = 3] - μ^2_Y|3
= 3 - (3/2)^2
= 3 - 9/4
= 3/4

example:
find the conditional variance Var[X|Y = -2] given that the conditional probability density function f_X|Y(x|-2) expected value μ_X|-2 = 4/3
f_X|Y(x|-2) = {
	4/x^5, x >= 1
	0,     otherwise
}
∫^∞_1 x^2 * 4/x^5 dx
= [-2/x^2]|_1-∞
= (0 - (-2))
= 2
2 - (4/3)^2
= 2 - 16/9
= 18/9 - 16/9
= 2/9

example:
find the conditional variance Var[Y|X = 3] given that the conditional probability density function f_Y|X(y|3) expected value μ_Y|3 = 5/3
f_Y|X(y|3) = {
	5/2y^(7/2), 1 <= y
	0,          otherwise
}
∫^∞_1 y^2 * 5/2y^(7/2) dy
= ∫^∞_1  5/2 y^(2 - 7/2) dy
= ∫^∞_1  5/2 y^(4/2 - 7/2) dy
= ∫^∞_1  5/2 y^(-3/2) dy
= ∫^∞_1  5/2 y^(-3/2) dy
= 5/2[y^(-3/2)]|_1-∞
= 5/2 [y^(-3/2 + 1)/-3/2 + 1]|_1-∞
= 5/2 [y^(-3/2 + 2/2)/-3/2 + 2/2]|_1-∞
= 5/2 [y^(-1/2)/-1/2]|_1-∞
= 5/2 [-2y^(-1/2)]|_1-∞
= 5/2 [-2y^(-1/2)]|_1-∞
= (0) - (-5/sqrt(1))
= 5
(5/3)^2
= 25/9
= 45/9 - 25/9
= 20/9

Calculating a conditional variance given joint and marginal density functions
f(x,y) = {
	2yln(x), 1 <= x <= e, 0 <= y <= 1
	0,       otherwise
}
f_X(x) = {
	ln(x), 1 <= x <= e
	0,	   otherwise
}
given the joint probability density function and the marginal probability density function. Find the conditional variance Var[Y|X = 1] the expected value is μ_Y|1 = 2/3
f_Y|X(y|x) = f(x,y)/f_X(x)
= 2yln(x)/ln(x)
= 2y
where 1 <= x <= e, 0 <= y <= 1
= ∫^1_0 y^2 * 2y dy
= ∫^1_0 2y^3 dy
= [y^4/2]|_0-1
= 1/2
Var[Y|X = 1] = E[Y^2|X = 1] - μ^2_Y|1
= 1/2 - (2/3)^2
= 1/2 - 4/9
= 1/18

example:
given the joint probability density function and the marginal probability density function
f(x,y) = {
	x + y, 0 <= x <= 1, 0 <= y <= 1
	0,     otherwise
}
f_Y(y) = {
	1/2 + y, 0 <= y <= 1
	0,       otherwise
}
find the conditional variance Var[X|Y = 1/2]
expected value is μ_X|1/2 = 7/12
= (x + y)/(1/2 + y)
= 2(x + y)/(1 + 2y)
= 2(x + (1/2))/(1 + 2(1/2))
= (2x + 1)/2
= x + 1/2
= ∫^1_0 x^2 * x + 1/2 dx
= ∫^1_0 (x^3 + x^2/2) dx
= [x^4/4 + x^2/6]|_0-1 + y
= (1/4 + 1/6) - (0)
= 5/12
(7/12)^2
= 5/12 - 49/144
= 11/144

example:
given the joint probability density function and the marginal probability density function
f(x,y) = {
	24y/7(1 + x), 0 <= x <= 1, x^2 <= y <= 1
	0,            otherwise
}
f_X(x) = {
	12(1 - x^4)/7(1 + x), 0 <= x <= 1
	0,                    otherwise
}
find the conditional variance Var[Y|X = 0] expected value is μ_Y|0 = 2/3
(24y/7(1 + x))/(12(1 - x^4)/7(1 + x))
= 24y/12(1 - x^4)
Var[Y|X = 0]
= 24y/12
= ∫^1_x^2 y^2 * 24y/12 dy
= ∫^1_x^2 24y^3/12 dy
= [24y^4/48]|_x^2-1
= (1/2 - 24x^6/48)
(2/3)^2 = 4/9
= 9/18 - 8/18
= 1/18
(lower limit bound should be 0 instead of x^2 answer is correct tho)
(i.e. E[Y^2|X = 0] when setting up the intgration the upper bound as always is given in the definition of f(x,y))

===================================================

The Rule of the lazy statistician for two random variables
If X is a discrete random variable with support S_X and Y = r(X) for some function r we can compute the expected value of Y using the rule of the lazy statistician
E[Y] = Σ_x∈S_X(r(t)f_X(x))
suppose that W = r(X,Y) is function of two discrete random variables X and Y with supports S_X and S_Y, the rule of the lazy statistician for two random variables
E[W] = E[r(X,Y)] = Σ_(x,y)∈S(r(x,y)f(x,y))
where f(x,y) is the joint probability mass function of X and Y
this is helpful for two reasons
it allows us to compute E[W] without needed to find the PMF of W
second we can use this to prove many useful properties of random variables
E[X +/- Y] = E[X] +/- E[Y]

Applying the rule to discrete random variables
The joint probability mass function f(x,y) for discrete random variables X and Y find E[X + 2Y]
f   |Y=1|Y=3|
X=2 |0.1|0.6|
X=3 | 0 |0.3|
-------------
r(x,y) = x + 2y
= (2 + 2*1) * f(2,1) + (2 + 2*3) + f(2,3) + (3 + 2*1) * f(3,1) + (3 + 2*3) * f(3,3)
= 4 * f(2,1) + 8 * f(2,3) + 5 * f(3,1) + 9 * f(3,3)
= 4 * 0.1 + 8 * 0.6 + 5 * 0 + 9 * 0.3
= 0.4 + 4.8 + 0 + 2.7
= 7.9

example:
given the joint PMF for discrete, find E[sin(X + Y)]
f   |Y=0|Y=pi/2|
X=0 |0.1|   0.5|
X=pi|0.4|     0|
----------------
r(x,y) = sin(x + y)
= (sin(0 + 0)*0.1 + sin(0 + pi/2)*0.5 + sin(pi + 0)*0.4 + sin(pi + pi/2)*0)
= 0.5

example:
given the joint PMF for discrete, find E[Xe^Y]
f  |Y=0|Y=ln(3)|
X=1|0.3|    0.1|
X=2|0.5|    0.1|
----------------
= (e^0)*0.3 + e^ln(3)*0.1 + 2e^0*0.5 + 2e^ln(3)*0.1
= 0.3 + 0.3 + 1 + 0.6
= 2.2

Applying the rule to continuous random variables
find E[2X + Y] given the joint probability density function
f(x,y) = {
	x, 0 <= x <= 1, 0 <= y <= 2
	0, otherwise
}
the lazy statistician states that
E[r(X,Y)] = ∫∫_R^2 r(x,y)f(x,y) dxdy
in our case: r(x,y) = 2x + y
D = {(x,y) : 0 <= x <= 1, 0 <= y <= 2}
E[2X + Y] = ∫∫_D (2x + y) * f(x,y) dxdy
= ∫^2_0 ∫^1_0 (2x + y) * x dxdy
= ∫^2_0 [∫^1_0 (2x^2 + xy)dx] dy
= ∫^2_0 [2x^3/3 + x^2y/2]|_0-1 dy
= ∫^2_0 (2/3 + y/2) dy
= [2/3(y) + y^2/4]|_0-2
= 7/3

example:
find E[XY^2] given the joint probability density function
f(x,y) = {
	1, 0 <= x <= 1, 0 <= y <= 1
	0, otherwise
}
∫^1_0 ∫^1_0 (xy^2) * 1 dxdy
= ∫^1_0 ∫^1_0 (xy^2) dxdy
= ∫^1_0 [∫^1_0 (xy^2) dx] dy
= ∫^1_0 [(x^2y^2/2)]|_0-1 dy
= ∫^1_0 (y^2/2) dy
= [y^3/6]|_0-1
= 1/6

example:
find E[X^3(Y + 1)] given the joint probability density function
f(x,y) = {
	y/x^2, 1 <= x <= 3, 1 <= y <= 2
	0,     otherwise
}
∫^2_1 ∫^3_1 (x^3(y + 1)) * (y/x^2) dxdy
= ∫^2_1 ∫^3_1 (x^3(y + 1)) * (y/x^2) dxdy
= ∫^2_1 ∫^3_1 xy^2 + xy dxdy
= ∫^2_1 [∫^3_1 xy^2 + xy dx] dy
= ∫^2_1 [x^2y^2/2 + x^2y/2]|_1-3 dy
= ∫^2_1 (9y^2/2 + 9y/2) - (y^2/2 + y/2) dy
= [(9y^3/6 + 9y^2/4) - (y^3/6 + y^2/4)]|_1-2
= [(72/6 + 36/4) - (8/6 + 4/4)]|_1-2
= [(144/12 + 108/12) - (16/12 + 12/12)]|_1-2
= [(234/12) - (28/12) - (9/6 + 9/4) - (1/6 + 1/4)]
= [- (9/6 + 9/4) - (1/6 + 1/4)]
= (206/12) - (18/12 + 27/12) - (5/12)
= 206/12 + 9/12 - 5/12
= 206/12 + 4/12
= 210/12
= 105/6
= 35/2
(this I got wrong, correct steps:)
(split up the integral)
= ∫^2_1 ∫^3_1 (x^3(y + 1)) * (y/x^2) dxdy
= ∫^2_1 y(y + 1) ∫^3_1 x dxdy
= ∫^2_1 y(y + 1) [x^2/2]|_1-3 dy
= ∫^2_1 4y(y + 1) dy
= [4y^3/3 + 2y^2]|_1-2
= (32/3 + 8) - (4/3 + 2)
= 46/3

Applying the rule to continuous random variables: non-rectangular
find E[e^(X + Y)] given joint PDF
f(x,y) = {
	1/2, 0 <= x <= 2, 0 <= y <= x
	0,   otherwise
}
r(x,y) = e^(x+y)
D = {(x,y) : 0 <= x <= 2, 0 <= y <= x}
= ∫^2_0 ∫^x_0 e^(x+y) * 1/2 dydx
= 1/2 ∫^2_0 ∫^x_0 e^(x+y) dydx
= 1/2 ∫^2_0 [e^(x+y)]|_0-x dx
= 1/2 ∫^2_0 (e^(2x) - e^x) dx
= 1/2 [1/2e^(2x) - e^x]|_0-2
= 1/2[(1/2e^(4) - e^2) - (1/2 - 1)]
= 1/2[(1/2e^4 - e^2 + 1/2)]
= 1/2[(1/2e^4 - 2/2e^2 + 1/2)]
= 1/4(e^4 - 2e^2 + 1)
= 1/4(e^2 - 1)^2

example:
find E[sin(X + Y)] given the joint probability density function
f(x,y) = {
	8/pi^2, 0 <= x <= pi/2, 0 <= y <= x
	0,      otherwise
}
r(x,y) = sin(x + y)
= ∫^pi/2_0 ∫^x_0 sin(x + y) * 8/pi^2 dydx
= 8/pi^2 ∫^pi/2_0 [∫^x_0 sin(x + y) dy] dx
= 8/pi^2 ∫^pi/2_0 [sin(x + y)]|_0-x dx
= 8/pi^2 ∫^pi/2_0 [-cos(x + y)]|_0-x dx
= 8/pi^2 ∫^pi/2_0 (-cos(2x) - (-cos(x))) dx
= 8/pi^2 ∫^pi/2_0 (cos(x) - cos(2x)) dx
= 8/pi^2 [sin(x) - 1/2sin(2x)]|_0-pi/2
= 8/pi^2 [sin(pi/2) - 1/2sin(pi) - 0]
= 8/pi^2(1)

example:
find E[XY^3] given the joint probability density function
f(x,y) = {
	2/9y^2, 0 <= x <= y^3, 0 <= y <= 3
	0,      otherwise
}
r(x,y) = xy^3
= ∫^y^3_0 ∫^3_0 xy^3 * 2/9y^2 dydx
= 2/9 [x^2/2]|_0-y^3 ∫^3_0 y dy 
= 2/9 (y^6/2) ∫^3_0 y dy 
= 2/9 ∫^3_0 y^7/2 dy 
= 2/9 [y^8/16]|_0-3
= 2/9(6561/16)
= 13122/144
= 6561/72
= 2187/24
= 729/8

Proof of the linearity of expectation
lets prove that for continuous random variables X and Y
E[X + Y] = E[X] + E[Y]
r(x,y) = x + y, the rule of the lazy statistician
E[X + Y] = E[r(X,Y)]
= ∫∫_R^2 (x + y)f(x,y) dxdy
= ∫∫_R^2 xf(x,y) dxdy + ∫∫_R^2 yf(x,y) dxdy
= ∫^∞_-∞ ∫^∞_-∞ xf(x,y) dxdy + ∫^∞_-∞ ∫^∞_-∞ yf(x,y) dxdy
since the integration domain is rectangular we can swap the order of integration in the first integral
E[X + Y] = ∫^∞_-∞ x ∫^∞_-∞ f(x,y) + ∫^∞_-∞ y ∫^∞_-∞ f(x,y) dxdy
= ∫^∞_-∞ xf_X(x) dx + ∫^∞_-∞ yf_Y(y) dy
= E[X] + E[Y]
also this is obvious
f_X(x) = ∫^∞_-∞ f(x,y) dy
f_Y(y) = ∫^∞_-∞ f(x,y) dx
the proof for discrete random variables is similar

Proof of the product rule for independent random variables
E[XY] = E[X] * E[Y]
r(x,y) = xy, by rule of the lazy statistician
E[XY] = E[r(X,Y)]
= ∫∫_R^2 xyf(x,y) dxdy
since X and Y are independent (by assumption) f(x,y) = f_X(x) * f_Y(y)
= ∫∫_R^2 xyf_X(x)f_Y(y) dxdy
= ∫^∞_-∞ ∫^∞_-∞ xyf_X(x)f_Y(y) dxdy
= ∫^∞_-∞ xf_X(x) dx * ∫^∞_-∞ yf_Y(y) dy
= E[X] * E[Y]
again proof for discrete is similar

===================================================

The covariance of two random variables
We often want to measure the strength of the linear dependence between two random variables one measure that we use for this purpose is covariance
The covariance between two discrete random variables X and Y
Cov[X,Y] = E[(X - E[X])(Y - E[Y])]
(X - E[X]) is the deviation of X from its mean
(Y - E[Y]) is the deviation of Y from its mean
the covariance measures the expected value of the product of these deviations
convariance can be expressed as
Cov[X,Y] = Σ_(x,y)∈S(x - E[X])(y - E[Y])f(x,y)
f(x,y) is the joint PMF of X and Y, and S is the joint support
A positive covariance means that if X increases then Y increase. X and Y are positively correlated
A negative means that if X increases then Y is likely to decrease we say that X and Y are negatively correlated
if X and Y and independent then
Cov[X,Y] = 0
the reverse does not always hold, two random variables can have zero covariance yet be dependent. This is because covariance measures linear dependence only
The covariance of a random variable with itself is its variance
Cov[X,X] = E[(X - E[X])(X - E[X])]
= E[(X - E[X])^2]
= Var[X]
besides checking if two random variables are positively or negatively correlated, the covariance by itself does not tell us very much, if
Cov[X1,Y1] > Cov[X2,Y2] > 0
this does not mean that the dependence between X1 and Y1 is somehow stronger than the dependence between X2 and Y2 this is because the covariance does not take the variance of the individual random variables into account
the correlation coefficient DOES consider the variance of the individual random variables and so can be used to compare the relative strength of the linear dependence between two pairs of random variables

The joint probability mass function f(x,y) for the discrete random variables X and Y with joint support S = S_X x S_Y
f  | Y=2| Y=5|
X=1|0.15|0.25|
X=4|0.45|0.15|
--------------
find an expression for the covariance in sigma notation
The covariance of two discrete random variables X and Y with joint support S
Cov[X,Y] = Σ_(x,y)∈S(x - E[X])(y - E[Y])f(x,y)
first find the marginal mass function for X and Y
f  | Y=2| Y=5|f_X|
X=1|0.15|0.25|0.4|
X=4|0.45|0.15|0.6|
f_Y| 0.6| 0.4| 1 |
------------------
now compute E[X] and E[Y]
E[X] = Σ_x∈S_X(x * f_X(x))
= 1 * 0.4 + 4 * 0.6
= 0.4 + 2.4
= 2.8
E[Y] = Σ_y∈S_Y(y * f_Y(y))
= 2 * 0.6 + 5 * 0.4
= 1.2 + 2
= 3.2
applying the covariance formula
Cov[X,Y] = Σ_(x,y)∈S(x - E[X])(y - E[Y])f(x,y)
= Σ_(x,y)∈S(x - 2.8)(y - 3.2)f(x,y)

example:
f  | Y=2| Y=5|
X=1|0.12| 0.4|
X=3|0.45| 0.2|
--------------
marginal mass
f  | Y=2| Y=5| f_X|
X=1|0.12| 0.4|0.52|
X=3|0.28| 0.2|0.48|
f_Y| 0.4| 0.6|  1 |
-------------------
= 1*0.52 + 3*0.48
E[X] = 1.96
= 2*0.4 + 5*0.6
E[Y] = 3.8
Cov[X,Y] = Σ_(x,y)∈S(x - 1.96)(y - 3.8)f(x,y)

example:
the joint probability mass function f(x,y) for the discrete random variables X and Y with joint support S = S_X x S_Y
f  | Y=0| Y=1|
X=1|0.15|0.25|
X=2|0.05|0.15|
X=3|0.25|0.15|
--------------
marginal mass function
f  | Y=0| Y=1|f_X|
X=1|0.15|0.25|0.4|
X=2|0.05|0.15|0.2|
X=3|0.25|0.15|0.4|
f_Y|0.45|0.55| 1 |
------------------
E[X] = 1*0.4 + 2*0.2 + 3*0.4
= 2
E[Y] = 0*0.45 + 1*0.55
= 0.55
Cov[X,Y] = Σ_(x,y)∈S(x - 2)(y - 0.55)f(x,y)

example:
the joint probability mass function f(x,y) for the discrete random variables X and Y
f  |Y=0|Y=1|
X=1|0.1|0.5|
X=2|  0|0.4|
------------
marginal mass function for X and Y
f  |Y=0|Y=1|f_X|
X=1|0.1|0.5|0.6|
X=2|  0|0.4|0.4|
f_Y|0.1|0.9| 1 |
----------------
E[X] = 1*0.6 + 2*0.4
= 1.4
E[Y] = 0*0.1 + 1*0.9
= 0.9
Cov[X,Y] = Σ_(x,y)∈S(x - 1.4)(y - 0.9)f(x,y)
= (1 - 1.4)(0 - 0.9)f(1,0)
+ (1 - 1.4)(1 - 0.9)f(1,1)
+ (2 - 1.4)(0 - 0.9)f(2,0)
+ (2 - 1.4)(1 - 0.9)f(2,1)
= (-0.4)(-0.9)(0.1)
+ (-0.4)(0.1)(0.5)
+ (0.6)(-0.9)(0)
+ (0.6)(0.1)(0.4)
= 0.04

example:
the joint probability mass function f(x,y) for the discrete random variables X and Y
f  | Y=0| Y=1|
X=0|0.28|0.12|
X=1|0.42|0.18|
--------------
calculate the covariance of X and Y
Hint: it can be shown that P(X = x,Y = y) = P(X = x) * P(Y = y) for all possible values of x and y
marginal mass function for X and Y
f  | Y=0| Y=1|f_X|
X=0|0.28|0.12|0.4|
X=1|0.42|0.18|0.6|
f_Y| 0.7| 0.3| 1 |
------------------
E[X] = 0*0.4 + 1*0.6
= 0.6
E[Y] = 0*0.7 + 1*0.3
= 0.3
Cov[X,Y] = Σ_(x,y)∈S(x - 0.6)(y - 0.3)f(x,y)
= (-0.6)(-0.3)(0.28)
+ (1 - 0.6)(-0.3)(0.42) = (0.4)(-0.3)(0.42)
+ (-0.6)(1 - 0.3)(0.12) = (-0.6)(0.7)(0.12)
+ (1 - 0.6)(1 - 0.3)(0.18) = (0.4)(0.7)(0.18)
= 0.0504 - 0.0504 - 0.0504 + 0.0504
(we should've used the hint since X and Y are independent we can assume Cov(X,Y) = 0)

example:
the joint probability mass function f(x,y) for the discrete random variables X and Y
f  |Y=1|Y=2|f_X|
X=0|0.1|0.1|0.2|
X=1|0.2|0.6|0.8|
f_Y|0.3|0.7| 1 |
----------------
calculate the covariance of X and Y
E[X] = 0*0.2 + 1*0.8
= 0.8
E[Y] = 1*0.3 + 2*0.7
= 1.7
Cov[X,Y] = Σ_(x,y)∈S(x - 0.8)(y - 1.7)f(x,y)
(0 - 0.8)(1 - 1.7)f(x,y) = (-0.8)(-0.7)(0.1)
(1 - 0.8)(1 - 1.7)f(x,y) = (0.2)(-0.7)(0.2)
(0 - 0.8)(2 - 1.7)f(x,y) = (-0.8)(0.3)(0.1)
(1 - 0.8)(2 - 1.7)f(x,y) = (0.2)(0.3)(0.6)
= 0.056 - 0.028 - 0.024 + 0.036
= 0.04

A formula for quickly computing covariance
(because why not? Its a lot of work)
there is a formula that makes this easier
Cov[X,Y] = E[XY] - E[X] * E[Y]

Finding the covariance using our new knowledge
the joint probability mass function f(x,y) for the discrete random variables X and Y
f  |Y=2|Y=3|
X=1|0.2|0.2|
X=4|0.4|0.2|
------------
if E[X] = 2.8, E[Y] = 2.4, calculate covariance
use the rule of the lazy statistician to find E[XY]
E[XY] = Σ_(x,y)∈S(xy * f(x,y))
= (1)(2)(0.2) + (1)(3)(0.2) + (4)(2)(0.4) + (4)(3)(0.2)
= 6.6
using the covariance formula
Cov[X,Y] = E[XY] - E[X] * E[Y]
= 6.6 - (2.8)(2.4)
= -0.12

example:
if E[X] = 20, E[Y] = 2, and E[XY] = 44.8 for two random variables X and Y calculate the covariance of X and Y
44.8 - (20)(2)
= 4.8

example:
the joint probability mass function f(x,y) for the discrete random variables X and Y
f  |Y=2| Y=3| Y=4|
X=1|0.1|0.05|0.15|
X=2|0.1|0.35|0.25|
------------------
if E[X] = 1.7 and E[Y] = 3.2, calculate the covariance of X and Y
(1)(2)(0.1) + (1)(3)(0.05) + (1)(4)(0.15) + (2)(2)(0.1) + (2)(3)(0.35) + (2)(4)(0.25)
= 5.45
5.45 - (1.7)(3.2)
= 0.01

Covariance for continuous random variables
Cov[X,Y] = ∫∫_R^2 (x - E[X])(y - E[Y])f(x,y) dxdy
just like discrete variables, its easier using 
Cov[X,Y] = E[XY] - E[X] * E[Y]
joint probability function
f(x,y) = {
	1/2(x + y/2), 0 <= x <= 1, 0 <= y <= 2
	0,            otherwise
}
to compute the covariance of this distribution we first use the rule of the lazy statistician to find E[XY]
E[XY] = ∫^2_0 ∫^1_0 xyf(x,y) dxdy
= ∫^2_0 ∫^1_0 xy * 1/2(x + y/2) dxdy
= ∫^2_0 1/2(y) ∫^1_0 (x^2 + 1/2xy) dxdy
= ∫^2_0 1/2(y) [x^2 + 1/2xy]|_0-1 dy
= ∫^2_0 1/2(y) [x^3/3 + 1/4x^2y]|_0-1 dy
= ∫^2_0 1/2(y)(1/3 + 1/4y) dy
= ∫^2_0 (1/6y + 1/8y^2) dy
= [1/12y^2 + 1/24y^3]|_0-2
= 1/12 * 2^2 + 1/24 * 2^3
= 1/3 + 1/3
= 2/3
E[X] = 7/12, E[Y] = 7/6
using the covariance formula
= 2/3 - (7/12)(7/6)
= -1/72

Joint probability density function
f(x,y) = {
	3/5(sqrt(x)y), 0 <= x <= 1, 2 <= y <= 3
	0,             otherwise
}
find the covariance of X and Y
the joint support of X and Y
S = [0,1] x [2,3]
f(x,y) = g(x) * h(x)
where g(x) = 3/5(sqrt(x)) and h(y) = y
X and Y are independent then Cov[X,Y] = 0

example:
joint PDF
f(x,y) = {
	e^(-x-y), x > 0, y > 0
	0,        otherwise
}
find the covariance of X and Y
S = (0,∞) x (0, ∞)
we can write
f(x,y) = g(x) * h(y)
where e^-x and h(y) = e^-y
X and Y are independent and Cov[X,Y] = 0

The joint probability density function
f(x,y) = {
	x + 3/2(y^2), 0 <= x <= 1, 0 <= y <= 1
	0,            otherwise
}
given E[X] = 7/12 and E[Y] = 5/8 calculate the covariance
= ∫^1_0 ∫^1_0 x^2y + 3/2(xy^3)  dxdy
(breakup the x's)
∫^1_0 x^2y dx + 3/2(y^3)∫^1_0 x dx
∫^1_0 y[x^3/3]|_0-1 dx = y/3
∫^1_0 x dx = [x^2/2]|_0-1 = 1/2
= 3/2(y^3) * 1/2 = 3/4y^3
combining
= y/3 + 3/4(y^3)
outer integral
= ∫^1_0 (y/3 + 3/4(y^3)) dy
= ∫^1_0 y/3 dy = 1/3[y^2/2]|_0-1 = 1/3 * 1/2 = 1/6
= ∫^1_0 3/4(y^3) dy = 3/4[y^4/4]|_0-1 = 3/4 * 1/4 = 3/16
= 1/6 + 3/16
= 8/48 + 9/48
= 17/48
= 17/48 - (7/12)(5/8)
= 17/48 - 35/96
= 34/96 - 35/96
= -1/96

Proof of the covariance formula
Cov[X,Y] = E[XY] - E[X] * E[Y]
definition
Cov[X,Y] = E[(X - E[X])(Y - E[Y])]
expanding
Cov[X,Y] = E[XY - X * E[Y] - E[X] * Y + E[X] * E[Y]]
if a and b are constants
E[a] = a, E[aX + bY] = a * E[X] + b * E[Y]
since E[X] and E[Y] are both contants
Cov[X,Y] = E[XY - X * E[Y] - E[X] * Y + E[X] * E[Y]]
= E[XY] - E[X * E[Y]] - E[E[X] * Y] + E[E[X] * E[Y]]
= E[XY] - E[X] * E[Y] - E[X] * E[Y] + E[X] * E[Y]
= E[XY] - E[X] * E[Y]

===================================================

Given two random variables X and Y the variance of the sum or difference of these variables
Var[X +/- Y] = Var[X] + Var[Y] +/- 2Cov[X,Y]
if X and Y are independent then Cov[X,Y] = 0, we have
Var[X +/- Y] = Var[X] + Var[Y]

For two random variables X and Y, Var[X] = 3, Var[Y] = 0.24, and Cov[X,Y] = 0.2 determine Var[X - Y]
Var[X +/- Y] = Var[X] + Var[Y] +/- 2Cov[X,Y]
Var[X - Y] = 3 + 0.24 - 2 * 0.2 = 2.84

example:
Var[X] = 7, Var[Y] = 5 and Cov[X,Y] = 3 find Var[X + Y]
Var[X + Y] = 7+5 + 2*3 = 18

example:
Var[X] = 7, Var[Y] = 5 and Cov[X,Y] = 3 find Var[X - Y]
Var[X - Y] = 7+5 - 2*3 = 6

calculating the variance of a sum of scaled random variables
Var[X] = 5, Var[Y] = 2, and Cov[X,Y] = 3
find Var[2X - 5Y]
Var[aX + bY] = a^2Var[X] + b^2Var[Y] + 2abCov[X,Y]
Var[2X - 5Y] = 2^2*5 + (-5)^2*2 + 2*2*(-5)*3
= 20 + 50 - 60
= 10

example:
Var[X] = 3, Var[Y] = 6 and Cov[X,Y] = 2 find Var[4X + 2Y]
Var[4X + 2Y] = 4^2*3 + 2^2*6 + 2*4*2*2
= 48 + 24 + 32
= 104

example:
Var[X] = 3, Var[Y] = 6 and Cov[X,Y] = 2 find Var[4X + 2Y]
Var[4X + 2Y] = 4^2*3 + 2^2*6 + 2*4*2*2
= 48 + 24 + 32
= 104

example:
Var[X] = 5, Var[Y] = 11, Cov[X,Y] = 4, find Var[2X - 3Y]
Var[2X - 3Y] = 2^2*5 + (-3)^2*11 + 2*2*(-3)*4
= 20 + 99 - 48
= 71

Calculating a combined variance given some moments
PDF:
f  |Y=0|Y=2|
X=0|0.1|0.3|
X=3|0.4|0.2|
------------
find Var[5X + 2Y]
Var[X] = E[X^2] - E[X]^2
Cov[X,Y] = E[XY] - E[X] * E[Y]
first marginal mass
f  |Y=0|Y=2|f_X|
X=0|0.1|0.3|0.4|
X=3|0.4|0.2|0.6|
f_Y|0.5|0.5| 1 |
----------------
now we can find Var[X]
E[X] = 0*0.4 + 3*0.6
= 1.8
E[X^2] = 0^2*0.4 + 3^2*0.6
= 5.4
Var[X] = E[X^2] - E[X]^2
= 5.4 - 1.8^2
= 2.16
E[Y] = 0*0.5 + 2*0.5
= 1
E[Y^2] = 0^2*0.5 + 2^2*0.5
= 2
Var[Y] = E[Y^2] - E[Y]^2
= 2 - 1^2
= 1
now lets find Cov[X,Y], first E[XY] using rule of the lazy statistician
= 0*0*0.1 + 0*2*0.3 + 3*0*0.4 + 3*2*0.2
= 1.2
covariance formula
Cov[X,Y] = E[XY] - E[X] * E[Y]
= 1.2 - 1.8 * 1
= -0.6
Var[5X + 2Y] = 5^2*2.16 + 2^2*1 + 2*5*2*(-0.6)
= 54 + 4 - 12
= 46

example:
E[X] = 2, E[Y] = 4, mixed moment E[XY] = 10, and second raw moments E[X^2] = 9 and E[Y^2] = 20 find the variance of 2x - 5Y
Var[X] = E[X^2] - E[X]^2
= 9 - 2^2
= 5
Var[Y] = E[Y^2] - E[Y]^2
= 20 - 4^2
= 4
Cov[X,Y] = E[XY] - E[X] * E[Y]
= 10 - 2 * 4
= 2
[2x - 5Y] = 2^2*5 + (-5)^2*4 + 2*2*(-5)*2
= 20 + 100 - 40
= 80

example:
PDF:
f  |Y=1|Y=2|
X=0|0.1|0.1|
X=1|0.3|0.5|
------------
find Var[2X + 3Y]
first marginal mass
f  |Y=1|Y=2|f_X|
X=0|0.1|0.1|0.2|
X=1|0.3|0.5|0.8|
f_Y|0.4|0.6| 1 |
----------------
E[X] = 0*0.2 + 1*0.8
= 0.8
E[X^2] = 0^2*0.2 + 1^2*0.8
= 0.8
Var[X] = E[X^2] - E[X]^2
0.8 - (0.8)^2
= 0.16
E[Y] = 1*0.4 + 2*0.6
= 1.6
E[Y^2] = 1^2*0.4 + 2^2*0.6
= 2.8
Var[Y] = E[Y^2] - E[Y]^2
2.8 - (1.6)^2
= 0.24
lazy statistician (E[XY])
= 0*1*0.1 + 0*2*0.1 + 1*1*0.3 + 1*2*0.5
= 0.3 + 1
= 1.3
Cov[X,Y] = E[XY] - E[X] * E[Y]
= 1.3 - 0.8*1.6
Var[2X + 3Y] = 2^2*0.16 + 3^2*(0.24) + 2*2*3*0.02
= 0.64 + 2.16 + 0.24
= 3.04

generalizing to multiple random variables
variance of a sum of n random variables
Var[Σ^n_i=1(aiXi)] = Σ^n_i=1(ai^2Var[Xi]) + 2Σ^n_i=1<j(aiaj)Cov[Xi,Xj]

For three random variables X1, X2, and X3 we have the following variances and covariances
Var[X1] = 1, Var[X2] = 3, Var[X3] = 8
Cov[X1,X2] = 2, Cov[X1,X3] = 4, Cov[X2,X3] = -5
find Var[5X1 + 2X2 + 3X3]
for n = 3
Var[a1X1 + a2X2 + a3X3] = a1^2Var[X1] + a2^2Var[X2] + a3^2Var[3]
+ 2a1a2Cov[X1,X2] + 2a1a3Cov[X1,X3] + 2a2a3Cov[X2,X3]
substituting
Var[5X1 + 2X2 + 3X3] = 5^2*1 + 2^2*3 + 3^2*8
+ 2*5*2*2 + 2*5*3*4 + 2*2*3*(-5)
= 25 + 12 + 72 + 40 + 120 - 60
= 209

example:
Var[X1] = 5, Var[X2] = 10, Var[X3] = 7
Cov[X1,X2] = 6, Cov[X1,X3] = 3, Cov[X2,X3] = 5
find Var[X1 + X2 + X3]
= 5 + 10 + 7
+ 2*1*1*6 + 2*1*1*3 + 2*1*1*5
= 50

example:
Var[X1] = 5, Var[X2] = 4, Var[X3] = 2
Cov[X1,X2] = 3, Cov[X1,X3] = 1, Cov[X2,X3] = 2
find Var[3X1 + 2X2 + 4X3]
= 3^2*5 + 2^2*4 + 4^2*2
+ 2*3*2*3 + 2*3*4*1 + 2*2*4*2
= 93 + 36 + 24 + 32
= 185

Proof of the sum formula
Var[X + Y] = Var[X] + Var[Y] + 2Cov[X,Y]
using the linearity of epxectation
Var[X + Y] = E[((X + Y) - E[X + Y])^2]
= E[((X - E[X]) + (Y - E[Y]))^2]
= E[(X - E[X])^2 + (Y - E[Y])^2 + 2(X - E[X])(Y - E[Y])]
distributing the expected value, using the fact that E[aX] = a*E[X]
Var[X + Y] = E[(X - E[X])^2 + (Y - E[Y])^2 + 2(X - E[X])(Y - E[Y])]
= E[(X - E[X])^2] + E[(Y - E[Y])^2] + 2*E[(X - E[X])(Y - E[Y])]
= Var[X] + Var[Y] + 2Cov[X,Y]

===================================================

The correlation coefficient of X and Y also known as the Pearson correlation coefficient is denoted ρ ("rho")
ρ = Cov[X,Y]/SD[X]*SD[Y]
The correlation coefficient satisfies
-1 <= ρ <= 1
if X and Y are independent random variables then
ρ = 0
the reverse does not always hold, ρ = 0 does not imply that X and Y are independent
unlike the covariance, ρ is dimensionless quantity, this means we'll get the same answer regardless of which units X and Y are measured
also unlike covariance ρ tells is the strength of the correlation between X and Y we typically have the following rule of thumb:
if |ρ| > 0.7 the correlation is strong and the data generated by our two random variables closely follow a straight line
if 0.3 <= |ρ| <= 0.7 the correlation is weak the data generated by our random variables follow a straight line but many are far from the line
if |ρ| < 0.3 then the correlation is negligible no (linear) correlation exists between the data points generated by the random variables.

finding a correlation coefficient given variances and covariance
the variances of the random variables X and Y are 25 and 36 given that Cov[X,Y] = 18 find the correlation coefficient of X and Y
correlation coefficient
ρ = Cov[X,Y]/SD[X]*SD[Y]
Cov[X,Y] is the covariance
SD[X] and SD[Y] are the standard deviations
SD[X] = sqrt(Var[X]) = sqrt(25) = 5
SD[Y] = sqrt(Var[Y]) = sqrt(36) = 6
ρ = 18/5*6
= 3/5

example:
Cov[X,Y] = 5, SD[X] = 2, SD[Y] = 4, find the correlation coefficient
ρ = 5/2*4 = 5/8

example:
random variables of X = 3, Y = 12, given Cov[X,Y] = 3 find the correlation coefficient
sqrt(3), sqrt(12)
= 3/2sqrt(3)sqrt(3)
= 3/6
= 1/2

finding the correlation coefficient of two discrete random variables
joint PMF
f(x,y)|Y=0|Y=1|f_X|
X=0   |0.1|0.5|0.6|
X=1   |0.3|0.1|0.4|
f_Y   |0.4|0.6| 1 |
-------------------
find the correlation coefficient of X and Y
E[X] = 0.4, E[Y] = 0.6, SD[X] ~= 0.4899, SD[Y] ~= 0.4899
E[XY] = 0*0*0.1 + 0*1*0.5 + 1*0*0.3 + 1*1*0.1
= 0.1
Cov[X,Y] = E[XY] - E[X] * E[Y]
= 0.1 - (0.4)(0.6)
= -0.14
~= -0.14/0.4899*0.4899
~= -0.58

example:
joint PMF
f(x,y)|Y=-1|Y=1|f_X|
X=0   | 0.2|0.5|0.7|
X=1   | 0.2|0.1|0.3|
f_Y   | 0.4|0.6| 1 |
--------------------
E[X] = 0.3, E[Y] = 0.2, SD[X] ~= 0.4583, SD[Y] ~= 0.9798
find the correlation coefficient
E[XY] = 0*(-1)*0.2 + 0*1*0.5 + 1*(-1)*0.2 + 1*1*0.1
= -0.2 + 0.1
= -0.1
Cov[X,Y] = E[XY] - E[X] * E[Y]
-0.1 - 0.3 * 0.2
= -0.16
~= -0.16/0.4583*0.9798
~= -0.36

example:
joint PMF
f(x,y)|Y=0|Y=1|Y=2|f_X|
X=0   |0.1|0.1|0.2|0.4|
X=1   |0.3|0.1|0.2|0.6|
f_Y   |0.4|0.2|0.4| 1 |
-----------------------
E[X] = 0.6, E[Y] = 1, SD[X] ~= 0.4899, SD[Y] ~= 0.8944
find the correlation coefficient
E[XY] = 0*0*0.1 + 0*1*0.1 + 0*2*0.2 + 0*1*0.3 + 1*1*0.1 + 1*2*0.2
= 0.1 + 0.4
= 0.5
Cov[X,Y] = E[XY] - E[X] * E[Y]
0.5 - 0.6 * 1
= -0.1
~= -0.1/0.4899*0.8944
~= -0.23

finding the correlation coefficient of two continuous random variables
joint PDF:
f(x,y) = {
	2(x + y), 0 <= y <= x <= 1
	0,        otherwise
}
given:
E[X] = 3/4, E[Y] = 5/12, SD[X] ~= 0.1936, SD[Y] ~= 0.2444
find the correlation coefficient of X and Y
ρ = Cov[X,Y]/SD[X]*SD[Y]
first apply the rule of the lazy statistician
E[XY] = ∫∫_R^2 xyf(x,y) dxdy
= ∫^1_0 [∫^x_0 xy * 2(x + y) dy] dx
= ∫^1_0 [∫^x_0 2x^2y + 2xy^2 dy] dx
= ∫^1_0 [x^2y^2 + 2xy^3/3]|_0-x dx
= ∫^1_0 (x^4 + 2x^4/3) dx
= ∫^1_0 5/3(x^4) dx
= [x^5/3]|_0-1
= 1/3
then compute the covariance
= 1/3 - (3/4)(5/12)
= 1/48
correlation coefficient
ρ = Cov[X,Y]/SD[X]*SD[Y]
~= (1/48)/(0.1936)*(0.2444)
~= 0.44

example:
joint PDF:
f(x,y) = {
	3/11(x + y^2), 0 <= x <= 1, 0 <= y <= 2
	0,             otherwise
}
given:
E[X] = 6/11, E[Y] = 15/11, SD[X] ~= 0.2851, SD[Y] ~= 0.4996
find the correlation coefficient
= ∫^1_0 [∫^2_0 xy * 3/11(x + y^2) dy] dx
= 3/11 ∫^1_0 [∫^2_0 (x^2y + xy^3) dy] dx
= 3/11 ∫^1_0 [x^2y^2/2 + xy^4/4]|_0-2 dx
= 3/11 ∫^1_0 (2x^2 + 4x) dx
= 3/11 [2x^3/3 + 2x^2]|_0-1 dx
= 3/11 (2/3 + 2)
= 8/11
covariance
= 8/11 - (6/11)(15/11)
= -2/121
~= (-2/121)/(0.2851)*(0.4996)
~= -0.12

example:
joint PDF:
f(x,y) = {
	3/10(xy^3 + 2), 0 <= 2x <= y <= 2
	0,              otherwise
}
given:
E[X] = 3/7, E[Y] = 52/35, SD[X] ~= 0.2575, SD[Y] ~= 0.4389
find the correlation coefficient
= ∫^1_0 [∫^2_2x xy * 3/10(xy^3 + 2) dy] dx
= 3/10 ∫^1_0 [∫^2_2x x^2y^4 + 2xy dy] dx
= 3/10 ∫^1_0 [x^2y^5/5 + xy^2]|_2x-2 dx
= 3/10 ∫^1_0 (32x^2/5 + 4x - 32x^7/5 - 4x^3) dx
= 3/10 [32/15x^3 + 2x^2 - 4/5x^8 - x^4]|_0-1
= 3/10(35/15)
= 7/10
covariance
= 7/10 - (3/7)(52/35)
= 31/490
~= (31/490)/(0.2575)*(0.4389)
~= 0.56

===================================================

For a set of random variables X1,...,Xn, the covariance matrix Σ
Σ = [
	 Var[X1]   Cov[X1,X2] ... Cov[X1,Xn]
	Cov[X2,X1]   Var[X2]  ... Cov[X2,Xn]
	    .           .     .       .
	    .           .      .      .
	    .           .       .     .
	Cov[Xn,X1] Cov[Xn,X2] ...  Var[Xn]
]
The ith diagonal elements is Σ_ii = Var[Xi]
The element in the ith row jth column for i != j is Σ_ij = Cov[Xi,Xj]
Since Cov[Xi,Xj] = Cov[Xj,Xi] for any pair of random variables Xi, Xj
theorem:
The covariance matrix is always symmetric it is a positive semi-definite matrix

determining elements of a covariance matrix
Σ = [
	4 2 0
	a 5 b
	c 1 2
]
the covariance matrix Σ for random variables X1,X2, and X3 is given. find a+b+c
Cov[X,Y] = Cov[Y,X]
the covariance matrix is always symmetric (postive semi-definite)
we can find the missing elements
a = Σ_21 = Cov[X2,X1] = Cov[X1,X2] = Σ_12 = 2
b = Σ_23 = Cov[X2,X3] = Cov[X3,X2] = Σ_32 = 1
c = Σ_31 = Cov[X3,X1] = Cov[X1,X3] = Σ_13 = 0
a+b+c = 2+1+0 = 3

example:
Σ = [
	1 -1
	a  2
]
the covariance matrix Σ, find a
a = Σ_21 = Cov[X2,X1] = Cov[X1,X2] = Σ_12 = -1

example:
Σ = [
	4 -1 1
	a  5 3
	b  c 6
]
the covariance matrix Σ, find a+b+c
a = Σ_21 = Cov[X2,X1] = Cov[X1,X2] = Σ_12 = -1
b = Σ_31 = Cov[X2,X3] = Cov[X3,X2] = Σ_13 = 1
c = Σ_32 = Cov[X3,X1] = Cov[X1,X3] = Σ_23 = 3
= 3

finding the covariance matrix given some moments
E[X] = 4, E[Y] = 3, mixed moment E[XY] = 9, second raw moments E[X^2] = 18 and E[Y^2] = 14 find the covariance matrix
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
]
Var[X] = E[X^2] - E[X]^2
= 18 - 4^2
= 2
Var[Y] = E[Y^2] - E[Y]^2
= 14 - 3^2
= 5
Cox[X,Y] = E[XY] - E[X] * E[Y]
= 9 - 4*3
= -3
Σ = [
	 2 -3
	-3  5
]

example:
E[X] = 6, E[Y] = 5, mixed moment E[XY] = 32, and variance Var[X] = 4, Var[Y] = 3, find the covariance matrix
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	4 2
	2 3
]
Cox[X,Y] = E[XY] - E[X] * E[Y]
= 32 - 6*5
= 2

example:
E[X] = 2, E[Y] = 4, mixed moment E[XY] = 10, second raw moments E[X^2] = 9 and E[Y^2] = 20, find the covariance matrix
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	5 2
	2 4
]
Var[X] = E[X^2] - E[X]^2
= 9 - 2^2
= 5
Var[Y] = E[Y^2] - E[Y]^2
= 20 - 4^2
= 4
Cox[X,Y] = E[XY] - E[X] * E[Y]
= 10 - 2*4
= 2

finding a covariance matrix given a discrete bivariate distribution
the joint probability mass function f(x,y)
f  |Y=1|Y=3|
X=2|0.3|0.2|
X=4|0.1|0.4|
------------
given: E[X] = 3, E[Y] = 2.2, find the covariance matrix
first marginal mass for X and Y
f  |Y=1|Y=3|f_X|
X=2|0.3|0.2|0.5|
X=4|0.1|0.4|0.5|
f_Y|0.4|0.6| 1 |
----------------
E[X^2]
= 2^2*0.5 + 4^2*0.5
= 10
Var[X] = E[X^2] - E[X]^2
= 10 - 3^2
= 1
E[Y^2]
= 1^2*0.4 + 3^2*0.6
= 5.8
Var[Y] = E[Y^2] - E[Y]^2
= 5.8 - 2.2^2
= 0.96
E[XY] = 2*1*0.3 + 4*1*0.1 + 2*3*0.2 + 4*3*0.4
= 7
Cov[XY] = E[XY] - E[X]*E[Y]
= 7 - 3*2.2
= 0.4
covariance matrix
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	 1   0.4
	0.4 0.96
]

example:
joint PMF
f  |Y=1|Y=2|
X=0|0.1|0.1|
X=1|0.3|0.5|
------------
given: E[X] = 0.8, E[Y] = 1.6, find the covariance matrix
marginal mass for X and Y
f  |Y=1|Y=2|f_X|
X=0|0.1|0.1|0.2|
X=1|0.3|0.5|0.8|
f_Y|0.4|0.6| 1 |
----------------
E[X^2]
= 0^2*0.2 + 1^2*0.8
= 0.8
E[Y^2]
= 1^2*0.4 + 2^2*0.6
= 2.8
Var[X] = E[X^2] - E[X]^2
= 0.8 - (0.8)^2
= 0.16
Var[Y] = E[Y^2] - E[Y]^2
= 2.8 - (1.6)^2
= 0.24
E[XY] = 0*1*0.1 + 0*2*0.1 + 1*1*0.3 + 1*2*0.5
= 1.3
Cov[XY] = E[XY] - E[X]*E[Y]
= 1.3 - 0.8*1.6
= 0.02
covariance matrix
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	0.16 0.02
	0.02 0.24
]

example:
joint PMF
f  | Y=2| Y=4|
X=0|0.15|0.35|
X=2| 0.1| 0.4|
--------------
given: E[X] = 1, E[Y] = 3.5, find the covariance matrix
marginal mass totals
f  | Y=2| Y=4|f_X|
X=0|0.15|0.35|0.5|
X=2| 0.1| 0.4|0.5|
f_Y|0.25|0.75| 1 |
------------------
E[X^2]
= 0^2*0.5 + 2^2*0.5
= 2
E[Y^2]
= 2^2*0.25 + 4^2*0.75
= 1 + 12
= 13
Var[X] = E[X^2] - E[X]^2
= 2 - (1)^2
= 1
Var[Y] = E[Y^2] - E[Y]^2
= 13 - (3.5)^2
= 0.75
E[XY] = 0*2*0.15 + 0*4*0.35 + 2*2*0.1 + 2*4*0.4
= 0.4 + 3.2
= 3.6
Cov[XY] = E[XY] - E[X]*E[Y]
= 3.6 - 1*3.5
= 0.1
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	   1  0.1
	 0.1 0.75
]

finding a covariance matrix given a continuous bivariate distribution
joint probability density function
(many steps, easy to make a mistake)
f(x,y) = {
	x + 3/2(y^2), 0 <= x <= 1, 0 <= y <= 1
	0,            otherwise
}
E[X] = 7/12, E[Y] = 5/8, E[XY] = 17/48, find the covariance matrix
first we find Var[X], we compute E[X^2] using lazy statistician
E[X^2] = ∫^1_0 ∫^1_0 x^2 * f(x,y) dxdy
= ∫^1_0 ∫^1_0 x^2(x + 3/2(y^2)) dxdy
= ∫^1_0 ∫^1_0 x^3 + 3/2(x^2y^2) dxdy
= ∫^1_0 [x^4/4 + x^3y^2/2]|_0-1 dy
= ∫^1_0 (1/4 + y^2/2) dy
= [1/4y + y^3/6]|_0-1
= 1/4 + 1/6
= 5/12
Var[X] = E[X^2] - E[X]^2
= 5/12 - (7/12)^2
= 11/144
E[Y^2] = ∫^1_0 ∫^1_0 y^2 * f(x,y) dxdy
= ∫^1_0 ∫^1_0 y^2(x + 3/2(y^2)) dxdy
= ∫^1_0 ∫^1_0 (xy^2 + 3/2(y^4)) dxdy
= ∫^1_0 [x^2y^2/2 + 3/2xy^4]|_0-1 dy
= ∫^1_0 (y^2/2 + 3/2y^4) dy
= [y^3/6 + 3/10y^56]|_0-1
= 1/6 + 3/10
= 7/15
Var[Y] = E[Y^2] - E[Y]^2
= 7/15 - (5/8)^2
= 73/960
Cov[X,Y] = E[XY] - E[X]*E[Y]
= 17/48 - (7/12)(5/8)
= -1/96
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	11/144  -1/96
	 -1/96 73/960
]

example:
joint PDF
f(x,y) = {
	1/16(xy), 0 <= x <= 2, 0 <= y <= 4
	0,        otherwise
}
given: E[X] = 4/3 and E[Y] = 8/3, find the covariance matrix
E[X^2] = ∫^4_0 ∫^2_0 x^2 * f(x,y) dxdy
= ∫^4_0 ∫^2_0 x^2(1/16(xy)) dxdy
= ∫^4_0 ∫^2_0 x^3y/16 dxdy
= ∫^4_0 [x^4y/64]|_0-2 dy
= ∫^4_0 (16y/64) dy
= [16y^2/128]|_0-4
= 256/128
Var[X] = E[X^2] - E[X]^2
= 2 - (4/3)^2
= 2 - 16/9
= 18/9 - 16/9
= 2/9
E[Y^2] = ∫^4_0 ∫^2_0 y^2 * f(x,y) dxdy
= ∫^4_0 ∫^2_0 y^2(1/16(xy)) dxdy
= ∫^4_0 ∫^2_0 (xy^3/16) dxdy
= ∫^4_0 [x^2y^3/32]|_0-2 dy
= ∫^4_0 (4y^3/32) dy
= [y^4/32]|_0-4
= 256/32
= 8
Var[Y] = E[Y^2] - E[Y]^2
= 8 - (8/3)^2
= 72/9 - 64/9
= 8/9
S = [0,2] x [0,4]
f(x,y) = g(x) * h(y)
g(x) = x/2 and h(y) = y/8
X and Y are independent Cov[X,Y] = 0
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	2/9 0
	 0 8/9
]

example:
joint PDF
f(x,y) = {
	x + y, 0 <= x <= 1, 0 <= y <= 1
	0,     otherwise
}
given: E[X] = E[Y] = 7/12, E[X^2] = 5/12, find the covariance matrix
Var[X] = E[X^2] - E[X]^2
= 5/12 - (7/12)^2
= 60/144 - 49/144
= 11/144
E[Y^2] = ∫^1_0 ∫^1_0 y^2 * (x + y) dxdy
= ∫^1_0 ∫^1_0 y^2x + y^3 dxdy
= ∫^1_0 [y^2x^2/2 + y^3x]|_0-1 dy
= ∫^1_0 (y^2/2 + y^3) dy
= [y^3/6 + y^4/4]|_0-1
= 1/6 + 1/4 = 2/12 + 3/12
= 5/12
Var[Y] = E[Y^2] - E[Y]^2
= 5/12 - (7/12)^2
= 5/12 - 49/144
= 60/144 - 49/144
= 11/144
E[XY] = ∫^1_0 ∫^1_0 xy * (x + y) dxdy
= ∫^1_0 ∫^1_0 xy * (x + y) dxdy
= ∫^1_0 ∫^1_0 (x^2y + y^2x) dxdy
= ∫^1_0 [x^3y/3 + y^2x^2/2]|_0-1 dy
= ∫^1_0 (y/3 + y^2/2) dy
= [y^2/6 + y^3/6]|_0-1
= 1/6 + 1/6
= 1/3
Cov[X,Y] = E[XY] - E[X]*E[Y]
= 1/3 - (7/12)(7/12)
= -1/144
Σ = [
	 Var[X] Cov[X,Y]
	Cov[Y,X] Var[Y]
] = [
	11/144 -1/144
	-1/144 11/144
] = 1/144[
	11 -1
	-1 11
]

===================================================
NOTE: Φ(z) were lookup tables thats how the values were found for them.

Normal approximation of binomial distributions
suppose the random variable X ~ B(n,p) follows a binomial distribution
There are certain situations where working with binomial random variables becomes difficult. For example, when the parameter n is large, computing exact probabilities can become porblematic due to the large numbers involved
under certain conditions we can approximate the discrete random variable X using a continuous normal random variable Y
Y ~ N(μ, σ^2)
where μ and σ^2 are calculated using the formulas for the mean and variance of the binomial random variable X
μ = np, σ^2 = np(1 - p)
to determine whether a given binomial distribution can be approximated using a normal distribution, we use the following rule of thumb:
np > 5
n(1 - p) > 5
lets consider a few examples of binomial distributions and their corresponding normal approximations
The case where n = 5, p = 0.2, its straightforward to show that
μ = 1
σ^2 = 0.8
np = 1 < 5
n(1 - p) = 4 < 5
since np < 5 the normal distribution N(1,0.8) is not a good approximation B(5,0.3)
The case where n = 5 and p = 0.5 its straightforward to show
μ = 2.5
σ^2 = 1.25
np = n(1 - p) = 2.5 < 5
since np = n(1 - p) < 5 the normal distribution N(2.5,1.25) is not a good approximation to B(5,0.5)
finally the case where n = 20 and p = 0.5 its straightforward to show
μ = 10
σ^2 = 5
np = n(1 - p) = 10 > 5
since np > 5 and n(1 - p) > 5 the normal distribution N(10,5) is a good approximation to B(20, 0.5)
We can approximate binomial random variables as normal when np > 5 and n(1-p) > 5 is a consequence of the central limit theorem (CLT)

Identifying situations where a normal approximation is appropriate
which statements could be approximated using the normal approximation of the binomial distribution
1. The probability that a basketball player scores at least 2 baskets in a series of 3 free throws if the probability of scoring a basket on each free throw is 0.55
2. The probability of getting 35 even numbers on 60 rolls of a 6-sided die.
3. The probability that a fair spinner with three regions, red, green, and blue, lands on green in 5 out of 12 spins
Given a binomial random variable X ~ B(n,p) if
np > 5
n(1-p) > 5
we can approximate X as a normal random variable Y with mean μ = np and variance σ^2 = np(1-p)
Y ~ N(np,np(1-p))
in situation 1, n = 3 and p = 0.55 so
np = 3 * 0.55
= 1.65
!= > 5
not greater than 5 so we cannot use the normal approximation of the binomial distribution
in situation 2, n = 60 and p = 1/2
np = 60*0.5
= 30
> 5
n(1-p) = 60 * (1-0.5)
= 30
> 5
we can use the normal approximation of the binomial distribution
in situation 3 n = 12 and p = 1/3
np = 12*1/3
= 4
!= > 5
we cannot use normal approximation of the binomial distribution
the correct answer is 2 only

example:
X ~ B(n,p)
1. The probability that a fair coin lands on heads 40 times out of 100 coin flips:
n = 100
np = 0.5
= 100*0.5
= 50
> 5
n(1-p) = 100 * (1-0.5)
= 50
> 5
this can be approximated using the normal approximation of the binomial distribution
2. The probability of getting 10 ones on 20 rolls of 6-sided die
n = 20
np = (1/6)
20 * (1/6)
= 10/3
!= > 5
3. The probability of a student getting 4 correct answers on a true/false quiz with 5 questions if the student answers all questions at random
n = 5
np = 1/2
= 5/2
!= > 5
1 is the only solution

example:
X ~ B(n,p)
1. probability getting 12 defective bulbs in a box containing 20 bulbs if the probability that an individual bulb is defective 0.2
20 * 0.2
!= > 5
2. probability that a soccer player scores 20 of 30 penalties if the probability of scoring each penalty is 0.55
30*0.55
16.5
n(1-p) = 30 * (1-0.55)
= 13.5 > 5
3. probability of getting 9 fives on 28 rolls of a 6-sided die
28 * 1/6
= 14/3 !> = 5
2 only works here

using the normal approximation to the binomial distribution
X ~ B(50,0.6) lets estimate the value of P(X = 33) using normal approximation
np = 50*0.6
= 30 > 5
n(1 - p) = 50*(1 - 0.6)
= 20 > 5
we can approximate our binomial random variable X with a normal random variable Y with mean 
μ = 50 * 0.6 = 30
σ^2 = 50 * 0.6 * (1 - 0.6) = 12
we can approximate X by
Y ~ N(30,12)
since X is discrete and Y is continuous we need to apply a continuity correction. The required continuity in this case is
P(X = 33) ~= P(32.5 <= Y <= 33.5)
to compute this probability we tansform to a standard normal random variable by z-scoring
P(32.5 <= Y <= 33.5) = P((32.5 - 30)/sqrt(12) <= Z <= (33.5 - 30)/sqrt(12))
~= P(0.72 <= Z <= 1.01)
= P(Z <= 1.01) - P(Z < 0.72)
= P(Z <= 1.01) - P(Z <= 0.72)
= Φ(1.01) - Φ(0.72)
using a table of values for the CDF of the standard normal distribution
Φ(1.01) = 0.8438, Φ(0.72) = 0.7642
P(32.5 <= Y <= 33.5) = Φ(1.01) - Φ(0.72)
= 0.8438 - 0.7642
= 0.0796
P(X = 33) ~= 0.0796

Rounding errors
our previous approximation:
P(X = 33) ~= P(32.5 <= Y <= 33.5) ~= 0.0796
if we use a software package to evaluate this approximation we might get
P(X = 33) ~= P(32.5 <= Y <= 33.5) ~= 0.0791
a difference of around 0.0005
there is a reason for this discrepancy when we calculated that P(X = 33) ~= P(32.5 <= Y <= 33.5) ~= 0.0796 we rounded our z-value two decimal places and then used a lookup table to find Φ(z) for those rounded values as a reuslt we introduced some small rounding errors
typically the rounding errors are small 

using a normal approximation to approximate a binomial probability
In a town 44% of the people have blood type B+ using the normal approximation to the binomial distribution approximate the probability that if 150 people are picked at random then 70 of them will have blood type B+, round 3 decimal
n = 150, p = 0.44
np = 150*0.44
= 66 > 5
n(1-p) = 150*(1 - 0.44)
= 84 > 5
approximate our binomial random variable X with normal random variable Y with mean
μ = np = (150)(0.44) = 66
variance
σ^2 = np(1-p) = (150)(0.44)(1-0.44) = 36.96
approximate X by Y ~ N(66,36.96)
now we can approximate the desired probability P(X = 70) using a continuity correction
P(X = 70) ~= P(69.5 <= Y <= 70.5)
to compute this probability we transform to a standard normal random variable by z-scoring
P(69.5 <= Y <= 70.5)
= P((69.5 - 66)/sqrt(36.96) <= Z <= (70.5 - 66)/sqrt(36.96))
~= P(0.58 <= Z <= 0.74)
= P(Z <= 0.74) - P(Z < 0.58)
= P(Z <= 0.74) - P(Z <= 0.58)
= Φ(0.74) - Φ(0.58)
= 0.7704 - 0.7190
= 0.0514
P(X = 70) ~= 0.051
using a software package for the approximation gives a probability of 0.0528

example:
given that the random variable X ~ B(100,0.4) use the normal approximation to the binomial distribution to approximate P(X = 50), 3 decimals
μ = np = (100)(0.4) = 40
σ^2 = np(1-p) = (100)(0.4)(1-0.4) = 24
P(49.5 <= Y <= 50.5)
= P((49.5 - 40)/sqrt(24) <= Z <= (50.5 - 40)/sqrt(24))
~= P(1.94 <= Z <= 2.14)
= Φ(2.14) - Φ(1.94)
= 0.9838 - 0.9738 
P(X = 50) ~= 0.010

example the normal approximation to the binomial distribution, approximate the probability that a student gets 14 correct answers on a true/false quiz with 25 questions if the student answers all questions at random 
μ = np = (25)(0.5) = 12.5
σ^2 = np(1-p) = (25)(0.5)(1-0.5) = 6.25
P(X = 14)
P(13.5 <= Y <= 15.5)
= P((13.5 - 12.5)/sqrt(6.25) <= Z <= (14.5 - 12.5)/sqrt(6.25))
~= P(0.4 <= Z <= 0.8)
= Φ(0.8) - Φ(0.4)
= 0.7881 - 0.6554
P(X = 14) ~= 0.133

Using a normal approximation to approximate a binomial probability over an interval
an archer practices firing an arrow at a target 50 times. The archer has a 47% chance of hitting the bull's eye on each shot. Using the normal approximation to the binomial distribution, estimate the probability that the archer hits the bull's eye more than 29 times but less than 35 times
X ~ B(n,p)
n = 50, p = 0.47
np = 50*0.47
= 23.5 > 5
n(1 - p) = 50 * (1 - 0.47)
= 26.5 > 5
μ = np = (50)(0.47) = 23.5
variance
σ^2 = np(1 - p) = (50)(0.47)(1 - 0.47) = 12.455
approximate X by Y ~ N(23.5,12.455)
we can estimate the desired probability P(29 < X < 35) using a continuity correction
P(29 < X < 35) = P(30 <= X <= 34)
~= P(29.5 <= Y <= 34.5)
tansform to a standard normal variable by z-scoring
= P((29.5 - 23.5)/sqrt(12.455) <= Z <= (34.5 - 23.5)/sqrt(12.455))
~= P(1.70 <= Z <= 3.12)
= Φ(3.12) - Φ(1.70)
= 0.9991 - 0.9554
= 0.0437
P(29 < X < 35) ~= 0.044
software package: 0.0436

example:
given that the random variable X ~ B(120,0.41) use the normal approximation to the binomial distribution to approximate P(X < 48)
μ = np = (120)(0.41) = 49.2
σ^2 = np(1 - p) = (120)(0.41)(1 - 0.41) = 29.028
P(X < 48) = P(X <= 47) ~= P(Y <= 47.5)
P(Z <= (47.5 - 49.2)/sqrt(29.028))
~= P(Z <= -0.32)
= Φ(-0.32)
= 0.3745
P(X < 48) ~= 0.375
software package: 0.3762

example:
43% of users of twitter purchase a verify subscription. Using the normal approximation to the binomial distribution estimate the probability that in a random sample of 200 users, more then 80 but less than 105 of them purchase a digital verify.
μ = np = (200)(0.43) = 86
σ^2 = np(1 - p) = (200)(0.43)(1 - 0.43) = 49.02
P(80 < X < 105) = P(81 <= X <= 104) = P(80.5 <= Y <= 104.5)
= P((80.5 - 86)/sqrt(49.02) <= Z <= (104.5 - 86)/sqrt(49.02))
~= P(-0.79 Z <= 2.64)
= Φ(2.64) - Φ(-0.79)
= 0.9959 - 0.2177
= 0.7811
P(80 < X < 105) ~= 0.781
software package: 0.7798

The normal approximation as a consequence of the central limit theorem
if X1,X2,...,Xn are independent and identically distributed (I.I.D) random variables then 
X = Σ^n_i=1(Xi) ~= N(nμ,nσ^2)
where n is sufficiently large
E[Xi] = μ, Var[Xi] = σ^2
if X ~ B(n,p) then X can be thought of as a sum of n I.I.D Bernoulli random variables
Xi ~ Bernoulli(p)
where the PMF of each Xi is given by
f(x) = {
	p,     x = 1
	1 - p, x = 0
	0,     otherwise
}
E[Xi] = μ = p, Var[Xi] = σ^2 = p(1 - p)
by substituting our values of μ and σ^2 into our normal approximation for X
X ~= N(np,np(1-p))
the sufficiently large condition on n is taken care of stipulating that np > 5 and n(1 - p) > 5
when p ~= 0.5 then n ~= 10 is sufficiently large for the normal approximation to be applied
if p is closer to 0 or one then the distribution of X is not symmetric and we need n to be much larger for the central limit theorem to be valid
