Combining Random Variables:

Distributions of Two Discrete Random Variables
Distributions of Two Continuous Random Variables
Expectation for Joint Distributions
Covariance of Random Variables
Normally Distributed Random Variables

NOTE:
sigma notation is similar to integral notation as I am not using anything like MathML example: Σ^3_j=1(j + 1) in this case 3 is above the sigma and ^, and _ separates what would be on the top and bottom respectively and next to sigma is the iterator (j + 1)

--------------------------------------------

Sigma notation
Σ^n_i=1(ai)
refers to the sum of all terms in the sequence ai where index i ranges from 1 to n
Σ^n_i=1 ai = a1 + a2 + ... + an
sigma notion can also represents sums whose terms depend on two indices that is double sums
Σ^m_i=1Σ^n_j=1(aij)
where i ranges from 1 to m and j ranges from 1 to n, "sum of a sum"
first we evaluate the inner sum by fixing the index of the outer sum (i) and incrementing only the inner index (j)
next we evaluate out sum by incrementing the outer sum index
Σ^2_i=1Σ^4_j=1(i + j)
= Σ^2_i=1[Σ^4_j=1(i + j)]
= Σ^2_i=1[(i + 1) + (i + 2) + (i + 3) + (i + 4)]
= Σ^2_i=1(4i + 10)]
= 4(1) + 10 + 4(2) + 10
= 4 + 10 + 8 + 10
= 32

example:
Σ^3_j=1Σ^2_i=1(i + j)
= Σ^3_j=1[Σ^2_i=1(i + j)]
= Σ^3_j=1[(1 + j) + (2 + j)]
= Σ^3_j=1(3 + 2j)
outer
Σ^3_j=1(3 + 2j) = (3 + 2 * 1) + (3 + 2 * 2) + (3 + 2 * 3)
= 5 + 7 + 9
= 21

example:
Σ^3_i=1Σ^3_j=2(i^2 + j)
= Σ^3_i=1[Σ^3_j=2(i^2 + j)]
= Σ^3_i=1[(i^2 + 2) + (i^2 + 3)]
= Σ^3_i=1(2i^2 + 5)
outer
Σ^3_i=1(2i^2 + 5) = (2(1)^2 + 5) + (2(2)^2 + 5) + (2(3)^2 + 5)
= 7 + 13 + 23
= 43

example:
Σ^2_j=1Σ^3_i=1(ij)
= Σ^2_j=1[Σ^3_i=1(ij)]
= Σ^2_j=1[(1j) + (2j) + (3j)]
= Σ^2_j=1(6j)
outer
Σ^2_j=1(6j) = (6(1)) + (6(2))
= 6 + 12
= 18

The sum and constant multiple rules
The constant multiple rule
we can factor out a constant multiple from a double summation
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
The sum rule
we can distribute a double summation over a sum of terms
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
The double sum of units
for single summations we have
Σ^n_j=1(1) = 1 + 1 + ... + 1 = n (n times)
double summations
Σ^m_i=1Σ^n_j=1(1) = mn

Applying the sum and constant multiple rules
find the value of the double sum
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
given that
Σ^5_i=1Σ^8_j=1(aij) = 8
Σ^5_i=1Σ^8_j=1(bij) = 15
the sum and contant multiple rules for double summations
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
applying the sum rule to the given double summation
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= Σ^5_i=1Σ^8_j=1(1) + Σ^5_i=1Σ^8_j=1(2aij) - Σ^5_i=1Σ^8_j=1(bij)
= (Σ^5_i=1Σ^8_j=1(1)) + 2Σ^5_i=1Σ^8_j=1(aij) - Σ^5_i=1Σ^8_j=1(bij)
from the info given
= (Σ^5_i=1Σ^8_j=1(1)) + 2(8) - 15
= (Σ^5_i=1Σ^8_j=1(1)) + 1
using
Σ^m_i=1Σ^n_j=1(1) = mn
(Σ^5_i=1Σ^8_j=1(1)) + 1 = 5 * 8 + 1
= 41
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= 41

example:
find the value of the double sum
Σ^50_i=1Σ^100_j=1(4(aij + bij))
given
Σ^50_i=1Σ^100_j=1(aij) = 40
Σ^50_i=1Σ^100_j=1(bij) = 60
= 4Σ^50_i=1Σ^100_j=1(aij) + 4Σ^50_i=1Σ^100_j=1(bij)
= 4 * 40 + 4 * 60
= 160 + 240
= 400

example:
Σ^5_i=1Σ^10_j=1(1 - aij - bij)
Σ^5_i=1Σ^10_j=1(aij) = 10
Σ^5_i=1Σ^10_j=1(bij) = 20
= (Σ^5_i=1Σ^10_j=1(1)) - 30
= 5 * 10 - 30
= 50 - 30
= 20

The product and swap rules
the product rule
Σ^m_i=1Σ^n_j=1(aibj) = Σ^m_i=1(ai)Σ^n_j=1(bj)
the swap rule
Σ^m_i=1Σ^n_j=1(aij) = Σ^n_j=1Σ^m_i=1(aij)

Appling the product rule and swap rules
find the value of the double sum
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
given
Σ^15_i=1(ai) = 40
Σ^18_j=1(bj) = -8
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
= Σ^15_i=1(ai - 2)Σ^18_j=1(bj + 1)
= (Σ^15_i=1(ai) - Σ^15_i=1(2)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (Σ^15_i=1(ai) - 2Σ^15_i=1(1)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (40 - 2 * 15) * (-8 + 18)
= 10 * 10
= 100

example:
Σ^10_i=1Σ^20_j=1(ai - 1)(bj + 1)
given
Σ^10_i=1(ai) = 12
Σ^20_j=1(bj) = 5
= (Σ^10_i=1(ai - 1)Σ^20_j=1(bj + 1))
= (Σ^10_i=1(ai) - Σ^10_i=1(1)) * (Σ^20_j=1(bj) + Σ^20_j=1(1))
= (12 - 10) * (5 + 20)
= (2) * (25)
= 50

example:
Σ^30_i=1Σ^30_j=1(2ai(bj - 2))
given
Σ^30_i=1(ai) = 42
Σ^30_j=1(bj) = 70
= 2Σ^30_i=1(ai)Σ^30_j=1(bj - 2)
= (2Σ^30_i=1(ai)) * (Σ^30_j=1(bj) - 2Σ^30_j=1(1))
= (2 * 42) * (70 - 2 * 30)
= (84) * (70 - 60)
= (84) * (10)
= 840

===================================================

We often want to know if there is a relationship between two random variables. For this reason we wish to formulate the idea of a joint probability distribution
X represents the number of heads obtained when a fair coin is flipped. The support of X denoted S_X consists of all possible values of X is given by S_X = {0,1}
Y represents the result of rolling a fair tetrahedral die then the support of Y denoted S_Y is S_Y = {1,2,3,4}
The joint support of X and Y denoted S consists of all possible pairs (x, y) such that x is a possible outcome for X and y is a possible outcome for Y
S = S_X x S_Y = {
	(0,1), (0,2), (0,3), (0,4)
	(1,1), (1,2), (1,3), (1,4)
}
S_X x S_Y is the cartesian product of S_X and S_Y
flipping of the coin and rolling of the die are independent then for any x ∈ X and any y ∈ Y
P(X = x and Y = y) = 1/2 * 1/4 = 1/8
the joint probability mass function of X and Y denoted f(x,y) can be represented by the table
f(x,y)|y=1|y=2|y=3|y=4|
x = 0 |1/8|1/8|1/8|1/8|
x = 1 |1/8|1/8|1/8|1/8|
similar to the case of single random variables
f(x,y) = P(X = x and Y = y)
P(X = x,Y = y) as short hand

Bivariate and multivariate distributions
X and Y are discrete random variables with supports S_X and S_Y
f(x,y) = P(X = x,Y = y)
to be a valid joint probability mass function with joint support S = S_X x S_Y it must satisfy following conditions
0 <= f(x,y) <= 1 for all (x,y) ∈ S
Σ_(x,y)∈S f(x,y) = 1
P((X,Y) ∈ A) = Σ_(x,y)∈A f(x,y) where A is a subset of S
some intuition
the consition 0 <= f(x,y) <= 1 for all (x,y) ∈ S states that any possible outcome for (X,Y) must have a probability between 0 and 1
The Σ_(x,y)∈S f(x,y) = 1 states that the sum of all probabilities over all possible values of X and Y must add up to 1
The third condition states that we compute the probability of an event A by adding up the probabilites associated with A
The joint probability table
f(x,y) | Y = y1 | Y = y2 | ... | Y = yn |
x = x1 |f(x1,y1)|f(x1,y2)| ... |f(x1,yn)|
x = x2 |f(x2,y1)|f(x2,y2)| ... |f(x2,yn)|
 ...   |   ...  |   ...  | ... |   ...  |
x = xk |f(xk,y1)|f(xk,y2)| ... |f(xk,yn)|
two random variables the joint distribution is sometimes called a bivariate distribution, any number of random variables is a multivariate distribution

finding the joint distribution of two discrete random variables
   |Y=0|  Y=1 |
X=0|a/2|   a  |
X=1| a |5/2(a)|
the joint probability mass function f(x,y) for the discrete random variables X and Y is given above
The support of X is S_X = {0,1} the support of Y is S_Y = {0,1} and therefore the joint support S is
S = S_X x S_Y = {(0,0),(0,1),(1,0),(1,1)}
f(0,0) + f(0,1) + f(0,1) + f(1,1) = 1
a/2 + a + a + 5/2(a) = 1
5a = 1
a = 1/5

example:
   |Y=1|Y=2|
X=0| 2a| 3a|
X=1| 5a| 2a|
S = S_X x S_Y = {(0,1),(0,2),(1,1),(1,2)}
2a + 3a + 5a + 2a = 1
12a = 1
a = 1/12

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
which of the following conditions are true?
1. 0 <= f(x,y) <= 1 for all (x,y) in S
2. Σ_(x,y)∈S f(x,y) = 1
3. f(x,y) is a valid joint probability mass function
all 3 are valid, and since 1 and 2 we've seen makes 3 true.

Calculating a joint probability from a table
   |Y=1 |Y=3 |
X=2|0.05|0.25|
X=4|0.2 |0.05|
X=6|0.3 |0.15|
compute P((X,Y) ∈ {(2,3),(6,1)})
= P((X,Y) = (2,3)) + P((X,Y) = (6,1))
= f(2,3) + f(6,1)
= 0.25 + 0.3
= 0.55

example:
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
calculate P(X=0,Y=1)
= f(0,1) = 1/2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P((X,Y) ∈ {(1,4),(1,6),(2,6)})
= 0.25 + 0.25 + 0.2
= 0.7

Calculating a joint probability containing inequalities from a table
   |Y=0 |Y=2 |Y=4
X=0|0.15|0.1 |0.1
X=1|0.05|0.1 |0.05
X=2|0.05|0.25|0.15
Compute P(X*Y >= 4)
= P((X,Y) ∈ {(1,4),(2,2),(2,4)})
= 0.05 + 0.25 + 0.15
= 0.45

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X = 2 and Y <= 4)
= P((X,Y) ∈ {(2,2),(2,4)})
= 0.05 + 0.15
= 0.2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X < 2 or Y = 6)
= P((X,Y) ∈ {(1,2),(1,4),(1,6),(2,6)})
= 0.1 + 0.25 + 0.25 + 0.2
= 0.8

===================================================

The joint PMF f(x,y) tells us all possible events (X,Y) and the probabilities associated with each event, however we wish to determine the PMF for X only how can this be deduced from the joint PMF?
consider the joint PMF
   |Y=1 |Y=2 |
X=1|0.05|0.15|
X=2|0.7 |0.1 |
The support of X is S_X = {1,2} lets now deduce the probabilites associated with each value of X in S_X
According to the law of total probability we can deduce P(X = 1) by summing the values in the first row of the table
P(X = 1) = P(X = 1,Y = 1) + P(X = 1,Y = 2)
= 0.05 + 0.15
= 0.2
summing the values in second row in table
P(X = 2) = P(X = 2,Y = 1) + P(X = 2,Y = 2)
= 0.7 + 0.1
= 0.8
PMF for X
x     | 1 | 2 |
f_X(x)|0.2|0.8|
f_X(x) was deduced from the joint probability mass function, we call f_X(x) the marginal mass function for X

Deducing the marginal mass function for Y
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
the marginal mass function in the right margin of the table
P(X = 1) = f_X(1) = 0.2
P(X = 2) = f_X(2) = 0.8
similary we can compute the marginal mass function of Y denoted f_Y(y) by summing the columns adding these totals to our table
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
f_Y|0.75|0.25| 1 |
P(Y = 1) = f_Y(1) = 0.75
P(Y = 2) = f_Y(2) = 0.25
marginal mass function f_Y(y) of Y
x     | 1  |  2 |
f_Y(y)|0.75|0.25|

A formal definition of the margin mass function
The maginal mass function of X denoted f_X(x)
f_X(x) = P(X = x) = Σ_y∈SY f(x,y)
The marginal mass function of Y denoted f_Y(y)
f_Y(y) = P(Y = x) = Σ_x∈SX f(x,y)
To compute P(X = x) for some particular x we sum all possible values of Y in the row corresponding to X = x
To compute P(Y = y) for some particular y we sum all possible values of X in the column corresponding to Y = y
Marginal mass functions are sometimes called marginal probability mass functions, marginal PMFs, or marginal distributions

Finding a marginal probability
   |Y=0 |Y=1 |
X=0|1/6 |5/12|
X=1|1/4 |1/6 |
P(X = x) = f_X(x) = Σ_y f(x,y)
P(Y = y) = f_Y(y) = Σ_x f(x,y)
The marginal distribution for X corresponds to the row totals and the marginal distribution for Y corresponds to the column totals.
   |Y=0 |Y=1 |f_X |
X=0|1/6 |5/12|7/12|
X=1|1/4 |1/6 |5/12|
f_Y|5/12|7/12|  1 |
P(X = 0) = f_X(0)
= f(0,0) + f(0,1)
= 1/6 + 5/12
= 7/12

example:
find P(X = 0)
   |Y=1 |Y=2 |
X=0|0.4 |0.2 |
X=1|0.15|0.25|
= 0.4 + 0.2
= 0.6

example:
find f_Y(0)
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
= 1/4 + 1/8
= 3/8

(rest of examples are simple table lookups and summations no new knowledge to note)

===================================================

Determining whether two random variables are independent
   |Y=0 |Y=1 |
X=0|0.24|0.06|
X=1|0.56|0.14|
marginal mass functions
x     |0  |1  |
f_X(x)|0.3|0.7|
---------------
y     |0  |1  |
f_Y(y)|0.8|0.2|
which of the following statements are true?
1. P(X = 0,Y = 0) = P(X = 0)*P(Y = 0)
2. P(X = x,Y = y) = P(X = x)*P(Y = y) for all possible x and y
3. X and Y are independent
Two discrete random variables X and Y with joint probability mass function f(x,y) are independent iff
f(x,y) = f_X(x)*f_Y(y)
Statements 1,2, and 3 is true. 1 is true because
P(X = 0,Y = 0) = f(0,0) = 0.24
P(X = 0) * P(Y = 0) = f_X(0) * f_Y(0) = 0.3 * 0.8 = 0.24
2 is true because
f(1,0) = 0.56 = 0.7 * 0.8 = f_X(1) * f_Y(0)
f(0,1) = 0.06 = 0.3 * 0.2 = f_X(0) * f_Y(1)
f(1,1) = 0.14 = 0.7 * 0.2 = f_X(1) * f_Y(1)
P(X = x,Y = y) = P(X = x) * P(Y = y) for all possible x and y
statement 3 is true because of 1 and 2 is true X and Y must be independent

example:
   |Y=0 |Y=1|
X=0|0.05|0.2|
X=1|0.15|0.6|
computing the marginal probability mass functions for X and Y
   |Y=0 |Y=1| f_X|
X=0|0.05|0.2|0.25|
X=1|0.15|0.6|0.75|
f_Y| 0.2|0.8|  1 |
check if these statements are true
this first statement is true:
P(X = 1,Y = 1) = P(X = 1) * P(Y = 1)
0.75 * 0.8 = 0.6
this is also true
P(X = x,Y = y) = P(X = x) * P(Y = y)
making X and Y independent

example:
   |Y=0 | Y=1|
X=0|0.05|0.15|
X=1|0.25|0.55|
computing the marginal probability mass functions for X and Y
   |Y=0 | Y=1| f_X|
X=0|0.05|0.15| 0.2|
X=1|0.25|0.55| 0.8|
f_Y| 0.3| 0.7|  1 |
check if these statements are true
this first statement is true:
P(X = 0,Y = 1) = P(X = 0) * P(Y = 1)
0.2 * 0.7 = 0.14 != 0.15
so this would also be false
P(X = x,Y = y) = P(X = x) * P(Y = y)
and X and Y are not independent

example:
   |Y=2| Y=4|
X=1|0.2| 0.2|
X=3|0.1|0.25|
X=5|0.2|0.05|
computing the marginal probability mass functions for X and Y
   |Y=2| Y=4| f_X|
X=1|0.2| 0.2| 0.4|
X=3|0.1|0.25|0.35|
X=5|0.2|0.05|0.25|
f_Y|0.5| 0.5|  1 |
check if these statements are true
this is true:
P(X = 1,Y = 2) = P(X = 1) * P(Y = 2)
0.4 * 0.5 = 0.2
this is false (only have to check one instance)!
P(X = x,Y = y) = P(X = x) * P(Y = y)
0.5 * 0.35 = 0.175 != 0.1
making X and Y dependent

example:
   |Y=1| Y=3| f_X|
X=2| * |0.27|  * |
X=4| * |0.63|  ? |
f_Y|0.1| 0.9|  1 |
find P(X = 4)
normally to cross reference we * by totals, we don't
have all totals but we can find it by the info given to us
by dividing total with the cross reference cell
0.63/0.9 = 0.7
P(X = 4) = 0.7

===================================================

Conditional distributions for discrete random variables
the multiplication law for conditional probability for two events A and B
P(A|B) = P(A ∩ B)/P(B)
suppose X and Y are discrete random variables the conditional probability X = x given that Y = y follows immediately from the multiplication law
P(X = x|Y = y) = P(X = x,Y - y)/P(Y = y) = f(x,y)/f_Y(y)
F(x,y) is the joint PMF of X and Y, f_Y(y) is the marginal mass function
To avoid getting a zero in the denominator the formula assumes that P(Y = y) = f_Y(y) != 0
In the case where P(Y = y) = 0 the corresponding probability for X does not exist it makes no sense to ask about the event of X = x occuring given that Y = y has occurred if the event Y = y is impossible
similarly the conditional probability Y = y given X = x
P(Y = y|X = x) = P(X = x,Y = y)/P(X = x) = f(x,y)/f_X(x)
where f_X(x) is the marginal mass function of X this formula assumes that P(X = x) != 0

Computing a conditional probability
   | Y=0| Y=1|
X=0| 0.5|0.05|
X=1|0.15| 0.3|
compute P(Y = 0|X = 1) given that the discrete random variables X and Y have the joint probability function f(x,y)
P(Y = 0|X = 1) = P(X = 1,Y = 0)/P(X = 1) = f(1,0)/f_X(1)
f_X(1) = f(1,0) + f(1,1)
= 0.15 + 0.3
= 0.45
substituting
f_X(1) = 0.45 and f(1,0) = 0.15 in our formula
P(Y = 0|X = 1) = f(1,0)/f_X(1)
= 0.15/0.45
= 1/3

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(X = 2|Y = 3)
P_Y(3) = 0.25 + 0.05 + 0.15
= 0.45
f(2,3)/P_Y(3)
= 0.25/0.45
~= 0.56

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(Y = 3|X = 2)
P_X(2) = 0.05 + 0.25 = 0.3
f(2,3)/f_X(2)
= 0.25/0.3
~= 0.83

Computing a conditional probability over an interval
   | Y=0| Y=1|Y=2 |Y=3|
X=2| 0.2|   0|0.05|0.1|
X=4| 0.1|0.05|0.15|  0|
X=6| 0.2|0.05|   0|0.1|
Compute P(0 < Y < 3|X = 0)
= P((0 < Y < 3), X = 0)/P(X = 0)
= P(X = 0, (0 < Y < 3))/P(X = 0)
= P(X = 0,Y = 0) + P(X = 0,Y = 2)/P(X = 0)
= (f(0,1) + f(0,2))/f_X(0)
f_X(0) = 0.2 + 0 + 0.05 + 0.1
= 0.35
= 0 + 0.05/0.35
= 0.05/0.35
~= 0.14

example:
    | Y=0| Y=2| Y=4|
X=0 |0.15| 0.1| 0.1|
X=4 |0.05| 0.1|0.05|
X=8 | 0  | 0.2|0.15|
X=12|0.05|0.05|   0|
Compute P(4 <= X <= 8|Y = 0)
= (f(4,0) + (8,0))/f_Y(0)
= 0.05 + 0
f_Y(0) = 0.15 + 0.05 + 0 + 0.05 = 0.25
(0.05 + 0)/0.25
~= 0.2

example:
    | Y=0| Y=2| Y=4|Y=6|
X=1 |1/18|5/18| 1/9|2/9|
X=2 | 1/9|1/18|   0|1/6|
Compute P(2 <= Y <= 6|X = 2)
f_X(2) = 1/9 + 1/18 + 0 + 1/6
= 2/18 + 1/18 + 3/18
= 6/18
= 1/3
f(2,2) + f(2,4) + f(2,6)
= 1/18 + 0 + 1/6
= 4/18
= 2/9
= ((2/9)/(1/3)) 
= 2/9 * 3/1
= 6/9 = 2/3

Conditional probability mass functions
We can now define two more probability mass functions
The conditional probability mass function of X given that Y = y denoted f_X|Y(x|y)
f_X|Y(x|y)
= P(X = x,Y = y) = P(X = x,Y = y)/P(Y = y) 
= f(x,y)/f_Y(y)
f(x,y) is the joint probability mass function of X and Y and f_Y(y) is the marginal mass function of Y
The conditional probability mass function of Y given that X = x denoted f_Y|X(y|x)
f_Y|X(y|x)
= P(Y = y,X = x) = P(X = x,Y = y)/P(X = x)
= f(x,y)/f_X(x)
f(x,y) is a joint probability mass function of X and Y and f_X(x) in the marginal mass function of X
The conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y and independent
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)
X and Y are independent, knowing the value of one random variable does not affect the probability distribution of the other.

Computing the value of a conditional probability mass function at a point
    | Y=0| Y=1| Y=2|
X=1 | 1/6| 1/9| 1/6|
X=2 |1/18|1/18| 1/6|
X=3 |1/9 | 1/9|1/18|
Compute f_X|Y(2|2)
= f(x,y)/f_Y(2)
f_Y(2) = 1/6 + 1/6 + 1/18
= 3/18 + 3/18 + 1/18
= 7/18
f(2,2) = 1/6
= (1/6)/(7/18)
= 18/42
= 3/7

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Comput f_X|Y(1|0)
f_Y(0) = 0.3 + 0.1 = 0.4
f(1,0) = 0.1
0.1/0.4 = 0.25

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Compute f_Y|X(1|0)
f_X(0) = 0.3 + 0.2 + 0.1 = 0.6
f(0,1) = 0.2
0.2/0.6
~= 0.33

Computing conditional probability mass function
   | Y=1| Y=3| Y=5|
X=0| 0.1|   0| 0.2|
X=2| 0.3| 0.1|   0|
X=4| 0.1|   0| 0.2|
find f_X|Y(x|1)
= f(x,1)/f_Y(1)
x ∈ {0,2,4}
P(X = 0|Y = 1) = f(0,1)/f_Y(1) = 0.1/0.5 = 1/5
P(X = 2|Y = 1) = f(2,1)/f_Y(1) = 0.3/0.5 = 3/5
P(X = 4|Y = 1) = f(4,1)/f_Y(1) = 0.1/0.5 = 1/5
x         |0  |2  |4  |
f_X|Y(x|1)|1/5|3/5|1/5|
-----------------------

(rest of the example were faster to answer and calculate in my head then creating tables here for notes)
