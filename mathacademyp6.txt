Combining Random Variables:

Distributions of Two Discrete Random Variables
Distributions of Two Continuous Random Variables
Expectation for Joint Distributions
Covariance of Random Variables
Normally Distributed Random Variables

NOTE:
sigma notation is similar to integral notation as I am not using anything like MathML example: Σ^3_j=1(j + 1) in this case 3 is above the sigma and ^, and _ separates what would be on the top and bottom respectively and next to sigma is the iterator (j + 1)

NOTE:
When integrating we sometimes have to change an upper limit from a ∞ to a variable like b because I want to evalutate the integral step by step, to show the process before substituing back into the limit (a placeholder before arriving at the final form)

NOTE:
taking a CDF for Y we need to integrate with repect to x and keep y fixed, and vise versa.

--------------------------------------------

Sigma notation
Σ^n_i=1(ai)
refers to the sum of all terms in the sequence ai where index i ranges from 1 to n
Σ^n_i=1 ai = a1 + a2 + ... + an
sigma notion can also represents sums whose terms depend on two indices that is double sums
Σ^m_i=1Σ^n_j=1(aij)
where i ranges from 1 to m and j ranges from 1 to n, "sum of a sum"
first we evaluate the inner sum by fixing the index of the outer sum (i) and incrementing only the inner index (j)
next we evaluate out sum by incrementing the outer sum index
Σ^2_i=1Σ^4_j=1(i + j)
= Σ^2_i=1[Σ^4_j=1(i + j)]
= Σ^2_i=1[(i + 1) + (i + 2) + (i + 3) + (i + 4)]
= Σ^2_i=1(4i + 10)]
= 4(1) + 10 + 4(2) + 10
= 4 + 10 + 8 + 10
= 32

example:
Σ^3_j=1Σ^2_i=1(i + j)
= Σ^3_j=1[Σ^2_i=1(i + j)]
= Σ^3_j=1[(1 + j) + (2 + j)]
= Σ^3_j=1(3 + 2j)
outer
Σ^3_j=1(3 + 2j) = (3 + 2 * 1) + (3 + 2 * 2) + (3 + 2 * 3)
= 5 + 7 + 9
= 21

example:
Σ^3_i=1Σ^3_j=2(i^2 + j)
= Σ^3_i=1[Σ^3_j=2(i^2 + j)]
= Σ^3_i=1[(i^2 + 2) + (i^2 + 3)]
= Σ^3_i=1(2i^2 + 5)
outer
Σ^3_i=1(2i^2 + 5) = (2(1)^2 + 5) + (2(2)^2 + 5) + (2(3)^2 + 5)
= 7 + 13 + 23
= 43

example:
Σ^2_j=1Σ^3_i=1(ij)
= Σ^2_j=1[Σ^3_i=1(ij)]
= Σ^2_j=1[(1j) + (2j) + (3j)]
= Σ^2_j=1(6j)
outer
Σ^2_j=1(6j) = (6(1)) + (6(2))
= 6 + 12
= 18

The sum and constant multiple rules
The constant multiple rule
we can factor out a constant multiple from a double summation
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
The sum rule
we can distribute a double summation over a sum of terms
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
The double sum of units
for single summations we have
Σ^n_j=1(1) = 1 + 1 + ... + 1 = n (n times)
double summations
Σ^m_i=1Σ^n_j=1(1) = mn

Applying the sum and constant multiple rules
find the value of the double sum
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
given that
Σ^5_i=1Σ^8_j=1(aij) = 8
Σ^5_i=1Σ^8_j=1(bij) = 15
the sum and contant multiple rules for double summations
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
applying the sum rule to the given double summation
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= Σ^5_i=1Σ^8_j=1(1) + Σ^5_i=1Σ^8_j=1(2aij) - Σ^5_i=1Σ^8_j=1(bij)
= (Σ^5_i=1Σ^8_j=1(1)) + 2Σ^5_i=1Σ^8_j=1(aij) - Σ^5_i=1Σ^8_j=1(bij)
from the info given
= (Σ^5_i=1Σ^8_j=1(1)) + 2(8) - 15
= (Σ^5_i=1Σ^8_j=1(1)) + 1
using
Σ^m_i=1Σ^n_j=1(1) = mn
(Σ^5_i=1Σ^8_j=1(1)) + 1 = 5 * 8 + 1
= 41
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= 41

example:
find the value of the double sum
Σ^50_i=1Σ^100_j=1(4(aij + bij))
given
Σ^50_i=1Σ^100_j=1(aij) = 40
Σ^50_i=1Σ^100_j=1(bij) = 60
= 4Σ^50_i=1Σ^100_j=1(aij) + 4Σ^50_i=1Σ^100_j=1(bij)
= 4 * 40 + 4 * 60
= 160 + 240
= 400

example:
Σ^5_i=1Σ^10_j=1(1 - aij - bij)
Σ^5_i=1Σ^10_j=1(aij) = 10
Σ^5_i=1Σ^10_j=1(bij) = 20
= (Σ^5_i=1Σ^10_j=1(1)) - 30
= 5 * 10 - 30
= 50 - 30
= 20

The product and swap rules
the product rule
Σ^m_i=1Σ^n_j=1(aibj) = Σ^m_i=1(ai)Σ^n_j=1(bj)
the swap rule
Σ^m_i=1Σ^n_j=1(aij) = Σ^n_j=1Σ^m_i=1(aij)

Appling the product rule and swap rules
find the value of the double sum
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
given
Σ^15_i=1(ai) = 40
Σ^18_j=1(bj) = -8
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
= Σ^15_i=1(ai - 2)Σ^18_j=1(bj + 1)
= (Σ^15_i=1(ai) - Σ^15_i=1(2)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (Σ^15_i=1(ai) - 2Σ^15_i=1(1)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (40 - 2 * 15) * (-8 + 18)
= 10 * 10
= 100

example:
Σ^10_i=1Σ^20_j=1(ai - 1)(bj + 1)
given
Σ^10_i=1(ai) = 12
Σ^20_j=1(bj) = 5
= (Σ^10_i=1(ai - 1)Σ^20_j=1(bj + 1))
= (Σ^10_i=1(ai) - Σ^10_i=1(1)) * (Σ^20_j=1(bj) + Σ^20_j=1(1))
= (12 - 10) * (5 + 20)
= (2) * (25)
= 50

example:
Σ^30_i=1Σ^30_j=1(2ai(bj - 2))
given
Σ^30_i=1(ai) = 42
Σ^30_j=1(bj) = 70
= 2Σ^30_i=1(ai)Σ^30_j=1(bj - 2)
= (2Σ^30_i=1(ai)) * (Σ^30_j=1(bj) - 2Σ^30_j=1(1))
= (2 * 42) * (70 - 2 * 30)
= (84) * (70 - 60)
= (84) * (10)
= 840

===================================================

We often want to know if there is a relationship between two random variables. For this reason we wish to formulate the idea of a joint probability distribution
X represents the number of heads obtained when a fair coin is flipped. The support of X denoted S_X consists of all possible values of X is given by S_X = {0,1}
Y represents the result of rolling a fair tetrahedral die then the support of Y denoted S_Y is S_Y = {1,2,3,4}
The joint support of X and Y denoted S consists of all possible pairs (x, y) such that x is a possible outcome for X and y is a possible outcome for Y
S = S_X x S_Y = {
	(0,1), (0,2), (0,3), (0,4)
	(1,1), (1,2), (1,3), (1,4)
}
S_X x S_Y is the cartesian product of S_X and S_Y
flipping of the coin and rolling of the die are independent then for any x ∈ X and any y ∈ Y
P(X = x and Y = y) = 1/2 * 1/4 = 1/8
the joint probability mass function of X and Y denoted f(x,y) can be represented by the table
f(x,y)|y=1|y=2|y=3|y=4|
x = 0 |1/8|1/8|1/8|1/8|
x = 1 |1/8|1/8|1/8|1/8|
similar to the case of single random variables
f(x,y) = P(X = x and Y = y)
P(X = x,Y = y) as short hand

Bivariate and multivariate distributions
X and Y are discrete random variables with supports S_X and S_Y
f(x,y) = P(X = x,Y = y)
to be a valid joint probability mass function with joint support S = S_X x S_Y it must satisfy following conditions
0 <= f(x,y) <= 1 for all (x,y) ∈ S
Σ_(x,y)∈S f(x,y) = 1
P((X,Y) ∈ A) = Σ_(x,y)∈A f(x,y) where A is a subset of S
some intuition
the consition 0 <= f(x,y) <= 1 for all (x,y) ∈ S states that any possible outcome for (X,Y) must have a probability between 0 and 1
The Σ_(x,y)∈S f(x,y) = 1 states that the sum of all probabilities over all possible values of X and Y must add up to 1
The third condition states that we compute the probability of an event A by adding up the probabilites associated with A
The joint probability table
f(x,y) | Y = y1 | Y = y2 | ... | Y = yn |
x = x1 |f(x1,y1)|f(x1,y2)| ... |f(x1,yn)|
x = x2 |f(x2,y1)|f(x2,y2)| ... |f(x2,yn)|
 ...   |   ...  |   ...  | ... |   ...  |
x = xk |f(xk,y1)|f(xk,y2)| ... |f(xk,yn)|
two random variables the joint distribution is sometimes called a bivariate distribution, any number of random variables is a multivariate distribution

finding the joint distribution of two discrete random variables
   |Y=0|  Y=1 |
X=0|a/2|   a  |
X=1| a |5/2(a)|
the joint probability mass function f(x,y) for the discrete random variables X and Y is given above
The support of X is S_X = {0,1} the support of Y is S_Y = {0,1} and therefore the joint support S is
S = S_X x S_Y = {(0,0),(0,1),(1,0),(1,1)}
f(0,0) + f(0,1) + f(0,1) + f(1,1) = 1
a/2 + a + a + 5/2(a) = 1
5a = 1
a = 1/5

example:
   |Y=1|Y=2|
X=0| 2a| 3a|
X=1| 5a| 2a|
S = S_X x S_Y = {(0,1),(0,2),(1,1),(1,2)}
2a + 3a + 5a + 2a = 1
12a = 1
a = 1/12

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
which of the following conditions are true?
1. 0 <= f(x,y) <= 1 for all (x,y) in S
2. Σ_(x,y)∈S f(x,y) = 1
3. f(x,y) is a valid joint probability mass function
all 3 are valid, and since 1 and 2 we've seen makes 3 true.

Calculating a joint probability from a table
   |Y=1 |Y=3 |
X=2|0.05|0.25|
X=4|0.2 |0.05|
X=6|0.3 |0.15|
compute P((X,Y) ∈ {(2,3),(6,1)})
= P((X,Y) = (2,3)) + P((X,Y) = (6,1))
= f(2,3) + f(6,1)
= 0.25 + 0.3
= 0.55

example:
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
calculate P(X=0,Y=1)
= f(0,1) = 1/2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P((X,Y) ∈ {(1,4),(1,6),(2,6)})
= 0.25 + 0.25 + 0.2
= 0.7

Calculating a joint probability containing inequalities from a table
   |Y=0 |Y=2 |Y=4
X=0|0.15|0.1 |0.1
X=1|0.05|0.1 |0.05
X=2|0.05|0.25|0.15
Compute P(X*Y >= 4)
= P((X,Y) ∈ {(1,4),(2,2),(2,4)})
= 0.05 + 0.25 + 0.15
= 0.45

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X = 2 and Y <= 4)
= P((X,Y) ∈ {(2,2),(2,4)})
= 0.05 + 0.15
= 0.2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X < 2 or Y = 6)
= P((X,Y) ∈ {(1,2),(1,4),(1,6),(2,6)})
= 0.1 + 0.25 + 0.25 + 0.2
= 0.8

===================================================

The joint PMF f(x,y) tells us all possible events (X,Y) and the probabilities associated with each event, however we wish to determine the PMF for X only how can this be deduced from the joint PMF?
consider the joint PMF
   |Y=1 |Y=2 |
X=1|0.05|0.15|
X=2|0.7 |0.1 |
The support of X is S_X = {1,2} lets now deduce the probabilites associated with each value of X in S_X
According to the law of total probability we can deduce P(X = 1) by summing the values in the first row of the table
P(X = 1) = P(X = 1,Y = 1) + P(X = 1,Y = 2)
= 0.05 + 0.15
= 0.2
summing the values in second row in table
P(X = 2) = P(X = 2,Y = 1) + P(X = 2,Y = 2)
= 0.7 + 0.1
= 0.8
PMF for X
x     | 1 | 2 |
f_X(x)|0.2|0.8|
f_X(x) was deduced from the joint probability mass function, we call f_X(x) the marginal mass function for X

Deducing the marginal mass function for Y
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
the marginal mass function in the right margin of the table
P(X = 1) = f_X(1) = 0.2
P(X = 2) = f_X(2) = 0.8
similary we can compute the marginal mass function of Y denoted f_Y(y) by summing the columns adding these totals to our table
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
f_Y|0.75|0.25| 1 |
P(Y = 1) = f_Y(1) = 0.75
P(Y = 2) = f_Y(2) = 0.25
marginal mass function f_Y(y) of Y
x     | 1  |  2 |
f_Y(y)|0.75|0.25|

A formal definition of the margin mass function
The maginal mass function of X denoted f_X(x)
f_X(x) = P(X = x) = Σ_y∈SY f(x,y)
The marginal mass function of Y denoted f_Y(y)
f_Y(y) = P(Y = x) = Σ_x∈SX f(x,y)
To compute P(X = x) for some particular x we sum all possible values of Y in the row corresponding to X = x
To compute P(Y = y) for some particular y we sum all possible values of X in the column corresponding to Y = y
Marginal mass functions are sometimes called marginal probability mass functions, marginal PMFs, or marginal distributions

Finding a marginal probability
   |Y=0 |Y=1 |
X=0|1/6 |5/12|
X=1|1/4 |1/6 |
P(X = x) = f_X(x) = Σ_y f(x,y)
P(Y = y) = f_Y(y) = Σ_x f(x,y)
The marginal distribution for X corresponds to the row totals and the marginal distribution for Y corresponds to the column totals.
   |Y=0 |Y=1 |f_X |
X=0|1/6 |5/12|7/12|
X=1|1/4 |1/6 |5/12|
f_Y|5/12|7/12|  1 |
P(X = 0) = f_X(0)
= f(0,0) + f(0,1)
= 1/6 + 5/12
= 7/12

example:
find P(X = 0)
   |Y=1 |Y=2 |
X=0|0.4 |0.2 |
X=1|0.15|0.25|
= 0.4 + 0.2
= 0.6

example:
find f_Y(0)
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
= 1/4 + 1/8
= 3/8

(rest of examples are simple table lookups and summations no new knowledge to note)

===================================================

Determining whether two random variables are independent
   |Y=0 |Y=1 |
X=0|0.24|0.06|
X=1|0.56|0.14|
marginal mass functions
x     |0  |1  |
f_X(x)|0.3|0.7|
---------------
y     |0  |1  |
f_Y(y)|0.8|0.2|
which of the following statements are true?
1. P(X = 0,Y = 0) = P(X = 0)*P(Y = 0)
2. P(X = x,Y = y) = P(X = x)*P(Y = y) for all possible x and y
3. X and Y are independent
Two discrete random variables X and Y with joint probability mass function f(x,y) are independent iff
f(x,y) = f_X(x)*f_Y(y)
Statements 1,2, and 3 is true. 1 is true because
P(X = 0,Y = 0) = f(0,0) = 0.24
P(X = 0) * P(Y = 0) = f_X(0) * f_Y(0) = 0.3 * 0.8 = 0.24
2 is true because
f(1,0) = 0.56 = 0.7 * 0.8 = f_X(1) * f_Y(0)
f(0,1) = 0.06 = 0.3 * 0.2 = f_X(0) * f_Y(1)
f(1,1) = 0.14 = 0.7 * 0.2 = f_X(1) * f_Y(1)
P(X = x,Y = y) = P(X = x) * P(Y = y) for all possible x and y
statement 3 is true because of 1 and 2 is true X and Y must be independent

example:
   |Y=0 |Y=1|
X=0|0.05|0.2|
X=1|0.15|0.6|
computing the marginal probability mass functions for X and Y
   |Y=0 |Y=1| f_X|
X=0|0.05|0.2|0.25|
X=1|0.15|0.6|0.75|
f_Y| 0.2|0.8|  1 |
check if these statements are true
this first statement is true:
P(X = 1,Y = 1) = P(X = 1) * P(Y = 1)
0.75 * 0.8 = 0.6
this is also true
P(X = x,Y = y) = P(X = x) * P(Y = y)
making X and Y independent

example:
   |Y=0 | Y=1|
X=0|0.05|0.15|
X=1|0.25|0.55|
computing the marginal probability mass functions for X and Y
   |Y=0 | Y=1| f_X|
X=0|0.05|0.15| 0.2|
X=1|0.25|0.55| 0.8|
f_Y| 0.3| 0.7|  1 |
check if these statements are true
this first statement is true:
P(X = 0,Y = 1) = P(X = 0) * P(Y = 1)
0.2 * 0.7 = 0.14 != 0.15
so this would also be false
P(X = x,Y = y) = P(X = x) * P(Y = y)
and X and Y are not independent

example:
   |Y=2| Y=4|
X=1|0.2| 0.2|
X=3|0.1|0.25|
X=5|0.2|0.05|
computing the marginal probability mass functions for X and Y
   |Y=2| Y=4| f_X|
X=1|0.2| 0.2| 0.4|
X=3|0.1|0.25|0.35|
X=5|0.2|0.05|0.25|
f_Y|0.5| 0.5|  1 |
check if these statements are true
this is true:
P(X = 1,Y = 2) = P(X = 1) * P(Y = 2)
0.4 * 0.5 = 0.2
this is false (only have to check one instance)!
P(X = x,Y = y) = P(X = x) * P(Y = y)
0.5 * 0.35 = 0.175 != 0.1
making X and Y dependent

example:
   |Y=1| Y=3| f_X|
X=2| * |0.27|  * |
X=4| * |0.63|  ? |
f_Y|0.1| 0.9|  1 |
find P(X = 4)
normally to cross reference we * by totals, we don't
have all totals but we can find it by the info given to us
by dividing total with the cross reference cell
0.63/0.9 = 0.7
P(X = 4) = 0.7

===================================================

Conditional distributions for discrete random variables
the multiplication law for conditional probability for two events A and B
P(A|B) = P(A ∩ B)/P(B)
suppose X and Y are discrete random variables the conditional probability X = x given that Y = y follows immediately from the multiplication law
P(X = x|Y = y) = P(X = x,Y - y)/P(Y = y) = f(x,y)/f_Y(y)
F(x,y) is the joint PMF of X and Y, f_Y(y) is the marginal mass function
To avoid getting a zero in the denominator the formula assumes that P(Y = y) = f_Y(y) != 0
In the case where P(Y = y) = 0 the corresponding probability for X does not exist it makes no sense to ask about the event of X = x occuring given that Y = y has occurred if the event Y = y is impossible
similarly the conditional probability Y = y given X = x
P(Y = y|X = x) = P(X = x,Y = y)/P(X = x) = f(x,y)/f_X(x)
where f_X(x) is the marginal mass function of X this formula assumes that P(X = x) != 0

Computing a conditional probability
   | Y=0| Y=1|
X=0| 0.5|0.05|
X=1|0.15| 0.3|
compute P(Y = 0|X = 1) given that the discrete random variables X and Y have the joint probability function f(x,y)
P(Y = 0|X = 1) = P(X = 1,Y = 0)/P(X = 1) = f(1,0)/f_X(1)
f_X(1) = f(1,0) + f(1,1)
= 0.15 + 0.3
= 0.45
substituting
f_X(1) = 0.45 and f(1,0) = 0.15 in our formula
P(Y = 0|X = 1) = f(1,0)/f_X(1)
= 0.15/0.45
= 1/3

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(X = 2|Y = 3)
P_Y(3) = 0.25 + 0.05 + 0.15
= 0.45
f(2,3)/P_Y(3)
= 0.25/0.45
~= 0.56

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(Y = 3|X = 2)
P_X(2) = 0.05 + 0.25 = 0.3
f(2,3)/f_X(2)
= 0.25/0.3
~= 0.83

Computing a conditional probability over an interval
   | Y=0| Y=1|Y=2 |Y=3|
X=2| 0.2|   0|0.05|0.1|
X=4| 0.1|0.05|0.15|  0|
X=6| 0.2|0.05|   0|0.1|
Compute P(0 < Y < 3|X = 0)
= P((0 < Y < 3), X = 0)/P(X = 0)
= P(X = 0, (0 < Y < 3))/P(X = 0)
= P(X = 0,Y = 0) + P(X = 0,Y = 2)/P(X = 0)
= (f(0,1) + f(0,2))/f_X(0)
f_X(0) = 0.2 + 0 + 0.05 + 0.1
= 0.35
= 0 + 0.05/0.35
= 0.05/0.35
~= 0.14

example:
    | Y=0| Y=2| Y=4|
X=0 |0.15| 0.1| 0.1|
X=4 |0.05| 0.1|0.05|
X=8 | 0  | 0.2|0.15|
X=12|0.05|0.05|   0|
Compute P(4 <= X <= 8|Y = 0)
= (f(4,0) + (8,0))/f_Y(0)
= 0.05 + 0
f_Y(0) = 0.15 + 0.05 + 0 + 0.05 = 0.25
(0.05 + 0)/0.25
~= 0.2

example:
    | Y=0| Y=2| Y=4|Y=6|
X=1 |1/18|5/18| 1/9|2/9|
X=2 | 1/9|1/18|   0|1/6|
Compute P(2 <= Y <= 6|X = 2)
f_X(2) = 1/9 + 1/18 + 0 + 1/6
= 2/18 + 1/18 + 3/18
= 6/18
= 1/3
f(2,2) + f(2,4) + f(2,6)
= 1/18 + 0 + 1/6
= 4/18
= 2/9
= ((2/9)/(1/3)) 
= 2/9 * 3/1
= 6/9 = 2/3

Conditional probability mass functions
We can now define two more probability mass functions
The conditional probability mass function of X given that Y = y denoted f_X|Y(x|y)
f_X|Y(x|y)
= P(X = x,Y = y) = P(X = x,Y = y)/P(Y = y) 
= f(x,y)/f_Y(y)
f(x,y) is the joint probability mass function of X and Y and f_Y(y) is the marginal mass function of Y
The conditional probability mass function of Y given that X = x denoted f_Y|X(y|x)
f_Y|X(y|x)
= P(Y = y,X = x) = P(X = x,Y = y)/P(X = x)
= f(x,y)/f_X(x)
f(x,y) is a joint probability mass function of X and Y and f_X(x) in the marginal mass function of X
The conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y and independent
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)
X and Y are independent, knowing the value of one random variable does not affect the probability distribution of the other.

Computing the value of a conditional probability mass function at a point
    | Y=0| Y=1| Y=2|
X=1 | 1/6| 1/9| 1/6|
X=2 |1/18|1/18| 1/6|
X=3 |1/9 | 1/9|1/18|
Compute f_X|Y(2|2)
= f(x,y)/f_Y(2)
f_Y(2) = 1/6 + 1/6 + 1/18
= 3/18 + 3/18 + 1/18
= 7/18
f(2,2) = 1/6
= (1/6)/(7/18)
= 18/42
= 3/7

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Comput f_X|Y(1|0)
f_Y(0) = 0.3 + 0.1 = 0.4
f(1,0) = 0.1
0.1/0.4 = 0.25

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Compute f_Y|X(1|0)
f_X(0) = 0.3 + 0.2 + 0.1 = 0.6
f(0,1) = 0.2
0.2/0.6
~= 0.33

Computing conditional probability mass function
   | Y=1| Y=3| Y=5|
X=0| 0.1|   0| 0.2|
X=2| 0.3| 0.1|   0|
X=4| 0.1|   0| 0.2|
find f_X|Y(x|1)
= f(x,1)/f_Y(1)
x ∈ {0,2,4}
P(X = 0|Y = 1) = f(0,1)/f_Y(1) = 0.1/0.5 = 1/5
P(X = 2|Y = 1) = f(2,1)/f_Y(1) = 0.3/0.5 = 3/5
P(X = 4|Y = 1) = f(4,1)/f_Y(1) = 0.1/0.5 = 1/5
x         |0  |2  |4  |
f_X|Y(x|1)|1/5|3/5|1/5|
-----------------------

(rest of the example were faster to answer and calculate in my head then creating tables here for notes)

===================================================

The joint cumulative distribution function (or joint CDF)
F(x,y) = P(X <= x,Y <= y) = Σ_xi<=xΣ_yi<=y(f(xi,yi))
the sets S_X and S_Y are called the supports of X and Y
joint CDF is analogous to the cumulative distribution function of a single random variable
joint PMF:
   | Y=0| Y=1| Y=2|
X=0| 0.1| 0.2| 0.1|
X=1| 0.3| 0.2| 0.1|
the supports of X and Y are S_X = {0,1} and S_Y = {0,1,2}
lets compute F(1,1)
F(1,1) = P(X <= 1, Y <= 1)
f(0,0) + f(0,1) + f(1,0) + f(1,1)
= 0.1 + 0.2 + 0.3 + 0.2
= 0.8

Evaluating a joint CDF
   |Y=-2|Y=-1| Y=0|Y=2|
X=1| 0.1| 0.3|0.05|0.1|
X=3| 0.1|0.05| 0.1|0.2|
find F(2,1)
F(2,1) = P(X <= 2, Y <= 1)
f(1,-2) + f(1,-1) + f(1,0)
= 0.1 + 0.3 + 0.05
= 0.45

example:
   | Y=2| Y=4| Y=6|
X=1| 0.1|0.25|0.25|
X=2|0.05|0.15| 0.2|
find F(2,4)
F(2,4) = P(X <= 2, Y <= 4)
f(1,2) + f(1,4) + f(2,2) + f(2,4)
0.1 + 0.25 + 0.05 + 0.15
= 0.55

example:
   | Y=0| Y=1|
X=2| 0.2|0.25|
X=4|0.16|0.15|
X=6|0.04|0.15|
find F(5,1/2)
F(5,1/2) = P(X <= 5, Y <= 1/2)
f(2,0) + f(4,0)
0.2 + 0.16
= 0.36

Some properties of the joint CDF
0 <= F(x,y) <= 1
the marginal CDF of X is given by F_X(x) = F(x, ∞)
the marginal CDF of Y is given by F_Y(y) = F(∞, y)
F(∞, ∞) = 1
F(-∞, y) = F(x, -∞) = 0
if X and Y are independent random variables then F(x,y) = F_X(x)*F_Y(y)

Interpreting the joint CDF Geometrically
(think of f(x,y) as our top right corner)
S_X = {1,2,3,4}, S_Y = {2,3,4}
find an expression for the corresponding joint CDF when x > 4, y < 2
We can think of each point where f(x,y) != 0 as a point mass in the plane where the probability mass at each point equals the probability associated with that point
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis
all our points are on S = S_X x S_Y = {1,2,3,4} x {2,3,4} our region lies x > 4 and y < 2 there are no points that lie in our region
f(x,y) = 0

example:
S_X = {-2,0,2}, S_Y = {0,1,2,3}
when x < -3, y > 3
f(x,y) = 0
the region is outside any of our supports

example:
when x > 4, y > 3
S_X = {1,2,3,4}, S_Y = {0,1,2,3}
f(x,y) = 1
region includes all points

Finding the cumulative probabilities given values from the joint CDF
F(5,4) = 0.9, F(5,2) = 0.2, F(3,4) = 0.5, F(3,2) = 0.1
S_X = {1,3,5,7}, S_Y = {1,2,3,4,5}
Find P(3 < X <= 5, 2 < Y <= 4)
For a given joint PMF f(x,y) we can think of each point where f(x,y) != 0 as a point mass in the plane, where the probability mass at each point equals the probability associated with that point.
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis.
S = S_X x S_Y = {1,3,5,7} x {1,2,3,4,5}
if we want to find a sub-region within our support say at (5,4) 
subtract the infinite rectangles that have their top-right at (5,2) and (3,4)
we need to add back the infinite rectangle that has its top-right corner at (3,2) this is because we subtracted this region twice in step 2
Find P(3 < X <= 5, 2 < Y <= 4)
top-right corner at (5,4):
= P(X <= 5, Y <= 4)
top-right corner at (5,2) - top-right corner at (3,4)
- P(X <= 5, Y <= 2) - P(X <= 3, Y <= 4)
top-right corner at (3,2)
+ P(X <= 3, Y <= 2)
= F(5,4) - F(5,2) - F(3,4) + F(3,2)
= 0.9 - 0.2 - 0.5 + 0.1
= 0.3

example:
joint CDF of X and Y be F(x,y)
F(9,10) = 0.65, F(6,10) = 0.31, F(3,10) = 0.12
supports are
S_X = {3,6,9,12}, S_Y = {2,4,6,8,10,12}
Find P(3 < X <= 9,Y <= 10)
top-right corner at (9,10) - top-right corner at (3,10):
= P(X <= 9, Y <= 10) - P(X <= 3, Y <= 10)
= F(9,10) - F(3,10)
= 0.65 - 0.12
= 0.53

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.72, F(10,6) = 0.3, F(8,9) = 0.15, F(8,6) = 0.08
supports of X and Y
S_X {2,4,6,8,10,12}, S_Y = {3,6,9,12}
find P(8 < X <= 10, 6 < Y <= 9)
top-right corner at (10,9)
P(X <= 10, Y <= 9)
top-right corner at (10,6) - top-right corner at (8,9)
- P(X <= 10, Y <= 6) - P(X <= 8, Y <= 9)
top-right corner at (8,6)
+ P(X <= 8, Y <= 6)
= F(10,9) - F(10,6) - F(8,9) + F(8,6)
= 0.72 - 0.3 - 0.15 + 0.08
= 0.35

Further finding cumulative probabilities given values from the joint CDF
the joint CDF of X and Y be F(x,y)
F(8,3) = 0.9, F(6,3) = 0.7, F(4,3) = 0.4
S_X = {2,4,6,8}, S_Y = {1,3,5}
P(X > 6, Y <= 4)
top-right corner at (8,3) - top-right corner at (6,3)
P(X <= 8, Y <= 3) - P(X <= 6, Y <= 3)
= F(8,3) - F(6,3)
= 0.9 - 0.7
= 0.2

example:
joint CDF
F(6,5) = 3/5, F(6,3) = 2/5, F(6,2) = 1/10
S_X = {2,6,10}, S_Y = {0,1,2,3,4,5}
find P(X < 7, Y > 3)
top-right corner at (6,5) - top-right corner at (6,3)
P(X <= 6,Y <= 5) - P(X <= 6,Y <= 3)
F(6,5) - F(6,3)
= 3/5 - 2/5
= 1/5

example:
joint CDF
F(7,1) = 9/16, F(7,-2) = 1/4, F(3,1) = 1/8, F(3,-2) = 1/16
S_X = {1,3,5,7,9}, S_Y = {-2,-1,0,1,2}
P(5 <= X <= 8,-1 <= Y <= 1)
9/16 - 4/16 - 2/16 + 1/16
4/16 = 1/4

===================================================

Joint distributions for continuous random variables
A function f(x,y) is a joint probability density function (or joint PDF) for the two continuous random variables X and Y if the following is satisfied
f(x,y) >= 0 for all (x,y) ∈ R^2
states f must be nonnegative
∫∫_R^2 f(x,y)dxdy = 1
the sum (integral) of f over possible X and Y values must add up to 1
P((X,Y) ∈ A) = ∫∫_A f(x,y)dxdy where A is a subset of R^2 
compute the probability of an event A by summing (integrating) f over all regions associated with A
analog of those for the joint PMF of two discrete random variables
they are two dimensional analog for the PDF f(x) of a (single) continuous random variable X

Verifying that a function is a valid joint PDF
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified
to check the second condition we need to integrate
∫∫_R^2 f(x,y) dxdy = ∫∫_D f(x,y) dxdy
= ∫^1_0 ∫^1_0 3/2(x^2 + y^2) dydx
= 3/2 ∫^1_0 ∫^1_0 x^2 + y^2 dydx
= 3/2 ∫^1_0 [x^2y + y^3/3]|_0-1 dx
= 3/2 ∫^1_0 (x^2 + 1/3) dx
= 3/2[x^3/3 + x/3]|_0-1
= 3/2(1/3 + 1/3)
= 1
therefore second condition is satisfied
f(x,y) is a valid joint probability density function

Joint PDFs for continuous random variables
f(x,y) = {
	csin(x + y), 0 < x < pi/2, 0 < y < pi/2
	0,           otherwise
}
verify that f(x,y) is a valid joint PDF
c > 0 we have f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified 
∫^pi/2_0[∫^pi/2_0 csin(x + y) dy]dx
= c ∫^pi/2_0[∫^pi/2_0 sin(x + y) dy]dx
= c ∫^pi/2_0 [-cos(x + y)]|_0-pi/2 dx
= c ∫^pi/2_0 (-cos(x + pi/2) + cos(x)) dx
= c ∫^pi/2_0 cos(x) - cos(x + pi/2) dx
integrate with respect to x
= c[sin(x) - sin(x + pi/2)]|_0-pi/2
= c[(sin(pi/2) - sin(pi)) - (sin(0) - sin(pi/2))]
= c[(1 - 0) - (0 - 1)]
= c[1 - (-1)]
= 2c
hence
2c = 1  =>  c = 1/2

example:
joint PDF:
f(x,y) = {
	c, 0 < x < 2, 1 < y < 2
	0, otherwise
}
c is a positive real number, find the value of c
∫^2_0 ∫^2_1 c dydx
= ∫^2_0 ∫^2_1 cy dydx
= c ∫^2_0 [y]|_1-2 dx
= c ∫^2_0 1 dx
= c[x]|_0-2
= c(2 - 0)
= 2c
2c = 1
c = 1/2

Joint PDFs for continuous random variables (non-rectangular domains)
joint PDF:
f(x,y) = {
	c/(1+x+y)^3, 0 <= x < y
	0, 			 otherwise
}
where c is a positive real number, find the value of c
c > 0, f(x,y) for all (x,y) ∈ R^2 the first condition is satisfied
∫^∞_0 [∫^∞_x c/(1 + x + y)^3 dy] dx
= c ∫^∞_0 [∫^∞_x (1 + x + y)^-3 dy] dx
= c ∫^∞_0 [-1/2(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 [(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 0 - (1 + 2x)^-2 dx
= -c/2 ∫^∞_0 -(1 + 2x)^-2 dx
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
integrate with respect to x
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
= c/2[-1/2(1 + 2x)^-1]|_0-∞
= -c/4[1/1 + 2x]|0-∞
= -c/4[0 - 1]
= c/4
c/4 = 1  => c = 4

example:
joint PDF
f(x,y) = {
	cxy, 0 <= x <= y < 1
	0,   otherwise
}
find value of c
∫^1_0 ∫^1_x cxy dydx
= ∫^1_0 [cxy^2/2]|_x-1 dx
= ∫^1_0 (cx/2 - cx^3/2) dx
integrate with respect to x
= [cx^2/4 - cx^4/8]|_0-1
= c/4 - c/8
= c/8
c/8 = 1  =>  c = 8

example:
joint PDF
f(x,y) = {
	cx^2y, x^2 <= y <= 1
	0,     otherwise
}
find value of c
∫^1_-1 ∫^1_x^2 cx^2y dydx
= ∫^1_-1 [cx^2 * y^2/2]|_x^2-1 dx
= ∫^1_-1 cx^2((1 - x^4)/2)]|_x^2-1 dx
respect to x
= [c(x^3/6 - x^7/14)]|_-1-1
= 4c/21
4c/21 = 1  => c = 21/4

Finding probabilities using joint PDFs
f(x,y) is a joint probability density function for two continuous random variables X and Y
P((X,Y) ∈ A) = ∫∫_A f(x,y) dxdy
if A is the event defined as
A = {(X,Y) | a <= X <= b, c <= Y <= d}
the corresponding probability can be computed as
P(a <= X <= b, c <= Y <= d) = ∫^d_c ∫^b_a f(x,y) dxdy
= ∫^b_a ∫^d_c dydx
analogously to the case of a single continuous random variable the probability that (X,Y) takes on any particular value (x,y) is zero
P(X = x,Y = y) = 0

Computing a probability using a joint PDF
f(x,y) = {
	1/6, 0 <= x <= 3, 0 <= y <= 2
	0,   otherwise
}
find P(X > 1, Y > 1)
= 1/6 ∫∫_A dxdy
= 1/6Area(A)
= 1/6 * (3 - 1) * (2 - 1)
= 1/6 * 2 * 1
= 1/3

example:
PDF:
f(x,y) = {
	1, 0 <= x <= 1, 0 <= y <= 1
	0, otherwise
}
find P(X > Y)
= 0.5

example:
PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
find P(X > 1/2, Y > 1/2)
∫^1_1/2 ∫^1_1/2 4xy dxdy
∫^1_1/2 [∫^1_1/2 4xy dx] dy
∫^1_1/2 [2x^2y]|_1/2-1 dy
∫^1_1/2 3y/2 dy
= [3y^2/4]|_1/2-1
= 9/16

===================================================

Marginal distributions for continuous random variables
Suppose that X and Y are continuous random variables with joint probability density function f(x,y)
the marginal density function of X
f_X(x) = ∫^∞_-∞ f(x,y) dy
the marginal density function of Y
f_Y(y) = ∫^∞_-∞ f(x,y) dx
marginal density function can be known as marginal probability density function or marginal PDF
To convert from discrete to continuous we turn sums into integrals
when we compute f_X in the discrete case, we sum over all possible values of Y in the continuous case we integrate with repect to y
vise-versa for f_Y

example:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
lets find the expression for the MDF f_X(x) of X
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 x^2 + y^2 dy
= 3/2[x^2y + y^3/3]|_0-1
= 3/2((x^2*1 1^3/3) - (0))
= 3/2[x^2 + 1/3]
= 1/2(3x^2 + 1)
f_X(x) = {
	1/2(3x^2 + 1), 0 <= x <= 1
	0,             otherwise
}

finding a marginal distribution from a joint distribution
joint PDF
f(x,y) = {
	15/7(x^2y^4), 1 <= x <= 2, 0 <= y <= 1
	0,            otherwise
}
find expression for the marginal PDF f_Y(y)
the marginal density function of Y for 0 <= y <= 1
f_Y(y) ∫^2_1 15/7(x^2y^4) dx
15/7(y^4)[x^3/3]|_1-2
= 5/7(y^4)[x^3]|_1-2
= 5/7(y^4)(2^3 - 1^3)
= 5y^4
full expression
f_Y(y) = {
	5y^4, 0 <= y <= 1
	0,	  otherwise
}

example:
X and Y be two continuous random variables
joint PDF:
f(x,y) = {
	3/2(y^2), 0 <= x <= 2, 0 <= y <= 1,
	0,        otherwise
}
find the expression for the marginal PDF f_X(x)
∫^1_0 3/2(y^2) dy
= 3/2[y^3/3]|_0-1
= 1/2[y^3]|_0-1
= 1/2(1^3 - 0)
= 1/2
f_X(x) = {
	1/2, 0 <= x <= 2,
	0,   otherwise
}

example:
joint PDF:
f(x,y) = {
	8/3(x^3y), 0 <= x <= 1, 1 <= y <= 2
	0,   	   otherwise
}
find the marginal PDF f_Y(y)
∫^1_0 8/3(x^3y) dx
= 8/3y[x^4/4]|_0-1
= 2/3(y)[x^4]
= 2/3(y)(1^4 - 0)
= 2/3y

Finding a marginal probability from a joint PDF
f(x,y) = {
	e^-(4x+y/4), x >= 0, y >= 0
	0,           otherwise
}
find P(-4 <= Y <= 4)
the marginal density function of Y for y >= 0
f_Y(y) ∫^∞_0 e^-(4x+y/4) dx
= [-1/4e^-(4x+y/4)]|_0-∞
= -1/4(0 - e^-(0+y/4))
= 1/4e^(-y/4)
full expression for f_y(y)
f_y(y) = {
	1/4e^(-y/4), y >= 0
	0,         otherwise
}
the required probability is
P(-4 <= Y <= 4) = ∫^4_-4 f_Y(y) dy
= ∫^4_0 1/4e^(-y/4) dy
= [-e^(-y/4)]|_0-4
= (-e^(-4/4) + e^0)
= 1 - 1/e

example:
joint PDF:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,				otherwise
}
P(0.2 < X < 1)
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 (x^2 + y^2) dy
= [3/2(x^2y) + 3/6(y^3)]|_0-1
= [3/2(x^2y) + 1/2(y^3)]|_0-1
= [3/2(x^2y) + y^3/2]|_0-1
= (3/2(x^2) + 1/2) - 0
= 3/2(x^2) + 1/2
full expression for f_X(x)
f_X(x) = {
	3/2(x^2) + 1/2, 0 <= x <= 1
	0,              otherwise
}
∫^1_0.2 (3/2(x^2) + 1/2) dx
= [x^3/2 + 1/2x]|_0.2-1
= [1/2 + 1/2] - [(0.2)^3/2 + 0.2/2]
= 1 - 0.104
= 0.896

example:
joint PDF:
f(x,y) = {
	1/6(x + y), 0 <= x <= 1, 0 <= y <= 3
	0,			otherwise
}
P(Y <= 1)
∫^1_0 1/6(x + y) dx 
= [x^2/12 + yx/6]|_0-1
= (1/12 + y/6) - 0
full expression for f_X(x)
f_Y(y) = {
	1/12 + y/6, 0 <= y <= 3
	0,          otherwise
}
required probability
P(Y <= 1) = ∫^1_-∞ f_Y(y) dy
∫^1_0 (1/12 + y/6) dy
= [y/12 + y^2/12]|_0-1
= (1/12 + 1/12) - 0
= 1/6

Finding a marginal distribution from a joint distribution non-rectangular domains
joint PDF:
f(x,y) = 5ysqrt(x), y^2 <= x <= 1, y >= 0
find the expression for the marginal PDF f_Y(y)
0 <= y <= 1
f_Y(y) = ∫^1_y^2 5ysqrt(x) dx
= 5y ∫^1_y^2 sqrt(x) dx
= 5y[2/3(x^3/2)]|_y^2-1
= 10y/3[x^(3/2)]|_y^2-1
= 10y/3[1^(3/2) - (y^2)^(3/2)]
= 10y/3[1 - (y^3)]
= 10(y - y^4)/3
full expression for f_Y(y)
f_Y(y) = {
	10(y - y^4)/3, 0 <= y <= 1
	0, 			   otherwise
}

example:
joint PDF:
f(x,y) = 1/2xy 0 <= x <= y <= 2
find the expression for the marginal PDF f_X(x)
∫^2_x 1/2xy dy
= [xy^2/4]|_x-2
= (x - x^3/4)
f_X(x) = {
	x - x^3/4, 0 <= x <= 2
	0,         otherwise
}

example:
joint PDF:
f(x,y) = 1/(pi)(x^2), y^2 + 1 <= x
find the expression for the marginal PDF f_Y(y)
∫^∞_(y^2 + 1) 1/(pi)(x^2) dx
1/pi ∫^∞_(y^2 + 1) x^-2 dx
[-1/(pi)(x)]|_(y^2 + 1)-∞
= 0 - (-1/(pi)(y^2 + 1))
f_Y(y) = 1/pi(y^2 + 1)

Marginal cumulative distribution functions
joint probability density function
f(x,y) = {
	3/8(x + y)^2, -1 <= x <= 1, -1 <= y <= 1
	0,            otherwise
}
find the marginal cumulative distribution function (CDF) F_Y(y) of Y
the marginal function of Y for -1 <= y <= 1
f_Y(y) = ∫^1_-1 3/8(x + y)^2 dx
= 3/8[1/3(x + y)^3]|_-1-1
= 1/8[(x + y)^3]|_-1-1
= 1/8[(1 + y)^3 - (-1 + y)^3]
= 1/8[(1 + y)^3 - (y - 1)^3]
= 1/8[(y^3 + 3y^2 + 3y + 1) - (y^3 - 3y^2 + 3y - 1)]
(cancel out terms by distributing -)
= 1/8[2(3y^2 + 1)]
= 1/4(3y^2 + 1)
the marginal CDF of Y for -1 <= y <= 1
F_Y(y) = ∫^y_-∞ fx(u) du
= ∫^y_-1 1/4(3u^2 + 1) du
= 1/4 ∫^y_-1 (3u^2 + 1) du
= 1/4[u^3 + u]|_-1-y
= 1/4[y^3 + y - (-1)^3 - (-1)]
= 1/4[y^3 + y + 2]
= (y^3 + y + 2)/4
full expression for the marginal CDF of Y is
F_Y(y) = {
	0, 		          y < -1
	(y^3 + y + 2)/4, -1 <= y <= 1
	1,                y > 1
}

example:
joint PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
the marginal CDF F_X(x) of X
F_X(x) = {
	0, 	  x < 0
	g(x), 0 <= x <= 1
	1, 	  x > 1
}
what is the function g(x)
∫^1_0 4xy dy
= 4x[y^2/2]|_0-1
= 2x[y^2]|_0-1
= 2x(1^2 - 0)
= 2x
the marginal CDF of X for 0 <= x <= 1
∫^1_0 2u du
= [u^2]|_0-1
= x^2 - 0
= x^2
full expression F_X(x)
F_X(x) = {
	0,   x < 0
	x^2, 0 <= x <= 1
	1,   x > 1
}
g(x) = x^2

example:
joint PDF:
f(x,y) = {
	2y/(x + 1)^2 x >= 0, 0 <= y <= 1
	0,			 otherwise
}
the marginal CDF F_Y(y) of Y
F_Y(y) = {
	0,    y < 0
	g(y), 0 <= y <= 1
	1,    y > 1 
}
what is the function g(y)
∫^∞_0 2y/(x + 1)^2 dx
= 2y ∫^b_0 [(x + 1)^-1] dx
= 2y[-1/(x + 1)]|_0-b
= 2y((-1/b + 1) + (1/0 + 1))
= 2y(1 - 1/(b + 1))
(when b approaches infinity 1/(b + 1) approaches 0 simplifies)
= 2y
∫^y_0 2u du
= [u^2]|_0-y
= y^2 - 0
= y^2
full expression F_Y(y)
F_Y(y) = {
	0,   y < 0
	y^2, 0 <= y <= 1
	1,   y > 1
}
g(y) = y^2

===================================================

Independence of continuous random variables
Let X and Y be continuous random variables X and Y are independent if for all A ⊆ R and B ⊆ R
P(X ∈ A,Y ∈ B) = P(X ∈ A) * P(Y ∈ B)
X and Y are independent iff
f(x,y) = f_X(x) * f_Y(y)
(we've seen this before)
direct analogy with the case of independence for discrete random variables.

Finding the joint PDF of two independent random variables using marginal PDFs
X and Y are two independent continuous random variables whose MDF are
f_X(x) = {
	2/9(x), 0 <= x <= 3
	0,      otherwise
}
f_Y(y) = {
	1/4(y^3), 0 <= y <= 2
	0,        otherwise
}
find the joint probability density function f(x,y)
since our random variables are independent
f(x,y) = f_X(x) * f_Y(y)
= 2/9(x) * 1/4(y^3)
= 2/36(xy^2)
= 1/18(xy^2)
where (x,y) ∈ [0,3] x [0,2] the full expression f(x,y)
f(x,y) = {
	1/18(xy^3), 0 <= x <= 3, 0 <= y <= 2
	0,          otherwise
}

example:
X and Y and their marginal density functions:
f_X(x) = {
	4x^3, 0 <= x <= 1
	0,    otherwise
}
f_Y(y) = {
	2/3y, 1 <= y <= 2
	0,    otherwise
}
find the joint probability density function f(x,y)
X and Y are independent so 
f(x,y) = f_X(x) * f_Y(y)
4x^3 * 2/3y
= 8/3x^3y
where (x,y) ∈ [0,1] x [1,2] the full expression f(x,y)
f(x,y) = {
	8/3(x^3y), 0 <= x <= 1, 1 <= y <= 2
	0,         otherwise
}

example:
X and Y are two continuous random variables whose marginal density functions are
f_X(x) = {
	3x^2, 0 <= x <= 1
	0,    otherwise
}
f_Y(y) = {
	4y^3, 0 <= y <= 1
	0,    otherwise
}
find the joint probability density function f(x,y)
hint: We don't know if X and Y are independent
if we don't know if X and Y and independent then its impossible to determine using only the given information

Finding a joint probability for some independent random variables using marginal PDFs
Let X and Y be two independent continuous random variables whose marginal density functions
f_X(x) = {
	10/3x^2, 2 <= x <= 5
	0,       otherwise
}
f_Y(y) = {
	1/4(y), 1 <= y <= 3
	0,      otherwise
}
find P(X >= 3,Y <= 2)
f(x,y) = f_X(x) * f_Y(y)
= 10/3x^2 * 1/4(y)
= 5y/6x^2
where (x,y) ∈ [2,5] x [1,3] the full expression f(x,y)
f(x,y) = {
	5y/6x^2, 2 <= x <= 5, 1 <= y <= 3
	0,       otherwise
}
required probability
P(X >= 3,Y <= 2)
= ∫^5_3 ∫^2_1 5y/6x^2 dy dx
= ∫^5_3 [∫^2_1 5y/6x^2 dy] dx
= ∫^5_3 5/6x^2 [y^2/2]|_1-2 dx
= ∫^5_3 5y/6x^2 (3/2) dx
= ∫^5_3 5/4x^2 dx
= [-5/4x]|_3-5
= -5/4(1/5 - 1/3)
= 1/6

example:
X and Y are two independent continuous random variables MDF
f_X(x) = {
	2x, 0 <= x <= 1
	0,  otherwise
}
f_Y(y) = {
	3y, 0 <= y <= sqrt(2/3)
	0,  otherwise
}
find P(X <= 0.5,Y <= 1)
f(x,y) = f_X(x) * f_Y(y)
2x * 3y = 6xy
where (x,y) ∈ [0,1] x [0,sqrt(2/3)] the full expression f(x,y)
f(x,y) = {
	6xy, 0 <= x <= 1, 0 <= y <= sqrt(2/3)
	0,   otherwise
}
required probability
= ∫^0.5_0 [∫^1_0 6xy dy] dx
= ∫^0.5_0 6x[y]|_0-1 dx
= ∫^0.5_0 6x[y^2/2]|_0-1 dx
= ∫^0.5_0 6x(1/3) dx
= ∫^0.5_0 3x(1) dx
= ∫^0.5_0 3x dx
= [3x^2/2]|_0-0.5
= ((3(0.5)^2/2) - (0))
= 0.375

example:
MDF:
f_X(x) = {
	3/2(sqrt(x)), 0 <= x <= 1
	0,  		  otherwise
}
f_Y(y) = {
	2/3(y), 1 <= y <= 2
	0,      otherwise
}
find P(0 <= X <= 0.4, Y <= 2)
3/2(sqrt(x)) * 2/3(y)
= ysqrt(x)
where (x,y) ∈ [0,1] x [1,2] the full expression f(x,y)
f(x,y) = {
	ysqrt(x), 0 <= x <= 1, 1 <= y <= 2
	0,		  otherwise
}
required probability
= ∫^0.4_0 [∫^2_1 ysqrt(x) dy] dx
= ∫^0.4_0 sqrt(x)[y^2/2]|_1-2 dx
= ∫^0.4_0 sqrt(x)(4/2 - 1/2) dx
= ∫^0.4_0 sqrt(x)(3/2) dx
= 3/2[x^1/2]|_0-0.4
= 3/2[2/3(x^(3/2))]|_0-0.4
= 3/2(2/3(0.4)^(3/2) - 2/3(0)^(3/2))
(0.4^(3/2) = 0.4^1 * 0.4^1/2 = 0.4 * sqrt(0.4))
(sqrt(0.4) ~= 0.6325)
(0.4 * 0.253)
= 2/3 * 0.253 ~= 0.506/3 ~= 0.16867
= 3/2(0.16867)
~= 0.253

Determining whether two continuous random variables are independent
The joint PDF for the continuous random variables X and Y
f(x,y) = {
	1/4(xe^-y/2), 0 <= x <= 2, y >= 0
	0,            otherwise
}
which statements are true?
1. f_Y(y) = 1/2(e^-y/2) for y ∈ [0, ∞)
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
statement 1 is true, computing the MDF for Y
f_Y(y) = ∫^2_0 1/4(xe^-y/2) dx
= 1/4(e^-y/2) [x^2/2]|_0-2
= 1/2(e^-y/2)
where y ∈ [0, ∞)
statement 2 is true, computing the marginal density function for X
f_X(x) = ∫^∞_0 1/4xe^-y/2 dy
= 1/4(x) ∫^b_0 e^-y/2 dy
= 1/4(x) [-2e^-y/2]|_0-b
= 1/4(x) (-2e^-b/2 + 2)
(-2e^-b/2 b -> ∞ this becomes 0)
we are left with
= x/4(2)
= x/2
when x ∈ [0,2] and y ∈ [0, ∞)
1/4(xe^-y/2) <- f(x,y)
= x/2 <- f_X(x)  *  1/2(e^-y/2) <- f_Y(y)
Statement 3 is also true, because of 2 X and Y are independent

example:
joint PDF:
f(x,y) = {
	2/3(xy^3), 2 <= x <= 4, 0 <= y <= 1
	0,         otherwise
}
Which statements are true?
1. f_X(x) = 2x for x ∈ [2,4]
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
f_X(x) = ∫^1_0 2/3(xy^3) dy
= 1/3(x)[y^4/2]|_0-1
= 1/3(x)(1/2)
= 1/6(x)
statement 1 is false because 2x is not x/6
f_Y(y) = ∫^4_2 2/3(xy^3) dx
= 2/3(y^3) [x^2/2]|_2-4
= 2/3(y^3) (16/2 - 4/2)
= 2/3(y^3) (16/2 - 4/2)
= 2/3(y^3)(6)
= 4y^3
Statement 3 is true, since 2 is true. X and Y are independent
f(x,y) = f_X(x) * f_Y(y)
2/3(xy^3) = 1/6(x) * 4y^3

example:
joint PDF:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
1. f_X(x) = 3/2(x^2) + 1/2 for x ∈ [0,1]
2. f(x,y) = f_X(x) * f_Y(y) for all possible x and y
3. X and Y are independent
f_X(x) = ∫^1_0 3/2(x^2 + y^2) dy
= 3/2(x^2) ∫^1_0 3/2[y^2] dy
= 3/2(x^2) + 3/2[y^3/3]|_0-1
= 3/2(x^2) + 3/2(1/3)
= 3/2(x^2) + 1/2
statement 1 is true
f_Y(y) = ∫^1_0 3/2(x^2 + y^2) dx
= 3/2[x^3/3 + xy^2]|_0-1
= 3/2(y^2) + 1/2
statement 2 is false
f(x,y) 		   !=       f_X(x) 	  *     f_Y(y)
3/2(x^2 + y^2) != (3/2(x^2) + 1/2)(3/2(y^2) + 1/2)
statement 3 is false, since statment 2 is not true X and Y are not independent therefore correct answer is 1 only

theorem:
Suppose that the joint support of two continuous random variables X and Y with a joint probability density function f(x,y) is rectangular if
f(x,y) = g(x) * h(x)
for some two functions g and h (not necessarily PDF) then X and Y are independent, the rectangular joint support S does not need to be finite.
For example random variables X and Y with the following joint PDF:
f(x,y) = {
	1/4(xe^-y/2), 0 <= x <= 2, y >= 0
	0,			  otherwise
}
the joint support [0,2] x [0, ∞) is rectangular, we can write f(x,y) = g(x) * h(x) where
g(x) = 1/4x,  h(y) = e^-y/2
X and Y are independent

===================================================
**
For continuous random variables X and Y the conditional probability density function of X is given Y = y is
f_X|Y(x|y) = f(x,y)/f_Y(y), f_Y(y) != 0
and the CPD function Y given X = x is
f_Y|X(y|x) = f(x,y)/f_X(x), f_X(x) != 0
(just like discrete random variables)
To compute probabilities we work directly with these conditional PDFs to calculate the conditional probability of X given Y we use
P(X ∈ A|Y = y) = ∫_A f_X|Y(x|y) dx
the conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y are independent then
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)

Continuous random variables X and Y which the CPD function f_Y|X(y|x) and the MDF f_X(x)
f_Y|X(y|x) = {
	3(x^2 + y^2)/(3x^2 + 1), 0 <= x <= 1, 0 <= y <= 1
	0, 					     otherwise
}
f_X(x) = {
	(3x^2 + 1)/2, 0 <= x <= 1
	0,            otherwise
}
find the expression of the joint probability density function f(x,y) for 0 <= x <= 1, 0 <= y <= 1
conditional probability density function of Y given X = x for two continuous random variables X and Y
f_Y|X(y|x) = f(x,y)/f_X(x)
f(x,y) is the joint PDF for X and Y
f_X(x) is the MDF for X
from the equation 
f(x,y) = f_Y|X(y|x) * f_X(x)
0 <= x <= 1, 0 <= y <= 1
= 3(x^2 + y^2)/(3x^2 + 1) * (3x^2 + 1)/2
= 3(x^2 + y^2)/2
for other values of x and y the function f_Y|X(y|x) equals 0 the product will be equal to 0 as well.
the PDF:
f(x,y) = {
	3(x^2 + y^2)/2, (3x^2 + 1)/2, 0 <= x <= 1
	0,				otherwise
}

example:
joint probability density function f(x,y) and the marginal density function f_X(x)
f(x,y) = {
	3x, 0 < x < 1, 0 < y < x
	0,  otherwise
}
f_X(x) = {
	3x^2, 0 < x < 1
	0,    otherwise
}
find the expression of the conditional probability density function f_Y|X(y|x) for 0 < x < 1, 0 < y < x
f_Y|X(y|x) = f(x,y)/f_X(x)
= (3x)/3(x^2)
= 1/x
the conditional probability density function
f_Y|X(y|x) = {
	1/x, 0 < x < 1, 0 < y < x
	0,   otherwise
}

example:
The conditional probability density function f_X|Y(x|y) and the marginal density function f_Y(y)
f_X|Y(x|y) = {
	2x/y^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}
f_Y(y) = {
	5y^4, 0 < y < 1
	0,    otherwise
}
find the expression of the joint probability density function f(x,y) for 0 < x < y, 0 < y < 1
f(x,y) = f_X|Y(x|y) * f_Y(y)
= 2x/y^2 * 5y^4
= 10xy^2
joint probability density function
f(x,y) = {
	10xy^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}

Finding a conditional PDF
X and Y be continuous random variables with the joint probability density function
f(x,y) = {
	x + y, 0 <= x <= 1, 0 <= y <= 1
	0,     otherwise
}
find the expression of the conditional probability density function f_X|Y(x|y) for 0 <= x <= 1, 0 <= y <= 1
using the formula for the marginal density function for Y
f_Y(y) = ∫^1_0 (x + y) dx
= [x^2/2 + xy]|_0-1
= 1/2 + y - 0
= (2y + 1)/2
f_Y(y) = {
	(2y + 1)/2, 0 <= y <= 1
	0,          otherwise
}
The conditional probability density function of X given Y = y for two continuous random variables X and Y
f_X|Y(x|y) = f(x,y)/f_Y(y)
f(x,y) is the joint probability density function for X and Y
f_Y(y) is the marginal density function for Y
0 <= x <= 1, 0 <= y <= 1
f_X|Y(x|y) = f(x,y)/f_Y(y)
= x + y/((2y + 1)/2)
= 2(x + y)/2y + 1
for other values of x and y the function f(x,y) equals 0 as a result f_X|Y(x|y) will be equal to 0
The conditional probability density function
f_X|Y(x|y) = {
	2(x + y)/2y + 1, 0 <= x <= 1, 0 <= y <= 1
	0,               otherwise
}

example:
joint probability density function
f(x,y) = {
	2y(2x + 3y)/5, 1 <= x <= 2, 0 <= y <= 1
	0,             otherwise
}
find the expression of the conditional probability density function f_X|Y(x|y) for 1 <= x <= 2, 0 <= y <= 1
f_Y(y) = ∫^1_2 2y(2x + 3y)/5 dx
= ∫^1_2 (4xy/5 + 6y^2/5) dx
= [2x^2y/5 + 6xy^2]|_1-2
= (8y/5 + 12y^2/5) - (2y/5 - 6y^2/5)
= 6y(y + 1)/5
full expression for f_Y(y)
f_Y(y) = {
	6y(y + 1)/5, 0 <= y <= 1
	0,           otherwise
}
f_X|Y(x|y) = f(x,y)/f_Y(y)
f(x,y) is the joint probability density function for X and Y
F_Y(y) is the marginal density function for Y
1 <= x <= 2, 0 <= y <= 1
= (2y(2x + 3y)/5)/(6y(y + 1)/5)
= 2x + 3y/3(y + 1)
for other values of x and y the function f(x,y) equals 0 f_X|Y(x|y) will be equal to 0 as well
conditional probability density function
f_X|Y(x|y) = {
	2x + 3y/3(y + 1), 1 <= x <= 2, 0 <= y <= 1
	0,                otherwise
}

example:
joint probability function
f(x,y) = {
	1, 0 < x < 1, x - 1 < y < 1 - x
	0, otherwise
}
find the expression of the conditional probability density function f_Y|X(y|x) for 0 < x < 1, x - 1 < y < 1 - x
f_X(x) = ∫^(1-x)_(x-1) 1 dy
= [y]|_(x-1)-(1-x)
= ((1-x) - (x-1))
= 2 - 2x
full expression for f_X(x)
f_X(x) = {
	2 - 2x, 0 < x < 1
	0, 	    otherwise
}
f_Y|X(y|x) = f(x,y)/f_X(x)
f(x,y) is the joint probability density function for X and Y
f_X(x) is the marginal density function for X
0 < x < 1, x - 1 < y < 1 - x
f_Y|X(y|x) = f(x,y)/f_X(x)
= 1/2 - 2x
Conditional probability density function
f_Y|X(y|x) = {
	1/2 - 2x, 0 < x < 1, x - 1 < y < 1 - x
	0,        otherwise
}

Computing a conditional probability
X and Y are two continuous random variables with the joint probability density function
f(x,y) = {
	8x^2y, 0 < x <= 1, 0 <= y <= sqrt(x)
	0,     otherwise
}
find P(Y < 1/2|X = 1)
recall
P(Y ∈ A|X = x) = ∫_A f_Y|X(y|x) dy
where f_Y|X(y|x) is the conditional probability density function
marginal density function for X
= ∫^sqrt(x)_0 8x^2y dy
= 8x^2 ∫^sqrt(x)_0 y dy
= 8x^2 [y^2/2]|_0-sqrt(x)
= 4x^2(sqrt(2))^2 - 0
= 4x^3
full expression for f_X(x)
f_X(x) = {
	4x^3, 0 < x <= 1,
	0,    otherwise
}
conditional probability density function for two continuous random variables X and Y
f_Y|X(y|x) = f(x,y)/f_X(x)
like always f(x,y) joint probability density function
f_X(x) is the marginal density function for X
0 < x <= 1, 0 <= y <= sqrt(x)
= 8x^2y/4x^3
= 2y/x
the conditional probability density function
f_Y|X(y|x) = {
	2y/x, 0 < x <= 1, 0 <= y <= sqrt(x)
	0,    otherwise
}
computing the probability
P(Y < 1/2|X = 1) = ∫^1/2_0 f_Y|X(y|x = 1) dy
= ∫^1/2_0 2y/1 dy
= ∫^1/2_0 2y dy
= [y^2]|_0-1/2
= 1/4 - 0
= 1/4

example:
Conditional probability density function
f_X|Y(x|y) = {
	2x/y^2, 0 < x < y, 0 < y < 1
	0,      otherwise
}
find P(1/4 < X < 1/2|Y = 5/8)
= ∫^1/2_1/4 f_X|Y(x|y = 5/8) dx
= ∫^1/2_1/4 2x/(5/8)^2 dx
= 64/25 ∫^1/2_1/4 2x dx
= 64/25 [x^2]|_1/4-1/2
= 64/25(1/4 - 1/16)
= 64/25(3/16)
= 12/25

example:
X and Y be two continuous random variables with the following joint probability density function
f(x,y) = {
	24xy, 0 < x < 1, 0 < y < 1 - x
	0,    otherwise
}
find P(Y < X|X = 1/3)
P(Y ∈ A|X = x) = ∫_A f_Y|X(y|x) dy
where f_Y|X(y|x) is the conditional probability density function
marginal density function for X
= ∫^(1-x)_0 24xy dy
= 12x[y^2]|_0-(1-x)
= 12x(1 - x)^2
= 12x(x - 1)^2
f_X(x) = {
	12x(x - 1)^2, 0 < y < 1 - x,
	0,    		  otherwise
}
f(x,y)/f_X(x)
= 24xy/12x(x - 1)^2
= 2y/(x - 1)^2
conditional probability density function
f_Y|X(y|x) = {
	2y/(x - 1)^2, 0 < x < 1, 0 < y < 1 - x
	0,            otherwise
}
we compute probability
P(Y < X|X = 1/3) = P(Y < 1/3|X = 1/3)
= ∫^1/3_0 f_Y|X(y|x = 1/3) dy
= ∫^1/3_0 2y/(1/3 - 1)^2 dy
= ∫^1/3_0 2y/(1/3 - 3/3)^2 dy
= ∫^1/3_0 2y/(-2/3)^2 dy
= ∫^1/3_0 2y/(4/9) dy
= ∫^1/3_0 2y * 9/4 dy
= ∫^1/3_0 9y/2 dy
= 9/2 ∫^1/3_0 y dy
= 9/2 [y^2/2]|_0-1/3
= 9/2(1/18 - 0)
= 1/4

===================================================

The joint cumulative distribution function (joint CDF)
F(x,y) = P(X <= x,Y <= y)
Joint CDF gives the probability that pair (X,Y) will lie inside the inifinite rectangular region Δ that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axes
0 <= F(x,y) <= 1
the marginal CDF of X is given by F_X(x) = F(x, ∞)
the marginal CDF of Y is given by F_Y(y) = F(∞, y)
F(∞,∞) = 1
F(-∞,y) = F(x,-∞) = 0
if X and Y are independent random variables, F(x,y) = F_X(x) * F_Y(y)
if X and Y are continuous random variables with joint probability density function f(x,y) we have the following relationship between the joint PDF and joint CDF
F(x,y) = ∫^y_-∞ ∫^x_-∞ f(u,v) dudv
over the rectangle [a,b]x[c,d] then the joint CDF is
F(x,y) = ∫^d_c ∫^b_a f(u,v) dudv

Computing the value of the joint CDF at a point
X and Y be continuous random variables with the joint PDF
f(x,y) = {
	3/5(sqrt(x)y), (x,y) ∈ D
	0,              otherwise
}
if F(x,y) is the joint CDF of X and Y find F(2,2/5)
using the definition of the joint CDF
F(2, 5/2) = P(X < 2, Y < 5/2) = ∫^5/2_-∞ ∫^2_-∞ f(u,v) dudv
we're only interested in the region inside the Δ where f(x,y) is non-zero
D ∩ Δ = {(x,y) : 0 <= x <= 1, 2 <= y <= 5/2}
∫^5/2_2 ∫^1_0 3/5(sqrt(u)v) dudv
= ∫^5/2_2 ∫^1_0 3/5(u^(1/2)v) dudv
= ∫^5/2_2 [3/5 * 2/3(u^(3/2)v)]|_0-1 dv
= ∫^5/2_2 [2/5u^(3/2)v]|_0-1 dv
= ∫^5/2_2 2/5(v) dv
= [v^2/5]|_2-5/2
= 5/4 - 4/5
= 9/20

example:
joint probability density function
f(x,y) = {
	x + y, (x,y) ∈ D
	0,     otherwise
}
if F(x,y) is the joint CDF of X and Y find F(1/2,1/5)
F(1/2, 1/5) = P(X < 1/2, Y < 1/5) = ∫^1/5_-∞ ∫^1/2_-∞ f(u,v) dudv
D ∩ Δ = {(x,y) : 0 <= x <= 1/2, 0 <= y <= 1/5}
= ∫^1/5_0 ∫^1/2_0 (u + v) dudv
= ∫^1/5_0 [u^2/2 + uv]|_0-1/2 dv
= ∫^1/5_0 (1/8 + v/2) dv
= [v/8 + v^2/4]|_0-1/5
= [1/40 + 1/100] - 0
= 7/200

example:
joint probability density function
f(x,y) = {
	x + y, (x,y) ∈ D
	0,     otherwise
}
if F(x,y) is the joint CDF of X and Y find F(3/2, 1/2)
F(3/2, 1/2) = P(X < 3/2, Y < 1/2) = ∫^1/2_-∞ ∫^3/2_-∞ f(u,v) dudv
D ∩ Δ = {(x,y) : 0 <= x <= 1, 0 <= y <= 1/2}
= ∫^1/2_0 ∫^1_0 (u + v) dudv
= ∫^1/2_0 [u^2/2 + uv]|_0-1 dv
= ∫^1/2_0 (1/2 + v) dv
= [v/2 + v^2/2]|_0-1/2
= 1/4 + 1/8
= 2/8 + 1/8
= 3/8

Finding a probability geometrically using a CDF
the joint CDF of X and Y be F(x,y)
F(5,5) = 0.9, F(5,2) = 0.3, F(0,5) = 0.45, F(0,2) = 0.15
find P(0 <= X <= 5, 2 <= Y <= 5)
geometrically this region can be obtained by
1. take the infinite rectangle that has its top-right corner at (5,5)
2. subtract the infinite rectangles that have their top-right corners at (5,2) and (0,5)
3. add back the infinite rectangle that has its top-right corner at (0,2) this is because we subtracted this region in step 2
P(0 <= X <= 5, 2 <= Y <= 5)
top-right corner: at (5,5) - at (5,2) - at (0,5)
P(X <= 5,Y <= 5) - P(X <= 5,Y <= 2) - P(X <= 0,Y <= 5)
at (0,2)
+ P(X <= 0,Y <= 2)
= F(5,5) - F(5,2) - F(0,5) + F(0,2)
= 0.3

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.7, F(9,5) = 0.4, F(10,5) = 0.2
find P(X <= 10, 5 <= Y <= 9)
F(10,9) - F(10,5) (same x axis)
= 0.7 - 0.2
= 0.5

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.7, F(10,5) = 0.2, F(8,9) = 0.3, F(8,5) = 0.1
find P(8 <= X <= 10, 5 <= Y <= 9)
F(10,9) - F(10,5) - F(8,9) + F(8,5)
= 0.7 - 0.2 - 0.3 + 0.1
= 0.3

Finding Part of a Joint CDF from a joint PDF simple cases
joint probability density function
f(x,y) = {
	1/4(xy), 0 <= x <= 1, 0 <= y <= 4
	0,       otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 4
only interested in the region inside Δ where f(x,y) is nonzero
D ∩ Δ = D
our region covers the entire region
∫^y_-∞ ∫^x_-∞ f(u,v) dudv = ∫∫_D f(u,v) dudv = 1

example:
joint probability density function
f(x,y) = {
	1/3(x^2y^2), 0 <= x <= 1, 0 <= y <= 3
	0,           otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 3
D ∩ Δ = D (picked a point where x > 1 and y > 3)
so our entire region gets covered

example:
joint probability density function
f(x,y) = {
	1/3(x^2y^2), 0 <= x <= 1, 0 <= y <= 3
	0,           otherwise
}
find the expression for the corresponding joint CDF x > 1, y > 3
D ∩ Δ = D (picked a point where x < 0 and 1 < y < 3)
so our region lies below and to the left of our point (f(x,y))
D ∩ Δ = 0

finding part of a joint CDF from a joint PDF
joint probability density function
f(x,y) = {
	1/3(x + y), 0 <= x <= 2, 0 <= y <= 1
	0,          otherwise
}
find the expression corresponding joint CDF 0 <= x <= 2 and y > 1
∫^1_0 ∫^x_0 1/3(u + v) dudv
= 1/3 ∫^1_0 [u^2/2 + uv]|_0-x dv
= 1/3 ∫^1_0 [x^2/2 + xv] dv
= 1/3[x^2v/2 + xv^2/2]|_0-1
= 1/3(x^2/2 + x/2 - 0)
= 1/6(x^2 + x)

example:
joint probability density function
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
find the expression for the corresponding joint CDF when 0 <= x <= 1, 0 <= y <= 1
∫^y_0 ∫^x_0 4uv dudv
= ∫^y_0 [4vu^2/2]|_0-x dv
= ∫^y_0 4vx^2/2 dv
= [2v^2x^2/2]|_0-y
= 2y^2x^2/2
= y^2x^2

example:
joint probability density function
f(x,y) = {
	1/2sin(x + y), 0 <= x <= pi/2, 0 <= y <= pi/2
}
find the expression for the joint CDF x > pi/2 and 0 <= t <= pi/2
D ∩ Δ = {(u,v) : 0 <= u <= pi/2, 0 <= v <= y}
∫^y_0 ∫^pi/2_0 1/2sin(u + v) dudv
= 1/2 ∫^y_0 [-cos(u + v)]|_0-pi/2 dv
= 1/2 ∫^y_0 (cos(v) - cos(pi/2 + v)) dv
= 1/2 [sin(v) - sin(pi/2 + v)]|_0-y
= 1/2 (sin(y) - sin(pi/2 + y)) - 1/2(sin(0) - sin(pi/2 + 0))
= 1/2(sin(y) - sin(pi/2 + y) + 1)

=================================================

Recovering a marginal CDF
how to recover the marginal CDFs of X and Y from the joint CDF
the joint CDF of two continuous random variables X and Y is
F(x,y) = {
	(1 - e^-x)(1 - e^2y), x >= 0, y >= 0
	0,                    otherwise
}
lets calculate F_Y(y) the marginal CDF of Y using the property
F_Y(y) = F(∞,y) lim_x->∞ F(x,y)
therefore y >= 0
F_Y(y) = lim_x->∞ [(1 - e^-x)(1 - e^-2y)]
= (1 - e^-2y) lim_x->∞ (1 - 1/e^x)
= (1 - e^-2y) lim_x->∞ (1 - 0)
= 1 - e^-2y
the full expression for the marginal CDF
F_Y(y) = {
	1 - e^-2y, y >= 0
	0,         otherwise
}

Finding a marginal CDF using a joint CDF
The joint CDF of two continuous random variables X and Y for (x,y) memberof [0,1] x [0,2]
F(x,y) = xy/8(2x^2 + y)
find F_X(x) the marginal CDF of X for 0 <= x <= 1
the marginal CDF of X is given by
F_X(x) = F(x, ∞) = lim_y->∞ F(x,y)
for 0 <= x <= 1
F_X(x) = F(x, ∞)
= F(x,2)
= x(2)/8(2x^2 + (2))
= 4x/8(x^2 + 1)
= 1/2(x^3 + x)

example:
the joint CDF
F(x,y) {
	0, 			    x < 0 or y < 0
	(1 - e^-3x)y^2, x >= 0, 0 <= y <= 1
	F_X(x),         x >= 0, y > 1
}
find F_Y(y) the marginal CDF of Y for y >= 0
for y >= 0
y^2 lim_x->∞ (1 - e^-3x)
= y^2 lim_x->∞ (1 - 0)
= y^2

example:
the joint CDF for (x,y) [0,1] x [0,2]
F(x,y) = 1/20x^2y(4x + 3y)
find F_X(x) the marginal CDF of X for 0 <= x <= 1
1/20x^2(2)(4x + 3(2))
= 1/20x^2(2)(4x + 6)
= 1/20x^2(8x + 12)
= 1/5(2x^3 + 3x^2)

Finding part of a joint CDF from marginal CDFs for continuous random variables
the marginal CDFs
F_X(x) = {
	1 - 1/x, x >= 1
	0,       otherwise
}
F_Y(y) = {
	1 - 1/sqrt(y^3), y >= 1
	0,               otherwise
}
for x >= 1 and y >= 1 the joint CDF
= (1 - 1/x)(1 - 1/sqrt(y^3))
= 1 - 1/x - 1/sqrt(y^3) + 1/xsqrt(y^3)
full expression for the joint CDF
F(x,y) = {
	1 - 1/x - 1/sqrt(y^3) + 1/xsqrt(y^3), x >= 1, y >= 1
	0,								      otherwise
}

example:
The marginal CDFs
F_X(x) = {
	1 - e^-2x, x >= 0
	0,         otherwise
}
F_Y(y) = {
	1 - e^-3y, y >= 0
	0,         otherwise
}
find the expression for the joint CDF when x >= 0 and y >= 0
(1 - e^-2x)(1 - e^-3y)
= 1 - e^-2x - e^-3y + e^-2x-3y
F(x,y) = {
	1 - e^-2x - e^-3y + e^-2x-3y, x >= 0, y >= 0
	0,							  otherwise
}

example:
The marginal CDFs
F_X(x) = {
	1 - 1/x^2, x >= 1
	0,         otherwise
}
F_Y(y) = {
	1 - 1/y, y >= 1
	0,       otherwise
}
find the expression for the joint CDF when x >= 1 and y >= 1
(1 - 1/x^2)(1 - 1/y)
= 1 - 1/x^2 - 1/y + 1/x^2y
F(x,y) = {
	1 - 1/x^2 - 1/y + 1/x^2y, x >= 1, y >= 1
	0,                        otherwise
}

Recovering the joint PMF from the joint CDF
the joint CDF
F(x,y) = {
	0,             x < 0 or y < 0
	x^2(1 - e^-y), 0 <= x <= 1, y >= 0
	F_Y(y),        x > 1, y >= 0
}
find the joint PMF f(x,y) for 0 <= x <= 1, y >= 0
since the partial derivatives of F(x,y) are defined the joint probability density function f(x,y)
f(x,y) = ∂^2/∂x∂y F(x,y)
for 0 <= x <= 1 and y >= 0
= ∂/∂x(∂/∂y(x^2(1 - e^-y)))
= ∂/∂x(x^2∂/∂y(1 - e^-y))
= ∂/∂x(x^2e^-y)
= e^-y * ∂/∂x(x^2)
= e^-y * 2x
= 2xe^-y
f(x,y) = {
	2xe^-y, 0 <= x <= 1, y >= 0
	0,      otherwise
}

example:
joint CDF:
F(x,y) = {
	(1 - e^-x)(1 - e^-y), x >= 0, y >= 0
	0,					  otherwise
}
find the joint PMF f(x,y) for x >= 0, y >= 0
∂/∂x(∂/∂y(1 - e^-x)(1 - e^-y))
∂/∂x((1 - e^-x)(e^-y))
e^-x-y
f(x,y) = {
	e^-x-y, x >= 0, y >= 0
	0,      otherwise
}

example:
joint CDF:
F(x,y) = {
	0,	                  x < 1 or y < 1
	4(x^2 - 1)(y - 1)/9y, 1 <= x <= 2, 1 <= y <= 4
	F_X(x),				  1 <= x <= 2, y > 4
	F_Y(y),				  x > 2, 1 <= y <= 4
	1,					  x > 2, y > 4
}
where F_X and F_Y are the marginal CDFs of C and Y. Find the joint PMF f(x,y) for 1 <= x <= 2 and 1 <= y <= 4
∂/∂x(∂/∂y(4(x^2 - 1)(y - 1)/9y))
= ∂/∂x(4(x^2 - 1) * ∂/∂y(1/9 - 1/9y))
= ∂/∂x(4(x^2 - 1) * 1/9y^2)
= 4/9y^2 * ∂/∂x(x^2  -1)
= 8x/9y^2
f(x,y) = {
	8x/9y^2, 1 <= x <= 2, 1 <= y <= 4
	0,       otherwise
}

===================================================

For any random variables X and Y and constants a and b
E[aX + bY] = aE[X] + bE[Y]
The expected value of a sum of scaled random variables equals the sum of the scaled expected values
E[2X + 5Y] = 2E[X] + 5E[Y]
property also works with substraction
E[2X - 5Y] = 2E[X] - 5E[Y]
we can extend this idea to any number of random variables X1,X2...,Xn and real constants a1,a2,...an
E[a1X1 + a2X2 + ... + anXn] = a1E[X1] + a2E[X2] + ... + anE[Xn]
E[Σ^n_i=1(aiXi)] = Σ^n_i=1(aiE[Xi])

Finding the expected value of a sum of random variables
Compute E[5X - 2Y] if E[X] = 1 and E[Y] = 2
E[aX + bY] = aE[X] + bE[Y]
E[5X - 2Y] = 5E[X] - 2E[Y]
= 5(1) - 2(2)
= 1

example:
E[X] = 4, E[Y] = 5 find E[5X + 4Y]
= 5(4) + 4(5)
= 40

example:
find E[5X + 4Y]
x   | 0 | 1 |  2 |  3 | 4 |
f(x)|0.1|0.2|0.25|0.15|0.3|
---------------------------
y   |  2 | 4 |  6 |
f(y)|0.25|0.5|0.25|
-------------------
X = 0 * 0.1 + 1 * 0.2 + 2 * 0.25 + 3 * 0.15 + 4 * 0.3 = 2.35
y = 2 * 0.25 + 4 * 0.5 + 6 * 0.25 = 4
5(2.35) + 4(4)
= 27.75

Finding the expected value of a sum of random variables in context
Mathias has two dice one of them is fair and the other is not the rolls of the unfair die follow the PMF f(y) what is the expected sum of the outcomes of 2 throws of the fair die and 4 throws of the unfair one?
y   | 1 |  2 | 3 | 4 | 5 |  6 |
f(y)|1/4|1/16|1/4|1/8|1/4|1/16|
-------------------------------
Xi be the ith score from the fair die and Yj be the jth score from unfair die.
E[Σ^2_i=1(Xi) + Σ^4_j=1(Yj)]
= Σ^2_i=1(E[Xi]) + Σ^4_j=1(E[Yj])
= 2E[X] + 4E[Y]
E[Xi] = E[X] and E[Yj] = E[Y] for all 1 <= i <= 2, 1 <= j <= 4
The expected values of X and Y:
E[X] = 1*1/6 + 2*1/6 + 3*1/6 + 4*1/6 + 5*1/6 + 6*1/6 = 7/2
E[Y] = 1*1/4 + 2*1/16 + 3*1/4 + 4*1/8 + 5*1/4 + 6*1/16 = 13/4
E[2X + 4Y] = 2E[X] + 4E[Y] = 2*7/2 + 4*13/4 = 20

example:
Two machines produce same car the random variable X represents the number of rejects per day of first machine random variable Y are rejects per day on other machine, these are probability distributions
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
expected number of rejects per day
= 2

example:
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
5 type A (X) and 3 type B (Y) 
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
1.3(5) + 0.7(3)
= 8.6

Expected product of two independent random variables
E[X * Y] = E[X] * E[Y]
E[X] = 2 and E[Y] = 3
= 2 * 3 = 6
its important to check that the variables are independent if the random variables X1,X2,...Xn are mutually independent
E[X1*X2...Xn] = E[X1] * E[X2] * ... * E[Xn]
E[Π^n_i=1(Xi)] = Π^n_i=1(Xi)
(Π is the product operator known as capital Pi)

Finding the expected value of a product of random variables
The probability distributions of the independent random variables X and Y
x   | -4 | -2 | 2 | 4 |
f(x)| 1/6| 1/3|1/3|1/6|
-----------------------
y   | 1 | 2 | 3 |
f(y)|1/4|1/4|1/2|
-----------------
find E[XY]
E[X] = -4*1/6 - 2*1/3 + 2*1/3 + 4*1/6 = 0
E[Y] = 1*1/4 + 2*1/4 + 3*1/2 = 9/4
E[X * Y] = E[X] * E[Y] = 0 * 9/4 = 0

example:
E[X] = 4, E[Y] = 5, find E[XY]
E[X] * E[Y] = 20

example:
x   | 0 | 1 | 2 | 3 |
f(x)|0.1|0.6|0.2|0.1|
---------------------
y   | 0 | 1 | 2 |
f(y)|0.5|0.3|0.2|
-----------------
find E[XY]
E[X] = 0.6 + 2*0.2 + 3*0.1 = 1.3
E[Y] = 0.3 + 2*0.2 = 0.7
1.3 * 0.7 = 0.91

===================================================

Variance of sums of independent random variables
Var[X + Y] = Var[X] + Var[Y]
Var[X - Y] = Var[X] + Var[Y]
When computing the variance of a difference we add the variances

Calculating the variance of a sum or difference of independent random variables
For two independent random variables X and Y we have Var[X] = 3 and Var[Y] = 7 find Var[X - Y]
Var[X - Y] = 3 + 7 = 10

example:
Var[X] = 12 and Var[Y] = 5, find Var[X + Y]
= 17

example:
Var[X] = 12 and Var[Y] = 5, find Var[X - Y]
= 17

calculating the variance of a sum of scaled random variables
Var[X] = 10 and Var[Y] = 12, find Var[X - 2Y]
if X and Y are independent variables, and a and b are constants then
Var[aX + bY] = a^2Var[X] + b^2Var[Y]
substituting we get
Var[X - 2Y] =  (1)^2 * 10 + (-2)^2*12 = 58

example:
Var[X] = 12 and Var[Y] = 5, find Var[2X + 5Y]
= (2)^2*12 + (5)^2*5
= 173

example:
Var[X] = 11 and Var[Y] = 15, find Var[2X - 3Y]
= (2)^2*11 + (3)^2*15
= 179

calculate a combined variance given some raw moments
E[X] = 5, E[Y] = 4 and second raw moments E[X^2] = 30 and E[Y^2] = 18 find the variance of 12X - 11Y
Var[X] = E[X^2] - E[X]^2
= 30 - 5^2
= 5
Var[Y] = E[Y^2] - E[Y]^2
= 18 - 4^2
= 2
Var[12X - 11Y] = 12^2Var[X] + (-11)^2Var[Y]
= 144(5) + 121(2)
= 962

example:
E[X] = 2, E[Y] = 4, second raw moments E[X^2] = 5, E[Y^2] = 23 find the variance of 5X + 7Y
Var[X] = 5 - 2^2 = 1
Var[Y] = 23 - 4^2 = 23 - 16 = 7
5^2(1) + 7^2(7)
25 + 343 = 368

example:
PMF
x   | 1 | 2 | 3 |
f(x)|0.2|0.3|0.5|
-----------------
y   |  1 |  2 |
f(y)|0.25|0.75|
---------------
find Var[3X + 2Y]
E[X] = 0.2 + 2*0.3 + 3*0.5 = 2.3
E[Y] = 0.25 + 2*0.75 = 1.75
E[X^2] = 1*0.2 + 4*0.3 + 9*0.5 = 5.9
Var[X] = E[X^2] - E[X]^2
= 5.9 - 2.3^2
= 0.61
E[Y^2] = 1*0.25 + 4*0.75
= 3.25
Var[Y] = E[Y^2] - E[Y]^2
= 3.25 - 1.75^2
= 0.1875
Var[3X - 2Y] = 3^2Var[X] + 2^2Var[Y]
= 3^2(0.61) + 2^2(0.1875)
= 6.24

===================================================

To compute the expected value of a discrete random variable X given a joint mass function f(x,y) we must first find its marginal mass function f_X once the marginal mass function is known we can compute E[X] using
E[X] = Σ^n_i=1(f_X(xi))
joint probability mass function f(x,y)
f  |Y=1 | Y=2|
X=1|0.05|0.15|
X=2| 0.7| 0.1|
--------------
the marginal distribution for X corresponds to the row totals and the marginal distributino for Y corresponds to the column totals, we're only interested in expected value of X

f  |Y=1 | Y=2| f_X|
X=1|0.05|0.15|0.20|
X=2| 0.7| 0.1|0.80|
-------------------
x     |  1 |  2 |
f_X(x)|0.20|0.80|
-----------------
expected value of X:
E[X] = 1*0.2 + 2*0.8
= 0.2 + 1.6
E[X] = 1.8

example:
joint probability distribution, find E[Y]
f  |Y=0 | Y=1| Y=2|
X=1|0.05| 0.1|0.25|
X=2|0.25|0.15| 0.2|
-------------------
y     | 0 |  1 |  2 |
f_Y(y)|0.3|0.25|0.45|
---------------------
E[Y] = 0*0.3 + 1*0.25 + 2*0.45
= 1.15

example:
joint probability distribution, find E[5X - 2Y]
f  |Y=0 | Y=1| Y=2|
X=1|0.05| 0.1|0.25|
X=2|0.25|0.15| 0.2|
-------------------
y     | 0 |  1 |  2 |
f_Y(y)|0.3|0.25|0.45|
---------------------
x     | 0 | 1 |
f_X(x)|0.4|0.6|
---------------
E[Y] = 0*0.3 + 1*0.25 + 2*0.45
= 1.15
E[X] = 0.6
5(0.6) - 2(1.15)
= 0.7

Continuous random variable
to compute the expected value of a continuous random variable X given a joint density function f(x,y) we must first find its marginal mass function f_X once the marginal mass function is known we can then compute E[X] using
E[X] = ∫^∞_-∞ x * f_X(x) dx

finding the expected value of a continuous random variable from a joint distribution
joint PDF:
f(x,y) = {
	4/x^3y^3, x >= 1, y >= 1
	0,        otherwise
}
find the expected value of Y
The marginal density for Y when y >= 1
f_Y(y) ∫^∞_1 4/x^3y^3 dx
= 4/y^3 ∫^b_1 1/x^3 dx
= 4/y^3 [-1/2x^2]|_1-b
= 4/y^3 (1/2 - 1/2b^2)
= 4/y^3 (1/2 - 0)
= 2/y^3
full expression for f_Y(y)
f_Y(y) = {
	2/y^3, y >= 1
	0,     otherwise
}
expected value of Y
E[Y] = ∫^∞_-∞ y * f_Y(y) dy
= ∫^∞_1 y * 2/y^3 dy
= 2 ∫^b_1 1/y^2 dy
= 2 [-1/y]|_1-b
= 2(1 - 1/b)
= 2

example:
joint probability density function
f(x,y) = {
	2/pi(sin(y)), 0 <= x <= pi/2, 0 <= y <= pi/2
	0,	          otherwise
}
find the expected value of X
marginal density function for X
f_X(x) ∫^pi/2_0 2/pi(sin(y)) dy
2/pi ∫^pi/2_0 sin(y) dy
2/pi [-cos(y)]|_0-pi/2
2/pi (-cos(pi/2) - (-cos(0)))
2/pi (0 - (-1))
= 2/pi
f_X(x) = {
	2/pi, 0 <= x <= pi/2
	0,    otherwise
}
E[X] = ∫^pi/2_0 x * 2/pi dx
= 2/pi [x^2/2]|_0-pi/2
= 2/pi (pi^2/8)
= 2pi^2/8pi
= pi^2/4pi
= pi/4

example:
joint PDF
f(x,y) = {
	3/2(x^2y), 0 <= x <= 1, 0 <= y <= 2
	0,		   otherwise
}
E[X] = 3/4, find E[4X + 6Y]
f_y(y) ∫^1_0 3/2(x^2y) dx
= 3/2 ∫^1_0 (x^2y) dx
= 3/2y [x^3/3]|_0-1
= 3/2y(1/3)
= 3/6(y)
= 1/2(y)
E[Y] = ∫^2_0 y * 1/2(y) dy
= ∫^2_0 y^2/2 dy
= [y^3/6]|_0-2
= 8/6 
= 4/3
E[4X + 6Y] = 4(3/4) + 6(4/3)
3 + 8 = 11

===================================================

For discrete random variables X and Y the conditional expected value of X given Y = y is
E[X|Y = y] = Σ_x(xf_X|Y(x|y))
where f_X|Y(x|y) is the conditional probability mass function of X given Y
This definition is similar to that of E[X] however we now use the conditional PMF f_X|Y(x|y) instead of the marginal PMF f_X(x) in the summation
The conditional expectation E[X|Y = y] is (in general) a function of y. If the distribution of X varies with Y then the expected value of X for a given Y must vary with the value of Y
If y is replaced with a number then E[X|Y = y] returns a single value
If X and Y are independent then E[X|Y = y] = E[X] for all y. If the outcome of Y has no influence on the outcome of X then the distribution of X and its mean is also unaffected by Y
something the notation μ_X|y to denote E[X|Y = y]
same for E[Y|X = x]
= Σ_y(yf_Y|X(y|x))

Calculating a conditional expected value given conditional mass
y         | 1 | 2 |
f_Y|X(y|2)|2/3|1/3|
-------------------
find the E[Y|X = 2] given that the conditional probability mass function f_Y|X(y|2) is shown in the table above
E[Y|X = 2] = Σ_y(yf_Y|X(y|2))
= 1 * 2/3 + 2 * 1/3
= 4/3

example:
x         | 2 | 3 |
f_X|Y(x|1)|4/5|1/5|
-------------------
find E[X|Y = 1] given that the conditional probability mass function f_X|Y(x|1)
= 2 * 4/5 + 3 * 1/5
= 11/5

example:
f_Y|X(y|5) = {
	0.2, y = 1
	0.8, y = 3
	0,   otherwise
}
find E[Y|X = 5] given that the conditional probability mass function f_Y|X(y|5)
E[Y|X = 5] = 0.2 + 0.8 * 3
= 2.6

Calculating a conditional expectation using row totals
f  | Y=1| Y=2|
X=1| 1/9| 2/9|
X=2|8/27|5/27|
X=3|2/27|1/91|
--------------
find E[Y|X = 2]
in order to find f_Y|X(y|x) lets first find the marginal distribution for X corresponding row totals
f  | Y=1| Y=2|f_X(x)|
X=1| 1/9| 2/9|   1/3|
X=2|8/27|5/27| 13/27|
X=3|2/27|1/91|  5/27|
---------------------
The conditional probability mass function of Y given that X = x is
f_Y|X(y|x) = f(x,y)/f_X(x)
dividing each joint probability by the corresponding row total we obtain the conditional probability mass function of Y given that X = x
f_Y|X(y|x)| Y=1| Y=2|f_X(x)|
X=1       | 1/3| 2/3|     1|
X=2       |8/13|5/13|     1|
X=3       | 2/5| 3/5|     1|
----------------------------
E[Y|X = 2] = 1 * 8/13 + 2 * 5/13
= 18/13

example:
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/8|1/4|1/8|   1/2|
X=1|1/4|1/8|1/8|   1/2|
-----------------------
find E[Y|X = 0] given the joint probability mass function
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/4|1/2|1/4|     1|
X=1|1/2|1/4|1/4|     1|
-----------------------
= 0*1/4 + 1*1/2 + 2*1/4
= 1

example:
f  |Y=0| Y=2| Y=4|
X=0|1/9|3/18|1/18|
X=2|1/3| 1/9| 2/9|
------------------
find E[Y|X = 2] givem the joint probability mass function
f  |Y=0| Y=2| Y=4|f_X(x)|
X=0|1/9|3/18|1/18|   1/3|
X=2|1/3| 1/9| 2/9|   2/3|
-------------------------
f  |Y=0| Y=2| Y=4|f_X(x)|
X=0|1/9|3/18|1/18|   1/3|
X=2|1/3| 1/9| 2/9|   2/3|
-------------------------
3/6 3/18 6/18 (multiplying row 2 by the recipical of total)
(0)9/18 + (2)3/18 + (4)6/18 = 6/18 + 24/18
= 30/18 = 15/9
= 5/3

Calculating a conditional expectation using column totals
f  |Y=1|Y=2|
X=2|1/9|2/9|
X=4|1/3|1/9|
X=6|1/9|1/9|
------------
find E[X|Y = 1] given the joint probability mass function
In order to find f_X|Y(x|y) lets first find the marginal distribution for Y corresponding to the column totals
f     |Y=1|Y=2|
X=2   |1/9|2/9|
X=4   |1/3|1/9|
X=6   |1/9|1/9|
f_Y(y)|5/9|4/9|
---------------
f_X|Y(x|y) = f(x,y)/f_Y(y)
same processes as the rows we divide each joint probability by the corresponding column total we obtain the conditional probability mass function of X given Y = y
f_X|Y(x|y)|Y=1|Y=2|
X=2       |1/5|1/2|
X=4       |3/5|1/4|
X=6       |1/5|1/4|
          |  1|  1|
-------------------
= 2*1/5 + 4*3/5 + 6*1/5
= 20/5
= 4

example:
f  |Y=0|Y=1|Y=2|
X=0|1/8|1/4|1/8|
X=1|1/4|1/8|1/8|
----------------
find E[X|Y = 1] given the joint probability mass function
f     |Y=0|Y=1|Y=2|
X=0   |1/8|1/4|1/8|
X=1   |1/4|1/8|1/8|
f_Y(y)|3/8|3/8|1/4|
-------------------
= (0)16/24 + (1)8/24 (focusing on second column only)
= 1/3

example:
f  |Y=4|Y=5|Y=6|
X=0|1/9|1/3|1/9|
X=3|2/9|1/9|1/9|
----------------
find E[X|Y = 6] given the joint probability mass function
f     |Y=4|Y=5|Y=6|
X=0   |1/9|1/3|1/9|
X=3   |2/9|1/9|1/9|
f_Y(y)|3/9|4/9|2/9|
-------------------
9/18 9/18 = (1)1/2 + (3)1/2
= 4/2 = 2

===================================================

X and Y are discrete random variables, where the conditional expected value of X given Y = y is given by μ_X|y
E[X|Y = y] = μ_X|y
The conditional variance of X given Y = y
Var[X|Y] = E[(X - μ_X|y)^2|Y = y]
This is similar to the definition of Var[X] in that it represents the average (squared) distance of the random variable X from the mean. The difference here is that the expected values are now dependent on the outcome of Y
similarly to the case of Var[X] this often isn't the easiest formula to use in practice, it can be shown that the definition of conditional variance is equivalent to
Var[X|Y = y] = E[X^2|Y = y] - μ^2_X|y
= [Σ_x(x^2f_X|Y(x|y))] - μ^2_X|y
where f_X|Y(x|y) is the conditional PMF of X given Y these formulas are analogous to those for Var[X]
similar Y given X = x
Var[Y|X] = E[(Y - μ^2_Y|x)^2|X = x]
= E[Y^2|X = x] - μ^2_Y|x
= [Σ_y(y^2f_Y|X(y|x))] - μ^2_Y|x

Calculating a conditional variance given a conditional mass function
     y    | 1 | 2 | 4 |
f_Y|X(y|3)|1/3|1/6|1/2|
-----------------------
X and Y are discrete random variables. Find Var[Y|X = 3] the corresponding conditional expected value is μ_Y|3 = 8/3
Var[Y|X = 3] = [Σ_y(y^2f_Y|X(y|3))] - μ^2_Y|3
= [(1)^2*1/3 + (2)^2*1/6 + (4)^2*1/2] - (8/3)^2
= 1/3 + 2/3 + 8 - 64/9
= 17/9

example:
f_X|Y(x|3) = {
	1/3, x = 12
	2/3, x = 18
	0,   otherwise
}
find the conditional variance Var[X|Y = 3] given that the conditional probability mass function f_X|Y(x|3) is shown above the corresponding conditional expected value is μ_X|3 = 16
= [(12)^2*1/3 + (18)^2*2/3]
= [48 + 216] - 256
= 8

example:
     y    | 0 | 1 | 2 |
f_Y|X(y|1)|0.1|0.4|0.5|
-----------------------
find Var[Y|X = 1] given the conditional probability mass function f_Y|X(y|1) conditional expected value is μ_Y|1 = 1.4
= [0.4 + 2^2*0.5] - 1.96
= 0.44

calculating a conditional variance using row totals
f  | Y=1| Y=2| Y=3|
X=1|1/10|1/10| 1/5|
X=2|3/10| 1/5|1/10|
-------------------
find Var[Y|X = 1] given the joint probability mass function f(x,y) corresponding conditional expected value is μ_Y|1 = 9/4
f  | Y=1| Y=2| Y=3|f_X(x)|
X=1|1/10|1/10| 1/5|   2/5|
X=2|3/10| 1/5|1/10|   3/5|
--------------------------
the conditional probability mass function of Y given that X = x
f_Y|X(y|x) = f(x,y)/f_X(x)
dividing each joint probability by the row total we obtain the conditional probability mass function of Y given that X = x
f_Y|X(y|x)|Y=1| Y=2|Y=3|f_X(x)|
X=1       |1/4| 1/4|1/2|     1|
X=2       |1/2| 1/3|1/6|     1|
-------------------------------
Var[Y|X = 1] = [Σ_y(y^2f_Y|X(y|1))] - μ^2_Y|1
= [1^2*1/4 + 2^2*1/4 + 3^2*1/2] - (9/4)^2
= 1/4 + 1 + 9/2 - 81/16
= 11/16

example:
f  |Y=0|Y=1|Y=2|
X=0|1/8|1/4|1/8|
X=1|1/4|1/8|1/8|
----------------
find Var[Y|X = 0] given the joint probability mass function f(x,y), conditional expected value is μ_Y|0 = 1
f  |Y=0|Y=1|Y=2|f_X(x)|
X=0|1/8|1/4|1/8|   1/2|
X=1|1/4|1/8|1/8|   1/2|
-----------------------
2/8 + 4/8 + 2/8 (first row)
4/8 + 8/8 = 12/8 - 1
= 6/4 = 3/2 - 2/2
= 1/2

example:
find Var[Y|X = 1] given the joint probability mass function f(x,y), corresponding conditional expected value is μ_Y|1 = 6/5
f  |Y=1|Y=2|f_X(x)|
X=1|1/2|1/8|   5/8|
X=2|1/8|1/4|   3/8|
-------------------
32/40 + 8/40
= 16/20 + 4/20
= 8/10 + 2/10
= 4/5 + 1/5
= 4/5 + 4*1/5 = 8/5 - (6/5)^2
= 40/25 - 36/25 = 4/25

Calculate a conditional variance using column totals
f  | Y=2| Y=4|
X=1| 1/2| 1/4|
X=2|1/16|3/16|
--------------
find Var[X|Y = 2] given the joint probability mass function f(x,y), conditional expected value μ_X|2 = 10/9
find f_X|Y(x|y) lets first find the marginal distribution of Y corresponding to the column totals
f     | Y=2| Y=4|
X=1   | 1/2| 1/4|
X=2   |1/16|3/16|
f_Y(y)|9/16|7/16|
-----------------
the conditional probability mass function of X given that Y = y
f_X|Y(x|y) = f(x,y)/f_Y(y)
dividing each joint probability by the corresponding column total we get the conditional probability mass function of X given Y = y
f_X|Y(x,y)|Y=2|Y=4|
X=1       |8/9|4/7|
X=2       |1/9|3/7|
          |  1|  1|
-------------------
= [1^2*8/9 + 2^2*1/9] - (10/9)^2
= 8/9 + 4/9 - 100/81
= 8/81

example:
f  |Y=0|Y=1|
X=0|1/2|1/8|
X=1|1/4|1/8|
------------
find Var[X|Y = 0] given the joint PMF the expected value is μ_X|0 = 1/3
f     |Y=0|Y=1|
X=0   |1/2|1/8|
X=1   |1/4|1/8|
f_Y(y)|3/4|2/8|
---------------
= [0^2*4/6 + 1^2*4/12] - (1/3)^2
1/3 - 1/9
3/9 - 1/9 = 2/9

example:
f     | Y=0|  Y=1|
X=0   |1/10|  1/5|
X=3   | 1/5|  1/4|
X=9   |3/20| 1/10|
f_Y(y)|9/20|11/20|
------------------
find Var[X|Y = 0] given the joint PMF, expected value is μ_X|0 = 13/3
[0 + 3^2*20/45 + 9^2*60/180] - 169/9
180/45 + 4860/180 - 169/9
4 + 27 - 169/9
279/9 - 169/9
110/9
