Combining Random Variables:

Distributions of Two Discrete Random Variables
Distributions of Two Continuous Random Variables
Expectation for Joint Distributions
Covariance of Random Variables
Normally Distributed Random Variables

NOTE:
sigma notation is similar to integral notation as I am not using anything like MathML example: Σ^3_j=1(j + 1) in this case 3 is above the sigma and ^, and _ separates what would be on the top and bottom respectively and next to sigma is the iterator (j + 1)

NOTE:
When integrating we sometimes have to change an upper limit from a ∞ to a variable like b because I want to evalutate the integral step by step, to show the process before substituing back into the limit (a placeholder before arriving at the final form)

NOTE:
taking a CDF for Y we need to integrate with repect to x and keep y fixed, and vise versa.

--------------------------------------------

Sigma notation
Σ^n_i=1(ai)
refers to the sum of all terms in the sequence ai where index i ranges from 1 to n
Σ^n_i=1 ai = a1 + a2 + ... + an
sigma notion can also represents sums whose terms depend on two indices that is double sums
Σ^m_i=1Σ^n_j=1(aij)
where i ranges from 1 to m and j ranges from 1 to n, "sum of a sum"
first we evaluate the inner sum by fixing the index of the outer sum (i) and incrementing only the inner index (j)
next we evaluate out sum by incrementing the outer sum index
Σ^2_i=1Σ^4_j=1(i + j)
= Σ^2_i=1[Σ^4_j=1(i + j)]
= Σ^2_i=1[(i + 1) + (i + 2) + (i + 3) + (i + 4)]
= Σ^2_i=1(4i + 10)]
= 4(1) + 10 + 4(2) + 10
= 4 + 10 + 8 + 10
= 32

example:
Σ^3_j=1Σ^2_i=1(i + j)
= Σ^3_j=1[Σ^2_i=1(i + j)]
= Σ^3_j=1[(1 + j) + (2 + j)]
= Σ^3_j=1(3 + 2j)
outer
Σ^3_j=1(3 + 2j) = (3 + 2 * 1) + (3 + 2 * 2) + (3 + 2 * 3)
= 5 + 7 + 9
= 21

example:
Σ^3_i=1Σ^3_j=2(i^2 + j)
= Σ^3_i=1[Σ^3_j=2(i^2 + j)]
= Σ^3_i=1[(i^2 + 2) + (i^2 + 3)]
= Σ^3_i=1(2i^2 + 5)
outer
Σ^3_i=1(2i^2 + 5) = (2(1)^2 + 5) + (2(2)^2 + 5) + (2(3)^2 + 5)
= 7 + 13 + 23
= 43

example:
Σ^2_j=1Σ^3_i=1(ij)
= Σ^2_j=1[Σ^3_i=1(ij)]
= Σ^2_j=1[(1j) + (2j) + (3j)]
= Σ^2_j=1(6j)
outer
Σ^2_j=1(6j) = (6(1)) + (6(2))
= 6 + 12
= 18

The sum and constant multiple rules
The constant multiple rule
we can factor out a constant multiple from a double summation
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
The sum rule
we can distribute a double summation over a sum of terms
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
The double sum of units
for single summations we have
Σ^n_j=1(1) = 1 + 1 + ... + 1 = n (n times)
double summations
Σ^m_i=1Σ^n_j=1(1) = mn

Applying the sum and constant multiple rules
find the value of the double sum
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
given that
Σ^5_i=1Σ^8_j=1(aij) = 8
Σ^5_i=1Σ^8_j=1(bij) = 15
the sum and contant multiple rules for double summations
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
applying the sum rule to the given double summation
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= Σ^5_i=1Σ^8_j=1(1) + Σ^5_i=1Σ^8_j=1(2aij) - Σ^5_i=1Σ^8_j=1(bij)
= (Σ^5_i=1Σ^8_j=1(1)) + 2Σ^5_i=1Σ^8_j=1(aij) - Σ^5_i=1Σ^8_j=1(bij)
from the info given
= (Σ^5_i=1Σ^8_j=1(1)) + 2(8) - 15
= (Σ^5_i=1Σ^8_j=1(1)) + 1
using
Σ^m_i=1Σ^n_j=1(1) = mn
(Σ^5_i=1Σ^8_j=1(1)) + 1 = 5 * 8 + 1
= 41
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= 41

example:
find the value of the double sum
Σ^50_i=1Σ^100_j=1(4(aij + bij))
given
Σ^50_i=1Σ^100_j=1(aij) = 40
Σ^50_i=1Σ^100_j=1(bij) = 60
= 4Σ^50_i=1Σ^100_j=1(aij) + 4Σ^50_i=1Σ^100_j=1(bij)
= 4 * 40 + 4 * 60
= 160 + 240
= 400

example:
Σ^5_i=1Σ^10_j=1(1 - aij - bij)
Σ^5_i=1Σ^10_j=1(aij) = 10
Σ^5_i=1Σ^10_j=1(bij) = 20
= (Σ^5_i=1Σ^10_j=1(1)) - 30
= 5 * 10 - 30
= 50 - 30
= 20

The product and swap rules
the product rule
Σ^m_i=1Σ^n_j=1(aibj) = Σ^m_i=1(ai)Σ^n_j=1(bj)
the swap rule
Σ^m_i=1Σ^n_j=1(aij) = Σ^n_j=1Σ^m_i=1(aij)

Appling the product rule and swap rules
find the value of the double sum
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
given
Σ^15_i=1(ai) = 40
Σ^18_j=1(bj) = -8
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
= Σ^15_i=1(ai - 2)Σ^18_j=1(bj + 1)
= (Σ^15_i=1(ai) - Σ^15_i=1(2)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (Σ^15_i=1(ai) - 2Σ^15_i=1(1)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (40 - 2 * 15) * (-8 + 18)
= 10 * 10
= 100

example:
Σ^10_i=1Σ^20_j=1(ai - 1)(bj + 1)
given
Σ^10_i=1(ai) = 12
Σ^20_j=1(bj) = 5
= (Σ^10_i=1(ai - 1)Σ^20_j=1(bj + 1))
= (Σ^10_i=1(ai) - Σ^10_i=1(1)) * (Σ^20_j=1(bj) + Σ^20_j=1(1))
= (12 - 10) * (5 + 20)
= (2) * (25)
= 50

example:
Σ^30_i=1Σ^30_j=1(2ai(bj - 2))
given
Σ^30_i=1(ai) = 42
Σ^30_j=1(bj) = 70
= 2Σ^30_i=1(ai)Σ^30_j=1(bj - 2)
= (2Σ^30_i=1(ai)) * (Σ^30_j=1(bj) - 2Σ^30_j=1(1))
= (2 * 42) * (70 - 2 * 30)
= (84) * (70 - 60)
= (84) * (10)
= 840

===================================================

We often want to know if there is a relationship between two random variables. For this reason we wish to formulate the idea of a joint probability distribution
X represents the number of heads obtained when a fair coin is flipped. The support of X denoted S_X consists of all possible values of X is given by S_X = {0,1}
Y represents the result of rolling a fair tetrahedral die then the support of Y denoted S_Y is S_Y = {1,2,3,4}
The joint support of X and Y denoted S consists of all possible pairs (x, y) such that x is a possible outcome for X and y is a possible outcome for Y
S = S_X x S_Y = {
	(0,1), (0,2), (0,3), (0,4)
	(1,1), (1,2), (1,3), (1,4)
}
S_X x S_Y is the cartesian product of S_X and S_Y
flipping of the coin and rolling of the die are independent then for any x ∈ X and any y ∈ Y
P(X = x and Y = y) = 1/2 * 1/4 = 1/8
the joint probability mass function of X and Y denoted f(x,y) can be represented by the table
f(x,y)|y=1|y=2|y=3|y=4|
x = 0 |1/8|1/8|1/8|1/8|
x = 1 |1/8|1/8|1/8|1/8|
similar to the case of single random variables
f(x,y) = P(X = x and Y = y)
P(X = x,Y = y) as short hand

Bivariate and multivariate distributions
X and Y are discrete random variables with supports S_X and S_Y
f(x,y) = P(X = x,Y = y)
to be a valid joint probability mass function with joint support S = S_X x S_Y it must satisfy following conditions
0 <= f(x,y) <= 1 for all (x,y) ∈ S
Σ_(x,y)∈S f(x,y) = 1
P((X,Y) ∈ A) = Σ_(x,y)∈A f(x,y) where A is a subset of S
some intuition
the consition 0 <= f(x,y) <= 1 for all (x,y) ∈ S states that any possible outcome for (X,Y) must have a probability between 0 and 1
The Σ_(x,y)∈S f(x,y) = 1 states that the sum of all probabilities over all possible values of X and Y must add up to 1
The third condition states that we compute the probability of an event A by adding up the probabilites associated with A
The joint probability table
f(x,y) | Y = y1 | Y = y2 | ... | Y = yn |
x = x1 |f(x1,y1)|f(x1,y2)| ... |f(x1,yn)|
x = x2 |f(x2,y1)|f(x2,y2)| ... |f(x2,yn)|
 ...   |   ...  |   ...  | ... |   ...  |
x = xk |f(xk,y1)|f(xk,y2)| ... |f(xk,yn)|
two random variables the joint distribution is sometimes called a bivariate distribution, any number of random variables is a multivariate distribution

finding the joint distribution of two discrete random variables
   |Y=0|  Y=1 |
X=0|a/2|   a  |
X=1| a |5/2(a)|
the joint probability mass function f(x,y) for the discrete random variables X and Y is given above
The support of X is S_X = {0,1} the support of Y is S_Y = {0,1} and therefore the joint support S is
S = S_X x S_Y = {(0,0),(0,1),(1,0),(1,1)}
f(0,0) + f(0,1) + f(0,1) + f(1,1) = 1
a/2 + a + a + 5/2(a) = 1
5a = 1
a = 1/5

example:
   |Y=1|Y=2|
X=0| 2a| 3a|
X=1| 5a| 2a|
S = S_X x S_Y = {(0,1),(0,2),(1,1),(1,2)}
2a + 3a + 5a + 2a = 1
12a = 1
a = 1/12

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
which of the following conditions are true?
1. 0 <= f(x,y) <= 1 for all (x,y) in S
2. Σ_(x,y)∈S f(x,y) = 1
3. f(x,y) is a valid joint probability mass function
all 3 are valid, and since 1 and 2 we've seen makes 3 true.

Calculating a joint probability from a table
   |Y=1 |Y=3 |
X=2|0.05|0.25|
X=4|0.2 |0.05|
X=6|0.3 |0.15|
compute P((X,Y) ∈ {(2,3),(6,1)})
= P((X,Y) = (2,3)) + P((X,Y) = (6,1))
= f(2,3) + f(6,1)
= 0.25 + 0.3
= 0.55

example:
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
calculate P(X=0,Y=1)
= f(0,1) = 1/2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P((X,Y) ∈ {(1,4),(1,6),(2,6)})
= 0.25 + 0.25 + 0.2
= 0.7

Calculating a joint probability containing inequalities from a table
   |Y=0 |Y=2 |Y=4
X=0|0.15|0.1 |0.1
X=1|0.05|0.1 |0.05
X=2|0.05|0.25|0.15
Compute P(X*Y >= 4)
= P((X,Y) ∈ {(1,4),(2,2),(2,4)})
= 0.05 + 0.25 + 0.15
= 0.45

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X = 2 and Y <= 4)
= P((X,Y) ∈ {(2,2),(2,4)})
= 0.05 + 0.15
= 0.2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X < 2 or Y = 6)
= P((X,Y) ∈ {(1,2),(1,4),(1,6),(2,6)})
= 0.1 + 0.25 + 0.25 + 0.2
= 0.8

===================================================

The joint PMF f(x,y) tells us all possible events (X,Y) and the probabilities associated with each event, however we wish to determine the PMF for X only how can this be deduced from the joint PMF?
consider the joint PMF
   |Y=1 |Y=2 |
X=1|0.05|0.15|
X=2|0.7 |0.1 |
The support of X is S_X = {1,2} lets now deduce the probabilites associated with each value of X in S_X
According to the law of total probability we can deduce P(X = 1) by summing the values in the first row of the table
P(X = 1) = P(X = 1,Y = 1) + P(X = 1,Y = 2)
= 0.05 + 0.15
= 0.2
summing the values in second row in table
P(X = 2) = P(X = 2,Y = 1) + P(X = 2,Y = 2)
= 0.7 + 0.1
= 0.8
PMF for X
x     | 1 | 2 |
f_X(x)|0.2|0.8|
f_X(x) was deduced from the joint probability mass function, we call f_X(x) the marginal mass function for X

Deducing the marginal mass function for Y
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
the marginal mass function in the right margin of the table
P(X = 1) = f_X(1) = 0.2
P(X = 2) = f_X(2) = 0.8
similary we can compute the marginal mass function of Y denoted f_Y(y) by summing the columns adding these totals to our table
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
f_Y|0.75|0.25| 1 |
P(Y = 1) = f_Y(1) = 0.75
P(Y = 2) = f_Y(2) = 0.25
marginal mass function f_Y(y) of Y
x     | 1  |  2 |
f_Y(y)|0.75|0.25|

A formal definition of the margin mass function
The maginal mass function of X denoted f_X(x)
f_X(x) = P(X = x) = Σ_y∈SY f(x,y)
The marginal mass function of Y denoted f_Y(y)
f_Y(y) = P(Y = x) = Σ_x∈SX f(x,y)
To compute P(X = x) for some particular x we sum all possible values of Y in the row corresponding to X = x
To compute P(Y = y) for some particular y we sum all possible values of X in the column corresponding to Y = y
Marginal mass functions are sometimes called marginal probability mass functions, marginal PMFs, or marginal distributions

Finding a marginal probability
   |Y=0 |Y=1 |
X=0|1/6 |5/12|
X=1|1/4 |1/6 |
P(X = x) = f_X(x) = Σ_y f(x,y)
P(Y = y) = f_Y(y) = Σ_x f(x,y)
The marginal distribution for X corresponds to the row totals and the marginal distribution for Y corresponds to the column totals.
   |Y=0 |Y=1 |f_X |
X=0|1/6 |5/12|7/12|
X=1|1/4 |1/6 |5/12|
f_Y|5/12|7/12|  1 |
P(X = 0) = f_X(0)
= f(0,0) + f(0,1)
= 1/6 + 5/12
= 7/12

example:
find P(X = 0)
   |Y=1 |Y=2 |
X=0|0.4 |0.2 |
X=1|0.15|0.25|
= 0.4 + 0.2
= 0.6

example:
find f_Y(0)
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
= 1/4 + 1/8
= 3/8

(rest of examples are simple table lookups and summations no new knowledge to note)

===================================================

Determining whether two random variables are independent
   |Y=0 |Y=1 |
X=0|0.24|0.06|
X=1|0.56|0.14|
marginal mass functions
x     |0  |1  |
f_X(x)|0.3|0.7|
---------------
y     |0  |1  |
f_Y(y)|0.8|0.2|
which of the following statements are true?
1. P(X = 0,Y = 0) = P(X = 0)*P(Y = 0)
2. P(X = x,Y = y) = P(X = x)*P(Y = y) for all possible x and y
3. X and Y are independent
Two discrete random variables X and Y with joint probability mass function f(x,y) are independent iff
f(x,y) = f_X(x)*f_Y(y)
Statements 1,2, and 3 is true. 1 is true because
P(X = 0,Y = 0) = f(0,0) = 0.24
P(X = 0) * P(Y = 0) = f_X(0) * f_Y(0) = 0.3 * 0.8 = 0.24
2 is true because
f(1,0) = 0.56 = 0.7 * 0.8 = f_X(1) * f_Y(0)
f(0,1) = 0.06 = 0.3 * 0.2 = f_X(0) * f_Y(1)
f(1,1) = 0.14 = 0.7 * 0.2 = f_X(1) * f_Y(1)
P(X = x,Y = y) = P(X = x) * P(Y = y) for all possible x and y
statement 3 is true because of 1 and 2 is true X and Y must be independent

example:
   |Y=0 |Y=1|
X=0|0.05|0.2|
X=1|0.15|0.6|
computing the marginal probability mass functions for X and Y
   |Y=0 |Y=1| f_X|
X=0|0.05|0.2|0.25|
X=1|0.15|0.6|0.75|
f_Y| 0.2|0.8|  1 |
check if these statements are true
this first statement is true:
P(X = 1,Y = 1) = P(X = 1) * P(Y = 1)
0.75 * 0.8 = 0.6
this is also true
P(X = x,Y = y) = P(X = x) * P(Y = y)
making X and Y independent

example:
   |Y=0 | Y=1|
X=0|0.05|0.15|
X=1|0.25|0.55|
computing the marginal probability mass functions for X and Y
   |Y=0 | Y=1| f_X|
X=0|0.05|0.15| 0.2|
X=1|0.25|0.55| 0.8|
f_Y| 0.3| 0.7|  1 |
check if these statements are true
this first statement is true:
P(X = 0,Y = 1) = P(X = 0) * P(Y = 1)
0.2 * 0.7 = 0.14 != 0.15
so this would also be false
P(X = x,Y = y) = P(X = x) * P(Y = y)
and X and Y are not independent

example:
   |Y=2| Y=4|
X=1|0.2| 0.2|
X=3|0.1|0.25|
X=5|0.2|0.05|
computing the marginal probability mass functions for X and Y
   |Y=2| Y=4| f_X|
X=1|0.2| 0.2| 0.4|
X=3|0.1|0.25|0.35|
X=5|0.2|0.05|0.25|
f_Y|0.5| 0.5|  1 |
check if these statements are true
this is true:
P(X = 1,Y = 2) = P(X = 1) * P(Y = 2)
0.4 * 0.5 = 0.2
this is false (only have to check one instance)!
P(X = x,Y = y) = P(X = x) * P(Y = y)
0.5 * 0.35 = 0.175 != 0.1
making X and Y dependent

example:
   |Y=1| Y=3| f_X|
X=2| * |0.27|  * |
X=4| * |0.63|  ? |
f_Y|0.1| 0.9|  1 |
find P(X = 4)
normally to cross reference we * by totals, we don't
have all totals but we can find it by the info given to us
by dividing total with the cross reference cell
0.63/0.9 = 0.7
P(X = 4) = 0.7

===================================================

Conditional distributions for discrete random variables
the multiplication law for conditional probability for two events A and B
P(A|B) = P(A ∩ B)/P(B)
suppose X and Y are discrete random variables the conditional probability X = x given that Y = y follows immediately from the multiplication law
P(X = x|Y = y) = P(X = x,Y - y)/P(Y = y) = f(x,y)/f_Y(y)
F(x,y) is the joint PMF of X and Y, f_Y(y) is the marginal mass function
To avoid getting a zero in the denominator the formula assumes that P(Y = y) = f_Y(y) != 0
In the case where P(Y = y) = 0 the corresponding probability for X does not exist it makes no sense to ask about the event of X = x occuring given that Y = y has occurred if the event Y = y is impossible
similarly the conditional probability Y = y given X = x
P(Y = y|X = x) = P(X = x,Y = y)/P(X = x) = f(x,y)/f_X(x)
where f_X(x) is the marginal mass function of X this formula assumes that P(X = x) != 0

Computing a conditional probability
   | Y=0| Y=1|
X=0| 0.5|0.05|
X=1|0.15| 0.3|
compute P(Y = 0|X = 1) given that the discrete random variables X and Y have the joint probability function f(x,y)
P(Y = 0|X = 1) = P(X = 1,Y = 0)/P(X = 1) = f(1,0)/f_X(1)
f_X(1) = f(1,0) + f(1,1)
= 0.15 + 0.3
= 0.45
substituting
f_X(1) = 0.45 and f(1,0) = 0.15 in our formula
P(Y = 0|X = 1) = f(1,0)/f_X(1)
= 0.15/0.45
= 1/3

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(X = 2|Y = 3)
P_Y(3) = 0.25 + 0.05 + 0.15
= 0.45
f(2,3)/P_Y(3)
= 0.25/0.45
~= 0.56

example:
   | Y=1| Y=3|
X=2|0.05|0.25|
X=4| 0.2|0.05|
X=6| 0.3|0.15|
Compute P(Y = 3|X = 2)
P_X(2) = 0.05 + 0.25 = 0.3
f(2,3)/f_X(2)
= 0.25/0.3
~= 0.83

Computing a conditional probability over an interval
   | Y=0| Y=1|Y=2 |Y=3|
X=2| 0.2|   0|0.05|0.1|
X=4| 0.1|0.05|0.15|  0|
X=6| 0.2|0.05|   0|0.1|
Compute P(0 < Y < 3|X = 0)
= P((0 < Y < 3), X = 0)/P(X = 0)
= P(X = 0, (0 < Y < 3))/P(X = 0)
= P(X = 0,Y = 0) + P(X = 0,Y = 2)/P(X = 0)
= (f(0,1) + f(0,2))/f_X(0)
f_X(0) = 0.2 + 0 + 0.05 + 0.1
= 0.35
= 0 + 0.05/0.35
= 0.05/0.35
~= 0.14

example:
    | Y=0| Y=2| Y=4|
X=0 |0.15| 0.1| 0.1|
X=4 |0.05| 0.1|0.05|
X=8 | 0  | 0.2|0.15|
X=12|0.05|0.05|   0|
Compute P(4 <= X <= 8|Y = 0)
= (f(4,0) + (8,0))/f_Y(0)
= 0.05 + 0
f_Y(0) = 0.15 + 0.05 + 0 + 0.05 = 0.25
(0.05 + 0)/0.25
~= 0.2

example:
    | Y=0| Y=2| Y=4|Y=6|
X=1 |1/18|5/18| 1/9|2/9|
X=2 | 1/9|1/18|   0|1/6|
Compute P(2 <= Y <= 6|X = 2)
f_X(2) = 1/9 + 1/18 + 0 + 1/6
= 2/18 + 1/18 + 3/18
= 6/18
= 1/3
f(2,2) + f(2,4) + f(2,6)
= 1/18 + 0 + 1/6
= 4/18
= 2/9
= ((2/9)/(1/3)) 
= 2/9 * 3/1
= 6/9 = 2/3

Conditional probability mass functions
We can now define two more probability mass functions
The conditional probability mass function of X given that Y = y denoted f_X|Y(x|y)
f_X|Y(x|y)
= P(X = x,Y = y) = P(X = x,Y = y)/P(Y = y) 
= f(x,y)/f_Y(y)
f(x,y) is the joint probability mass function of X and Y and f_Y(y) is the marginal mass function of Y
The conditional probability mass function of Y given that X = x denoted f_Y|X(y|x)
f_Y|X(y|x)
= P(Y = y,X = x) = P(X = x,Y = y)/P(X = x)
= f(x,y)/f_X(x)
f(x,y) is a joint probability mass function of X and Y and f_X(x) in the marginal mass function of X
The conditional distribution of X given Y does not equal the conditional distribution of Y given X
f_X|Y(x|y) != f_Y|X(y|x)
if X and Y and independent
f_X|Y(x|y) = f_X(x), f_Y|X(y|x) = f_Y(y)
X and Y are independent, knowing the value of one random variable does not affect the probability distribution of the other.

Computing the value of a conditional probability mass function at a point
    | Y=0| Y=1| Y=2|
X=1 | 1/6| 1/9| 1/6|
X=2 |1/18|1/18| 1/6|
X=3 |1/9 | 1/9|1/18|
Compute f_X|Y(2|2)
= f(x,y)/f_Y(2)
f_Y(2) = 1/6 + 1/6 + 1/18
= 3/18 + 3/18 + 1/18
= 7/18
f(2,2) = 1/6
= (1/6)/(7/18)
= 18/42
= 3/7

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Comput f_X|Y(1|0)
f_Y(0) = 0.3 + 0.1 = 0.4
f(1,0) = 0.1
0.1/0.4 = 0.25

example:
   | Y=0| Y=1| Y=2|
X=0| 0.3| 0.2| 0.1|
X=1| 0.1| 0.1| 0.2|
Compute f_Y|X(1|0)
f_X(0) = 0.3 + 0.2 + 0.1 = 0.6
f(0,1) = 0.2
0.2/0.6
~= 0.33

Computing conditional probability mass function
   | Y=1| Y=3| Y=5|
X=0| 0.1|   0| 0.2|
X=2| 0.3| 0.1|   0|
X=4| 0.1|   0| 0.2|
find f_X|Y(x|1)
= f(x,1)/f_Y(1)
x ∈ {0,2,4}
P(X = 0|Y = 1) = f(0,1)/f_Y(1) = 0.1/0.5 = 1/5
P(X = 2|Y = 1) = f(2,1)/f_Y(1) = 0.3/0.5 = 3/5
P(X = 4|Y = 1) = f(4,1)/f_Y(1) = 0.1/0.5 = 1/5
x         |0  |2  |4  |
f_X|Y(x|1)|1/5|3/5|1/5|
-----------------------

(rest of the example were faster to answer and calculate in my head then creating tables here for notes)

===================================================

The joint cumulative distribution function (or joint CDF)
F(x,y) = P(X <= x,Y <= y) = Σ_xi<=xΣ_yi<=y(f(xi,yi))
the sets S_X and S_Y are called the supports of X and Y
joint CDF is analogous to the cumulative distribution function of a single random variable
joint PMF:
   | Y=0| Y=1| Y=2|
X=0| 0.1| 0.2| 0.1|
X=1| 0.3| 0.2| 0.1|
the supports of X and Y are S_X = {0,1} and S_Y = {0,1,2}
lets compute F(1,1)
F(1,1) = P(X <= 1, Y <= 1)
f(0,0) + f(0,1) + f(1,0) + f(1,1)
= 0.1 + 0.2 + 0.3 + 0.2
= 0.8

Evaluating a joint CDF
   |Y=-2|Y=-1| Y=0|Y=2|
X=1| 0.1| 0.3|0.05|0.1|
X=3| 0.1|0.05| 0.1|0.2|
find F(2,1)
F(2,1) = P(X <= 2, Y <= 1)
f(1,-2) + f(1,-1) + f(1,0)
= 0.1 + 0.3 + 0.05
= 0.45

example:
   | Y=2| Y=4| Y=6|
X=1| 0.1|0.25|0.25|
X=2|0.05|0.15| 0.2|
find F(2,4)
F(2,4) = P(X <= 2, Y <= 4)
f(1,2) + f(1,4) + f(2,2) + f(2,4)
0.1 + 0.25 + 0.05 + 0.15
= 0.55

example:
   | Y=0| Y=1|
X=2| 0.2|0.25|
X=4|0.16|0.15|
X=6|0.04|0.15|
find F(5,1/2)
F(5,1/2) = P(X <= 5, Y <= 1/2)
f(2,0) + f(4,0)
0.2 + 0.16
= 0.36

Some properties of the joint CDF
0 <= F(x,y) <= 1
the marginal CDF of X is given by F_X(x) = F(x, ∞)
the marginal CDF of Y is given by F_Y(y) = F(∞, y)
F(∞, ∞) = 1
F(-∞, y) = F(x, -∞) = 0
if X and Y are independent random variables then F(x,y) = F_X(x)*F_Y(y)

Interpreting the joint CDF Geometrically
(think of f(x,y) as our top right corner)
S_X = {1,2,3,4}, S_Y = {2,3,4}
find an expression for the corresponding joint CDF when x > 4, y < 2
We can think of each point where f(x,y) != 0 as a point mass in the plane where the probability mass at each point equals the probability associated with that point
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis
all our points are on S = S_X x S_Y = {1,2,3,4} x {2,3,4} our region lies x > 4 and y < 2 there are no points that lie in our region
f(x,y) = 0

example:
S_X = {-2,0,2}, S_Y = {0,1,2,3}
when x < -3, y > 3
f(x,y) = 0
the region is outside any of our supports

example:
when x > 4, y > 3
S_X = {1,2,3,4}, S_Y = {0,1,2,3}
f(x,y) = 1
region includes all points

Finding the cumulative probabilities given values from the joint CDF
F(5,4) = 0.9, F(5,2) = 0.2, F(3,4) = 0.5, F(3,2) = 0.1
S_X = {1,3,5,7}, S_Y = {1,2,3,4,5}
Find P(3 < X <= 5, 2 < Y <= 4)
For a given joint PMF f(x,y) we can think of each point where f(x,y) != 0 as a point mass in the plane, where the probability mass at each point equals the probability associated with that point.
The joint CDF F(x,y) = P(X <= x,Y <= y) is the total probability mass that lies in the infinite rectangular region that has its top-right corner at (x,y) and whose sides are parallel to the coordinate axis.
S = S_X x S_Y = {1,3,5,7} x {1,2,3,4,5}
if we want to find a sub-region within our support say at (5,4) 
subtract the infinite rectangles that have their top-right at (5,2) and (3,4)
we need to add back the infinite rectangle that has its top-right corner at (3,2) this is because we subtracted this region twice in step 2
Find P(3 < X <= 5, 2 < Y <= 4)
top-right corner at (5,4):
= P(X <= 5, Y <= 4)
top-right corner at (5,2) - top-right corner at (3,4)
- P(X <= 5, Y <= 2) - P(X <= 3, Y <= 4)
top-right corner at (3,2)
+ P(X <= 3, Y <= 2)
= F(5,4) - F(5,2) - F(3,4) + F(3,2)
= 0.9 - 0.2 - 0.5 + 0.1
= 0.3

example:
joint CDF of X and Y be F(x,y)
F(9,10) = 0.65, F(6,10) = 0.31, F(3,10) = 0.12
supports are
S_X = {3,6,9,12}, S_Y = {2,4,6,8,10,12}
Find P(3 < X <= 9,Y <= 10)
top-right corner at (9,10) - top-right corner at (3,10):
= P(X <= 9, Y <= 10) - P(X <= 3, Y <= 10)
= F(9,10) - F(3,10)
= 0.65 - 0.12
= 0.53

example:
joint CDF of X and Y be F(x,y)
F(10,9) = 0.72, F(10,6) = 0.3, F(8,9) = 0.15, F(8,6) = 0.08
supports of X and Y
S_X {2,4,6,8,10,12}, S_Y = {3,6,9,12}
find P(8 < X <= 10, 6 < Y <= 9)
top-right corner at (10,9)
P(X <= 10, Y <= 9)
top-right corner at (10,6) - top-right corner at (8,9)
- P(X <= 10, Y <= 6) - P(X <= 8, Y <= 9)
top-right corner at (8,6)
+ P(X <= 8, Y <= 6)
= F(10,9) - F(10,6) - F(8,9) + F(8,6)
= 0.72 - 0.3 - 0.15 + 0.08
= 0.35

Further finding cumulative probabilities given values from the joint CDF
the joint CDF of X and Y be F(x,y)
F(8,3) = 0.9, F(6,3) = 0.7, F(4,3) = 0.4
S_X = {2,4,6,8}, S_Y = {1,3,5}
P(X > 6, Y <= 4)
top-right corner at (8,3) - top-right corner at (6,3)
P(X <= 8, Y <= 3) - P(X <= 6, Y <= 3)
= F(8,3) - F(6,3)
= 0.9 - 0.7
= 0.2

example:
joint CDF
F(6,5) = 3/5, F(6,3) = 2/5, F(6,2) = 1/10
S_X = {2,6,10}, S_Y = {0,1,2,3,4,5}
find P(X < 7, Y > 3)
top-right corner at (6,5) - top-right corner at (6,3)
P(X <= 6,Y <= 5) - P(X <= 6,Y <= 3)
F(6,5) - F(6,3)
= 3/5 - 2/5
= 1/5

example:
joint CDF
F(7,1) = 9/16, F(7,-2) = 1/4, F(3,1) = 1/8, F(3,-2) = 1/16
S_X = {1,3,5,7,9}, S_Y = {-2,-1,0,1,2}
P(5 <= X <= 8,-1 <= Y <= 1)
9/16 - 4/16 - 2/16 + 1/16
4/16 = 1/4

===================================================

Joint distributions for continuous random variables
A function f(x,y) is a joint probability density function (or joint PDF) for the two continuous random variables X and Y if the following is satisfied
f(x,y) >= 0 for all (x,y) ∈ R^2
states f must be nonnegative
∫∫_R^2 f(x,y)dxdy = 1
the sum (integral) of f over possible X and Y values must add up to 1
P((X,Y) ∈ A) = ∫∫_A f(x,y)dxdy where A is a subset of R^2 
compute the probability of an event A by summing (integrating) f over all regions associated with A
analog of those for the joint PMF of two discrete random variables
they are two dimensional analog for the PDF f(x) of a (single) continuous random variable X

Verifying that a function is a valid joint PDF
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified
to check the second condition we need to integrate
∫∫_R^2 f(x,y) dxdy = ∫∫_D f(x,y) dxdy
= ∫^1_0 ∫^1_0 3/2(x^2 + y^2) dydx
= 3/2 ∫^1_0 ∫^1_0 x^2 + y^2 dydx
= 3/2 ∫^1_0 [x^2y + y^3/3]|_0-1 dx
= 3/2 ∫^1_0 (x^2 + 1/3) dx
= 3/2[x^3/3 + x/3]|_0-1
= 3/2(1/3 + 1/3)
= 1
therefore second condition is satisfied
f(x,y) is a valid joint probability density function

Joint PDFs for continuous random variables
f(x,y) = {
	csin(x + y), 0 < x < pi/2, 0 < y < pi/2
	0,           otherwise
}
verify that f(x,y) is a valid joint PDF
c > 0 we have f(x,y) >= 0 for all (x,y) ∈ R^2 first condition satified 
∫^pi/2_0[∫^pi/2_0 csin(x + y) dy]dx
= c ∫^pi/2_0[∫^pi/2_0 sin(x + y) dy]dx
= c ∫^pi/2_0 [-cos(x + y)]|_0-pi/2 dx
= c ∫^pi/2_0 (-cos(x + pi/2) + cos(x)) dx
= c ∫^pi/2_0 cos(x) - cos(x + pi/2) dx
integrate with respect to x
= c[sin(x) - sin(x + pi/2)]|_0-pi/2
= c[(sin(pi/2) - sin(pi)) - (sin(0) - sin(pi/2))]
= c[(1 - 0) - (0 - 1)]
= c[1 - (-1)]
= 2c
hence
2c = 1  =>  c = 1/2

example:
joint PDF:
f(x,y) = {
	c, 0 < x < 2, 1 < y < 2
	0, otherwise
}
c is a positive real number, find the value of c
∫^2_0 ∫^2_1 c dydx
= ∫^2_0 ∫^2_1 cy dydx
= c ∫^2_0 [y]|_1-2 dx
= c ∫^2_0 1 dx
= c[x]|_0-2
= c(2 - 0)
= 2c
2c = 1
c = 1/2

Joint PDFs for continuous random variables (non-rectangular domains)
joint PDF:
f(x,y) = {
	c/(1+x+y)^3, 0 <= x < y
	0, 			 otherwise
}
where c is a positive real number, find the value of c
c > 0, f(x,y) for all (x,y) ∈ R^2 the first condition is satisfied
∫^∞_0 [∫^∞_x c/(1 + x + y)^3 dy] dx
= c ∫^∞_0 [∫^∞_x (1 + x + y)^-3 dy] dx
= c ∫^∞_0 [-1/2(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 [(1 + x + y)^-2]|_x-∞ dx
= -c/2 ∫^∞_0 0 - (1 + 2x)^-2 dx
= -c/2 ∫^∞_0 -(1 + 2x)^-2 dx
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
integrate with respect to x
= c/2 ∫^∞_0 (1 + 2x)^-2 dx
= c/2[-1/2(1 + 2x)^-1]|_0-∞
= -c/4[1/1 + 2x]|0-∞
= -c/4[0 - 1]
= c/4
c/4 = 1  => c = 4

example:
joint PDF
f(x,y) = {
	cxy, 0 <= x <= y < 1
	0,   otherwise
}
find value of c
∫^1_0 ∫^1_x cxy dydx
= ∫^1_0 [cxy^2/2]|_x-1 dx
= ∫^1_0 (cx/2 - cx^3/2) dx
integrate with respect to x
= [cx^2/4 - cx^4/8]|_0-1
= c/4 - c/8
= c/8
c/8 = 1  =>  c = 8

example:
joint PDF
f(x,y) = {
	cx^2y, x^2 <= y <= 1
	0,     otherwise
}
find value of c
∫^1_-1 ∫^1_x^2 cx^2y dydx
= ∫^1_-1 [cx^2 * y^2/2]|_x^2-1 dx
= ∫^1_-1 cx^2((1 - x^4)/2)]|_x^2-1 dx
respect to x
= [c(x^3/6 - x^7/14)]|_-1-1
= 4c/21
4c/21 = 1  => c = 21/4

Finding probabilities using joint PDFs
f(x,y) is a joint probability density function for two continuous random variables X and Y
P((X,Y) ∈ A) = ∫∫_A f(x,y) dxdy
if A is the event defined as
A = {(X,Y) | a <= X <= b, c <= Y <= d}
the corresponding probability can be computed as
P(a <= X <= b, c <= Y <= d) = ∫^d_c ∫^b_a f(x,y) dxdy
= ∫^b_a ∫^d_c dydx
analogously to the case of a single continuous random variable the probability that (X,Y) takes on any particular value (x,y) is zero
P(X = x,Y = y) = 0

Computing a probability using a joint PDF
f(x,y) = {
	1/6, 0 <= x <= 3, 0 <= y <= 2
	0,   otherwise
}
find P(X > 1, Y > 1)
= 1/6 ∫∫_A dxdy
= 1/6Area(A)
= 1/6 * (3 - 1) * (2 - 1)
= 1/6 * 2 * 1
= 1/3

example:
PDF:
f(x,y) = {
	1, 0 <= x <= 1, 0 <= y <= 1
	0, otherwise
}
find P(X > Y)
= 0.5

example:
PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
find P(X > 1/2, Y > 1/2)
∫^1_1/2 ∫^1_1/2 4xy dxdy
∫^1_1/2 [∫^1_1/2 4xy dx] dy
∫^1_1/2 [2x^2y]|_1/2-1 dy
∫^1_1/2 3y/2 dy
= [3y^2/4]|_1/2-1
= 9/16

===================================================

Marginal distributions for continuous random variables
Suppose that X and Y are continuous random variables with joint probability density function f(x,y)
the marginal density function of X
f_X(x) = ∫^∞_-∞ f(x,y) dy
the marginal density function of Y
f_Y(y) = ∫^∞_-∞ f(x,y) dx
marginal density function can be known as marginal probability density function or marginal PDF
To convert from discrete to continuous we turn sums into integrals
when we compute f_X in the discrete case, we sum over all possible values of Y in the continuous case we integrate with repect to y
vise-versa for f_Y

example:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,              otherwise
}
lets find the expression for the MDF f_X(x) of X
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 x^2 + y^2 dy
= 3/2[x^2y + y^3/3]|_0-1
= 3/2((x^2*1 1^3/3) - (0))
= 3/2[x^2 + 1/3]
= 1/2(3x^2 + 1)
f_X(x) = {
	1/2(3x^2 + 1), 0 <= x <= 1
	0,             otherwise
}

finding a marginal distribution from a joint distribution
joint PDF
f(x,y) = {
	15/7(x^2y^4), 1 <= x <= 2, 0 <= y <= 1
	0,            otherwise
}
find expression for the marginal PDF f_Y(y)
the marginal density function of Y for 0 <= y <= 1
f_Y(y) ∫^2_1 15/7(x^2y^4) dx
15/7(y^4)[x^3/3]|_1-2
= 5/7(y^4)[x^3]|_1-2
= 5/7(y^4)(2^3 - 1^3)
= 5y^4
full expression
f_Y(y) = {
	5y^4, 0 <= y <= 1
	0,	  otherwise
}

example:
X and Y be two continuous random variables
joint PDF:
f(x,y) = {
	3/2(y^2), 0 <= x <= 2, 0 <= y <= 1,
	0,        otherwise
}
find the expression for the marginal PDF f_X(x)
∫^1_0 3/2(y^2) dy
= 3/2[y^3/3]|_0-1
= 1/2[y^3]|_0-1
= 1/2(1^3 - 0)
= 1/2
f_X(x) = {
	1/2, 0 <= x <= 2,
	0,   otherwise
}

example:
joint PDF:
f(x,y) = {
	8/3(x^3y), 0 <= x <= 1, 1 <= y <= 2
	0,   	   otherwise
}
find the marginal PDF f_Y(y)
∫^1_0 8/3(x^3y) dx
= 8/3y[x^4/4]|_0-1
= 2/3(y)[x^4]
= 2/3(y)(1^4 - 0)
= 2/3y

Finding a marginal probability from a joint PDF
f(x,y) = {
	e^-(4x+y/4), x >= 0, y >= 0
	0,           otherwise
}
find P(-4 <= Y <= 4)
the marginal density function of Y for y >= 0
f_Y(y) ∫^∞_0 e^-(4x+y/4) dx
= [-1/4e^-(4x+y/4)]|_0-∞
= -1/4(0 - e^-(0+y/4))
= 1/4e^(-y/4)
full expression for f_y(y)
f_y(y) = {
	1/4e^(-y/4), y >= 0
	0,         otherwise
}
the required probability is
P(-4 <= Y <= 4) = ∫^4_-4 f_Y(y) dy
= ∫^4_0 1/4e^(-y/4) dy
= [-e^(-y/4)]|_0-4
= (-e^(-4/4) + e^0)
= 1 - 1/e

example:
joint PDF:
f(x,y) = {
	3/2(x^2 + y^2), 0 <= x <= 1, 0 <= y <= 1
	0,				otherwise
}
P(0.2 < X < 1)
∫^1_0 3/2(x^2 + y^2) dy
= 3/2 ∫^1_0 (x^2 + y^2) dy
= [3/2(x^2y) + 3/6(y^3)]|_0-1
= [3/2(x^2y) + 1/2(y^3)]|_0-1
= [3/2(x^2y) + y^3/2]|_0-1
= (3/2(x^2) + 1/2) - 0
= 3/2(x^2) + 1/2
full expression for f_X(x)
f_X(x) = {
	3/2(x^2) + 1/2, 0 <= x <= 1
	0,              otherwise
}
∫^1_0.2 (3/2(x^2) + 1/2) dx
= [x^3/2 + 1/2x]|_0.2-1
= [1/2 + 1/2] - [(0.2)^3/2 + 0.2/2]
= 1 - 0.104
= 0.896

example:
joint PDF:
f(x,y) = {
	1/6(x + y), 0 <= x <= 1, 0 <= y <= 3
	0,			otherwise
}
P(Y <= 1)
∫^1_0 1/6(x + y) dx 
= [x^2/12 + yx/6]|_0-1
= (1/12 + y/6) - 0
full expression for f_X(x)
f_Y(y) = {
	1/12 + y/6, 0 <= y <= 3
	0,          otherwise
}
required probability
P(Y <= 1) = ∫^1_-∞ f_Y(y) dy
∫^1_0 (1/12 + y/6) dy
= [y/12 + y^2/12]|_0-1
= (1/12 + 1/12) - 0
= 1/6

Finding a marginal distribution from a joint distribution non-rectangular domains
joint PDF:
f(x,y) = 5ysqrt(x), y^2 <= x <= 1, y >= 0
find the expression for the marginal PDF f_Y(y)
0 <= y <= 1
f_Y(y) = ∫^1_y^2 5ysqrt(x) dx
= 5y ∫^1_y^2 sqrt(x) dx
= 5y[2/3(x^3/2)]|_y^2-1
= 10y/3[x^(3/2)]|_y^2-1
= 10y/3[1^(3/2) - (y^2)^(3/2)]
= 10y/3[1 - (y^3)]
= 10(y - y^4)/3
full expression for f_Y(y)
f_Y(y) = {
	10(y - y^4)/3, 0 <= y <= 1
	0, 			   otherwise
}

example:
joint PDF:
f(x,y) = 1/2xy 0 <= x <= y <= 2
find the expression for the marginal PDF f_X(x)
∫^2_x 1/2xy dy
= [xy^2/4]|_x-2
= (x - x^3/4)
f_X(x) = {
	x - x^3/4, 0 <= x <= 2
	0,         otherwise
}

example:
joint PDF:
f(x,y) = 1/(pi)(x^2), y^2 + 1 <= x
find the expression for the marginal PDF f_Y(y)
∫^∞_(y^2 + 1) 1/(pi)(x^2) dx
1/pi ∫^∞_(y^2 + 1) x^-2 dx
[-1/(pi)(x)]|_(y^2 + 1)-∞
= 0 - (-1/(pi)(y^2 + 1))
f_Y(y) = 1/pi(y^2 + 1)

Marginal cumulative distribution functions
joint probability density function
f(x,y) = {
	3/8(x + y)^2, -1 <= x <= 1, -1 <= y <= 1
	0,            otherwise
}
find the marginal cumulative distribution function (CDF) F_Y(y) of Y
the marginal function of Y for -1 <= y <= 1
f_Y(y) = ∫^1_-1 3/8(x + y)^2 dx
= 3/8[1/3(x + y)^3]|_-1-1
= 1/8[(x + y)^3]|_-1-1
= 1/8[(1 + y)^3 - (-1 + y)^3]
= 1/8[(1 + y)^3 - (y - 1)^3]
= 1/8[(y^3 + 3y^2 + 3y + 1) - (y^3 - 3y^2 + 3y - 1)]
(cancel out terms by distributing -)
= 1/8[2(3y^2 + 1)]
= 1/4(3y^2 + 1)
the marginal CDF of Y for -1 <= y <= 1
F_Y(y) = ∫^y_-∞ fx(u) du
= ∫^y_-1 1/4(3u^2 + 1) du
= 1/4 ∫^y_-1 (3u^2 + 1) du
= 1/4[u^3 + u]|_-1-y
= 1/4[y^3 + y - (-1)^3 - (-1)]
= 1/4[y^3 + y + 2]
= (y^3 + y + 2)/4
full expression for the marginal CDF of Y is
F_Y(y) = {
	0, 		          y < -1
	(y^3 + y + 2)/4, -1 <= y <= 1
	1,                y > 1
}

example:
joint PDF:
f(x,y) = {
	4xy, 0 <= x <= 1, 0 <= y <= 1
	0,   otherwise
}
the marginal CDF F_X(x) of X
F_X(x) = {
	0, 	  x < 0
	g(x), 0 <= x <= 1
	1, 	  x > 1
}
what is the function g(x)
∫^1_0 4xy dy
= 4x[y^2/2]|_0-1
= 2x[y^2]|_0-1
= 2x(1^2 - 0)
= 2x
the marginal CDF of X for 0 <= x <= 1
∫^1_0 2u du
= [u^2]|_0-1
= x^2 - 0
= x^2
full expression F_X(x)
F_X(x) = {
	0,   x < 0
	x^2, 0 <= x <= 1
	1,   x > 1
}
g(x) = x^2

example:
joint PDF:
f(x,y) = {
	2y/(x + 1)^2 x >= 0, 0 <= y <= 1
	0,			 otherwise
}
the marginal CDF F_Y(y) of Y
F_Y(y) = {
	0,    y < 0
	g(y), 0 <= y <= 1
	1,    y > 1 
}
what is the function g(y)
∫^∞_0 2y/(x + 1)^2 dx
= 2y ∫^b_0 [(x + 1)^-1] dx
= 2y[-1/(x + 1)]|_0-b
= 2y((-1/b + 1) + (1/0 + 1))
= 2y(1 - 1/(b + 1))
(when b approaches infinity 1/(b + 1) approaches 0 simplifies)
= 2y
∫^y_0 2u du
= [u^2]|_0-y
= y^2 - 0
= y^2
full expression F_Y(y)
F_Y(y) = {
	0,   y < 0
	y^2, 0 <= y <= 1
	1,   y > 1
}
g(y) = y^2
