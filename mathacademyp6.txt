Combining Random Variables:

Distributions of Two Discrete Random Variables
Distributions of Two Continuous Random Variables
Expectation for Joint Distributions
Covariance of Random Variables
Normally Distributed Random Variables

NOTE:
sigma notation is similar to integral notation as I am not using anything like MathML example: Σ^3_j=1(j + 1) in this case 3 is above the sigma and ^, and _ separates what would be on the top and bottom respectively and next to sigma is the iterator (j + 1)

--------------------------------------------

Sigma notation
Σ^n_i=1(ai)
refers to the sum of all terms in the sequence ai where index i ranges from 1 to n
Σ^n_i=1 ai = a1 + a2 + ... + an
sigma notion can also represents sums whose terms depend on two indices that is double sums
Σ^m_i=1Σ^n_j=1(aij)
where i ranges from 1 to m and j ranges from 1 to n, "sum of a sum"
first we evaluate the inner sum by fixing the index of the outer sum (i) and incrementing only the inner index (j)
next we evaluate out sum by incrementing the outer sum index
Σ^2_i=1Σ^4_j=1(i + j)
= Σ^2_i=1[Σ^4_j=1(i + j)]
= Σ^2_i=1[(i + 1) + (i + 2) + (i + 3) + (i + 4)]
= Σ^2_i=1(4i + 10)]
= 4(1) + 10 + 4(2) + 10
= 4 + 10 + 8 + 10
= 32

example:
Σ^3_j=1Σ^2_i=1(i + j)
= Σ^3_j=1[Σ^2_i=1(i + j)]
= Σ^3_j=1[(1 + j) + (2 + j)]
= Σ^3_j=1(3 + 2j)
outer
Σ^3_j=1(3 + 2j) = (3 + 2 * 1) + (3 + 2 * 2) + (3 + 2 * 3)
= 5 + 7 + 9
= 21

example:
Σ^3_i=1Σ^3_j=2(i^2 + j)
= Σ^3_i=1[Σ^3_j=2(i^2 + j)]
= Σ^3_i=1[(i^2 + 2) + (i^2 + 3)]
= Σ^3_i=1(2i^2 + 5)
outer
Σ^3_i=1(2i^2 + 5) = (2(1)^2 + 5) + (2(2)^2 + 5) + (2(3)^2 + 5)
= 7 + 13 + 23
= 43

example:
Σ^2_j=1Σ^3_i=1(ij)
= Σ^2_j=1[Σ^3_i=1(ij)]
= Σ^2_j=1[(1j) + (2j) + (3j)]
= Σ^2_j=1(6j)
outer
Σ^2_j=1(6j) = (6(1)) + (6(2))
= 6 + 12
= 18

The sum and constant multiple rules
The constant multiple rule
we can factor out a constant multiple from a double summation
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
The sum rule
we can distribute a double summation over a sum of terms
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
The double sum of units
for single summations we have
Σ^n_j=1(1) = 1 + 1 + ... + 1 = n (n times)
double summations
Σ^m_i=1Σ^n_j=1(1) = mn

Applying the sum and constant multiple rules
find the value of the double sum
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
given that
Σ^5_i=1Σ^8_j=1(aij) = 8
Σ^5_i=1Σ^8_j=1(bij) = 15
the sum and contant multiple rules for double summations
Σ^m_i=1Σ^n_j=1(aij + bij) = Σ^m_i=1Σ^n_j=1(aij) + Σ^m_i=1Σ^n_j=1(bij)
Σ^m_i=1Σ^n_j=1(kaij) = kΣ^m_i=1Σ^n_j=1(aij)
applying the sum rule to the given double summation
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= Σ^5_i=1Σ^8_j=1(1) + Σ^5_i=1Σ^8_j=1(2aij) - Σ^5_i=1Σ^8_j=1(bij)
= (Σ^5_i=1Σ^8_j=1(1)) + 2Σ^5_i=1Σ^8_j=1(aij) - Σ^5_i=1Σ^8_j=1(bij)
from the info given
= (Σ^5_i=1Σ^8_j=1(1)) + 2(8) - 15
= (Σ^5_i=1Σ^8_j=1(1)) + 1
using
Σ^m_i=1Σ^n_j=1(1) = mn
(Σ^5_i=1Σ^8_j=1(1)) + 1 = 5 * 8 + 1
= 41
Σ^5_i=1Σ^8_j=1(1 + 2aj - bij)
= 41

example:
find the value of the double sum
Σ^50_i=1Σ^100_j=1(4(aij + bij))
given
Σ^50_i=1Σ^100_j=1(aij) = 40
Σ^50_i=1Σ^100_j=1(bij) = 60
= 4Σ^50_i=1Σ^100_j=1(aij) + 4Σ^50_i=1Σ^100_j=1(bij)
= 4 * 40 + 4 * 60
= 160 + 240
= 400

example:
Σ^5_i=1Σ^10_j=1(1 - aij - bij)
Σ^5_i=1Σ^10_j=1(aij) = 10
Σ^5_i=1Σ^10_j=1(bij) = 20
= (Σ^5_i=1Σ^10_j=1(1)) - 30
= 5 * 10 - 30
= 50 - 30
= 20

The product and swap rules
the product rule
Σ^m_i=1Σ^n_j=1(aibj) = Σ^m_i=1(ai)Σ^n_j=1(bj)
the swap rule
Σ^m_i=1Σ^n_j=1(aij) = Σ^n_j=1Σ^m_i=1(aij)

Appling the product rule and swap rules
find the value of the double sum
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
given
Σ^15_i=1(ai) = 40
Σ^18_j=1(bj) = -8
Σ^15_i=1Σ^18_j=1(ai - 2)(bj + 1)
= Σ^15_i=1(ai - 2)Σ^18_j=1(bj + 1)
= (Σ^15_i=1(ai) - Σ^15_i=1(2)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (Σ^15_i=1(ai) - 2Σ^15_i=1(1)) * (Σ^18_j=1(bj) + Σ^18_j=1(1))
= (40 - 2 * 15) * (-8 + 18)
= 10 * 10
= 100

example:
Σ^10_i=1Σ^20_j=1(ai - 1)(bj + 1)
given
Σ^10_i=1(ai) = 12
Σ^20_j=1(bj) = 5
= (Σ^10_i=1(ai - 1)Σ^20_j=1(bj + 1))
= (Σ^10_i=1(ai) - Σ^10_i=1(1)) * (Σ^20_j=1(bj) + Σ^20_j=1(1))
= (12 - 10) * (5 + 20)
= (2) * (25)
= 50

example:
Σ^30_i=1Σ^30_j=1(2ai(bj - 2))
given
Σ^30_i=1(ai) = 42
Σ^30_j=1(bj) = 70
= 2Σ^30_i=1(ai)Σ^30_j=1(bj - 2)
= (2Σ^30_i=1(ai)) * (Σ^30_j=1(bj) - 2Σ^30_j=1(1))
= (2 * 42) * (70 - 2 * 30)
= (84) * (70 - 60)
= (84) * (10)
= 840

===================================================

We often want to know if there is a relationship between two random variables. For this reason we wish to formulate the idea of a joint probability distribution
X represents the number of heads obtained when a fair coin is flipped. The support of X denoted S_X consists of all possible values of X is given by S_X = {0,1}
Y represents the result of rolling a fair tetrahedral die then the support of Y denoted S_Y is S_Y = {1,2,3,4}
The joint support of X and Y denoted S consists of all possible pairs (x, y) such that x is a possible outcome for X and y is a possible outcome for Y
S = S_X x S_Y = {
	(0,1), (0,2), (0,3), (0,4)
	(1,1), (1,2), (1,3), (1,4)
}
S_X x S_Y is the cartesian product of S_X and S_Y
flipping of the coin and rolling of the die are independent then for any x ∈ X and any y ∈ Y
P(X = x and Y = y) = 1/2 * 1/4 = 1/8
the joint probability mass function of X and Y denoted f(x,y) can be represented by the table
f(x,y)|y=1|y=2|y=3|y=4|
x = 0 |1/8|1/8|1/8|1/8|
x = 1 |1/8|1/8|1/8|1/8|
similar to the case of single random variables
f(x,y) = P(X = x and Y = y)
P(X = x,Y = y) as short hand

Bivariate and multivariate distributions
X and Y are discrete random variables with supports S_X and S_Y
f(x,y) = P(X = x,Y = y)
to be a valid joint probability mass function with joint support S = S_X x S_Y it must satisfy following conditions
0 <= f(x,y) <= 1 for all (x,y) ∈ S
Σ_(x,y)∈S f(x,y) = 1
P((X,Y) ∈ A) = Σ_(x,y)∈A f(x,y) where A is a subset of S
some intuition
the consition 0 <= f(x,y) <= 1 for all (x,y) ∈ S states that any possible outcome for (X,Y) must have a probability between 0 and 1
The Σ_(x,y)∈S f(x,y) = 1 states that the sum of all probabilities over all possible values of X and Y must add up to 1
The third condition states that we compute the probability of an event A by adding up the probabilites associated with A
The joint probability table
f(x,y) | Y = y1 | Y = y2 | ... | Y = yn |
x = x1 |f(x1,y1)|f(x1,y2)| ... |f(x1,yn)|
x = x2 |f(x2,y1)|f(x2,y2)| ... |f(x2,yn)|
 ...   |   ...  |   ...  | ... |   ...  |
x = xk |f(xk,y1)|f(xk,y2)| ... |f(xk,yn)|
two random variables the joint distribution is sometimes called a bivariate distribution, any number of random variables is a multivariate distribution

finding the joint distribution of two discrete random variables
   |Y=0|  Y=1 |
X=0|a/2|   a  |
X=1| a |5/2(a)|
the joint probability mass function f(x,y) for the discrete random variables X and Y is given above
The support of X is S_X = {0,1} the support of Y is S_Y = {0,1} and therefore the joint support S is
S = S_X x S_Y = {(0,0),(0,1),(1,0),(1,1)}
f(0,0) + f(0,1) + f(0,1) + f(1,1) = 1
a/2 + a + a + 5/2(a) = 1
5a = 1
a = 1/5

example:
   |Y=1|Y=2|
X=0| 2a| 3a|
X=1| 5a| 2a|
S = S_X x S_Y = {(0,1),(0,2),(1,1),(1,2)}
2a + 3a + 5a + 2a = 1
12a = 1
a = 1/12

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
which of the following conditions are true?
1. 0 <= f(x,y) <= 1 for all (x,y) in S
2. Σ_(x,y)∈S f(x,y) = 1
3. f(x,y) is a valid joint probability mass function
all 3 are valid, and since 1 and 2 we've seen makes 3 true.

Calculating a joint probability from a table
   |Y=1 |Y=3 |
X=2|0.05|0.25|
X=4|0.2 |0.05|
X=6|0.3 |0.15|
compute P((X,Y) ∈ {(2,3),(6,1)})
= P((X,Y) = (2,3)) + P((X,Y) = (6,1))
= f(2,3) + f(6,1)
= 0.25 + 0.3
= 0.55

example:
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
calculate P(X=0,Y=1)
= f(0,1) = 1/2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P((X,Y) ∈ {(1,4),(1,6),(2,6)})
= 0.25 + 0.25 + 0.2
= 0.7

Calculating a joint probability containing inequalities from a table
   |Y=0 |Y=2 |Y=4
X=0|0.15|0.1 |0.1
X=1|0.05|0.1 |0.05
X=2|0.05|0.25|0.15
Compute P(X*Y >= 4)
= P((X,Y) ∈ {(1,4),(2,2),(2,4)})
= 0.05 + 0.25 + 0.15
= 0.45

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X = 2 and Y <= 4)
= P((X,Y) ∈ {(2,2),(2,4)})
= 0.05 + 0.15
= 0.2

example:
   |Y=2 |Y=4 |Y=6 |
X=1|0.1 |0.25|0.25|
X=2|0.05|0.15|0.2 |
Compute P(X < 2 or Y = 6)
= P((X,Y) ∈ {(1,2),(1,4),(1,6),(2,6)})
= 0.1 + 0.25 + 0.25 + 0.2
= 0.8

===================================================

The joint PMF f(x,y) tells us all possible events (X,Y) and the probabilities associated with each event, however we wish to determine the PMF for X only how can this be deduced from the joint PMF?
consider the joint PMF
   |Y=1 |Y=2 |
X=1|0.05|0.15|
X=2|0.7 |0.1 |
The support of X is S_X = {1,2} lets now deduce the probabilites associated with each value of X in S_X
According to the law of total probability we can deduce P(X = 1) by summing the values in the first row of the table
P(X = 1) = P(X = 1,Y = 1) + P(X = 1,Y = 2)
= 0.05 + 0.15
= 0.2
summing the values in second row in table
P(X = 2) = P(X = 2,Y = 1) + P(X = 2,Y = 2)
= 0.7 + 0.1
= 0.8
PMF for X
x     | 1 | 2 |
f_X(x)|0.2|0.8|
f_X(x) was deduced from the joint probability mass function, we call f_X(x) the marginal mass function for X

Deducing the marginal mass function for Y
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
the marginal mass function in the right margin of the table
P(X = 1) = f_X(1) = 0.2
P(X = 2) = f_X(2) = 0.8
similary we can compute the marginal mass function of Y denoted f_Y(y) by summing the columns adding these totals to our table
   |Y=1 |Y=2 |f_X|
X=1|0.05|0.15|0.2|
X=2|0.7 |0.1 |0.8|
f_Y|0.75|0.25| 1 |
P(Y = 1) = f_Y(1) = 0.75
P(Y = 2) = f_Y(2) = 0.25
marginal mass function f_Y(y) of Y
x     | 1  |  2 |
f_Y(y)|0.75|0.25|

A formal definition of the margin mass function
The maginal mass function of X denoted f_X(x)
f_X(x) = P(X = x) = Σ_y∈SY f(x,y)
The marginal mass function of Y denoted f_Y(y)
f_Y(y) = P(Y = x) = Σ_x∈SX f(x,y)
To compute P(X = x) for some particular x we sum all possible values of Y in the row corresponding to X = x
To compute P(Y = y) for some particular y we sum all possible values of X in the column corresponding to Y = y
Marginal mass functions are sometimes called marginal probability mass functions, marginal PMFs, or marginal distributions

Finding a marginal probability
   |Y=0 |Y=1 |
X=0|1/6 |5/12|
X=1|1/4 |1/6 |
P(X = x) = f_X(x) = Σ_y f(x,y)
P(Y = y) = f_Y(y) = Σ_x f(x,y)
The marginal distribution for X corresponds to the row totals and the marginal distribution for Y corresponds to the column totals.
   |Y=0 |Y=1 |f_X |
X=0|1/6 |5/12|7/12|
X=1|1/4 |1/6 |5/12|
f_Y|5/12|7/12|  1 |
P(X = 0) = f_X(0)
= f(0,0) + f(0,1)
= 1/6 + 5/12
= 7/12

example:
find P(X = 0)
   |Y=1 |Y=2 |
X=0|0.4 |0.2 |
X=1|0.15|0.25|
= 0.4 + 0.2
= 0.6

example:
find f_Y(0)
   |Y=0|Y=1|
X=0|1/4|1/2|
X=1|1/8|1/8|
= 1/4 + 1/8
= 3/8

(rest of examples are simple table lookups and summations no new knowledge to note)
