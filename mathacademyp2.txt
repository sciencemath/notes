NOTE: for vectors or matrices I sometimes use the pipe symbol "|" or "," to denote next row to simply save space.

NOTE: In previous notebooks Ive used S to denote integrate, here ill use: ∫_0^2pi "_" the start of the ingtegral "^" represents end here 0 to 2pi. 

NOTE: iff = if and only if

Review:
Calculating dot product:
u = [
	7
	5
   -9
	3
]
v = [
	1
	2
   -5
   -2
]
and v⋅v = 34, what is (u⋅2v)/(3v⋅v)
we compute u⋅v first then multiply the result by 2:
u⋅2v = 2(u⋅v)
= 2(u1⋅v1 + u2⋅v2 + u3⋅v3 + u4⋅v4)
= 2(7⋅1 + 5⋅2 + (-9)⋅(-5) + 3⋅(-2))
= 2(7+10+45-6)
= 2⋅56
= 112
denominator:
3v⋅v = 3(v⋅v)
= 3⋅34
= 102

(u⋅2v)/(3v⋅v) = 112/102 = 56/51

example:
u = [
   -1
   -3
   -2
	1
]
v = [
   -3
   -1
   -2
	2
]
u⋅u = 15
(u⋅(1/2u))/(u⋅(3/4v)) =
numerator
u⋅(1/2u) = 1/2(u⋅u)
= 1/2 ⋅ 15
= 15/2
denominator
u⋅(3/4v) = 3/4(u⋅v)
3/4((-1)⋅(-3) + (-3)⋅(-1) + (-2)⋅(-2) + 1⋅2)
3/4(3+3+4+2)
3/4⋅12
= 9

example:
u = [
   -2
	3
	1
	4
]
v = [
	1
   -2
    2
   -4
]
u⋅v = -22, then (u⋅(2v))/(u⋅(5u))
-44/150
-22/75

apply properties of the dot product
(5a + 3b)⋅c + (4b + 2c)⋅d
c⋅a = 2, b⋅c = -1, b⋅d = 2, d⋅c = 3
= (5a)⋅c + (3b)⋅c + (4b)⋅d + (2c)⋅d
= 5(a⋅c) + 3(b⋅c) + 4(b⋅d) + 2(c⋅d)
= 5(2) + 3(-1) + 4(2) + 2(3)
= 21

example:
(3a + b)⋅c + (2b - 2c)⋅d
c⋅a = 3, c⋅b = -2, b⋅d = -1, d⋅c = 2
3(a⋅c) + (b⋅c) + 2(b⋅d) - 2(c⋅d)
= 1

example:
(5a - 2b)⋅c + (3b)⋅d
c⋅a = -1, c⋅b = -2, b⋅d = 3
5(a⋅c) - 2(b⋅c) + 3(b⋅d)
-5 + 4 + 9
= 8

==========================================

Review:
Vector space R^n paired with the dot product is called Euclidean space

for vector x=[x1,x2,x3 ... xn] in R^n, the norm or length:
||x|| = sqrt(x⋅x) = sqrt(x1^2 + x2^2 + ... + xn^2)

example:
vector x = [
	-5
	12
]
||x|| = sqrt((-5)^2 + (12)^2)
= sqrt(25 + 144)
= sqrt(169)
= 13

In Euclidean space, the distance between the vectors x and y is the norm of the difference x - y
d(x,y) = ||x - y|| = sqrt((x1-y1)^2 + (x2-y2)^2)

example: 
vectors x = [1|4], y = [-5|1]
sqrt((1-(-5))^2 + (4-1)^2)
= sqrt(6^2 + 3^2)
= sqrt(45)
= 3sqrt(5)

example:
u = [
	-4
	0
	1
	7
]
v = [
	-2
	-3
	1
	3
]
sqrt(-2^2 + 3^2 4^2)
= sqrt(29)

We can factor out any constants out of the norm and take the absolute value for instance:
vector x = [3|4] tp compute the norm of multiple of x say -2x, we can factor value |-2| out
||-2x|| = |-2|⋅||x||
= 2⋅sqrt(3^3 + 4^2)
= 2⋅sqrt(25)
= 2⋅5
= 10
we could have gotten the same result if we multiplied the -2 first before computing the norm:
[
  (-2)⋅3
  (-2)⋅4
] = [-6|-8]
= sqrt((-6)^2 + (-8)^2)
= sqrt(100)
= 10

example:
x = [
	-9
	sqrt(11)
	0
	-5
	2
]
find ||-3x||
= |3|⋅||x||
= 3 * sqrt(81 + 11 + 25 + 4)
= 3 * sqrt(121)
= 3 * 11
= 33

example:
||3x|| = 4, find ||x/5||
|3|⋅||x|| = 4
||x|| = 4/3

||x/5|| = 1/5⋅||x||
= 1/5 ⋅ 4/3
= 4/15

A unit vector is a vector length is equal to 1. If we divide a non-zero vector x by its norm ||x|| we obtaiin a unit vector v = x/||x||
||v|| = || x/||x|| || = 1/||x||⋅||x|| = 1

The process of dividing a vector by its norm ||x|| is called normalizing the vector x, the unit vector v = x/||x|| is the same direction as x.

example:
normalize x = [2, -1, -2, 3]
= sqrt(18)

1/3sqrt(2)⋅[2, -1, -2, 3]
= [
	2/3sqrt(2),
	-1/3sqrt(2),
	-2/3sqrt(2)
	3/3sqrt(2)
] = [
	sqrt(2)/3
	-sqrt(2)/6
	-sqrt(2)/3
	sqrt(2)/2
]

example:
x = [4, -2, -4]
= sqrt(36)
= 6⋅[4, -2, -4]
= 1/6⋅[4, -2, -4]
= [4/6, -2/6, -4/6]
= [2/3, -1/3, -2/3]

==========================================

Identifying a set that is closed under addition and scalar multiplication is the first step determining whether a set can form a vector space

Sets not closed under addition and scalar multiplication cannot be vector spaces

properties of addition in R^2 two vectors and M_2(R) two matrices:
communitive
R^2 u + v = v + u, M_2(R) A + B = B + A
associative
R^2 (u+v)+w = u+(v+w), M_2(R) (A+B)+C = A+(B+C)
unique zero element
R^2 u+0 = u, M_2(R) A+0 - A
unique opposite element:
R^2 u + (-u) = 0, M_2(R) A + (-A) = 0

properties of multiplication in R^2 two vectors and M_2(R) two matrices:
invariance by 1
R^2 1⋅u = u, M_2(R) 1⋅A = A
R^2 a(bu) = (ab)u, M_2(R) a(bA) = (ab)A
R^2 (a + b)u = au + bu, M_2(R) (a + b)A = aA + bA
R^2 a(u + v) = au + av, M_2(R) a(A + B) = aA + aB

theorem:
Vector addition:
For any two elements x,y ∈ V there is a unique element x + y that also lies in V. We say that V is closed under vector addition

Scalar multiplication:
For any element x ∈ V and any number α ∈ R, there is a unique element αx that also lies in V. We say that V is closed under scalar multiplication.

completing the sentences:
V is in the set of all polynomials of degree 3.
V is not a vector space because the sum of two polynomials of degree 3 may not be a polynomial of degree 3

W is in the set of all polynomials with coefficients of degree 5 or greater.
W is not a vector space (with respect to polynomials addition and multiplication of a polynomial by a real number) because W does not contain the zero element

C is in the set of function f: R->R such that f(x) >= 0.
C is not a vector space (with respect to function addition and scalar multiplication of a function by a real number) because multiplication of f ∈ C by a scalar is not closed.
example:
α = -1 and f(x) = x^2 + 1, then αf(x) = -f(x) < 0

Review:
weighted dot product is a type of inner product
<x,y> = 2x1y1 + x2y2 + 9x3y3
x = [1,-2,1], y = [5,3,1]
= 10 - 6 + 9 = 13

example:
<x,y> = 2x1y1 + 2x2y2 + 6x3y3
x = [2,2,0], y = [1,5,1]
= 24

An inner product of polynomials in one variable:
<x(t),y(t)> = x(1)y(1) + x(2)y(2) + x(3)y(3)
x(t) = 1 - t + t^2, y(t) = t - t^2
(1)(0) + (3)(-2) + (7)(-6)
= -48

example:
<x(t), y(t)> = x(-1)y(-1) + x(0)y(0) + x(1)y(1)
x(t) = 5 - 2t + t^2, y(t) = t - 1
(8)(-2) + (5)(-1) + (4)(0)
= -21

calculating inner product of two vectors in a vector space of continuous functions
V = C[0, 2pi] the vector space of all functions that are continuous on [0, 2pi]. Find <sin t, cos t> if the innter product on V is defined as:
<x(t),y(t)> = ∫_0^2pi sint cost dt

* INDENTITY:
* sin(2t) = sin(t)cos(t) + cos(t)sin(t)
* combine like terms
* sin(2t) = 2sin(t)cos(t)
* rearranging this we get:
* sin(t)cos(t) = 1/2sin(2t)

∫_0^2pi sin(t)cos(t) dt = 1/2 ∫_0^2pi sin(2t) dt
= -1/4cos(2t) |0 to 2pi
= -1/4(cos(4pi) - cos(0))
= -1/4(1 - 1)
= 0

example:
V = C[0,2] the vector space of all functions that are continuous on [0,2], find <t e^t> if inner product on V is defined as:
<x(t),y(t)> = ∫_0^2 x(t)y(t)dt
= <t,e^t> = ∫_0^2 te^t dt

* INTEGRATION BY PARTS:
* ∫u dv = uv - ∫v du

* u = t  =>  u' = 1
* v' = e^t  =>  v = ∫v'dt = ∫e^t dt = e^t

* more simply: 
* ∫te^t dt = te^t - ∫e^t
* ∫te^t dt = te^t - e^t

∫_0^2 te^t dt = uv |0to2 - ∫_0^2 vu'dt
= (te^t) |0to2 - ∫_0^2 e^t dt
= (2e^2 - 0e^0) - e^t |0to2
= 2e^2 - (e^2 - 1)
= e^2 + 1
or:
= (te^t - e^t) |0to2
= upper limit t=2: (2e^2 - e^2) = e^2
= lower limit t=0: (0*e^0 - e^0) = -1
= upper-lower: e^2-(-1) = e^2 + 1

example:
V = C[-pi/2, pi/2] the vector space of all functions that are continuous on [-pi/2, pi/2]. Find <sin^2(t),cos(t)> if the inner product on V is defined as:
<x(t),y(t)> = ∫_-p1/2^pi/2 x(t)y(t)dt

<sin^2(t),cos(t)> = ∫_-p1/2^pi/2 sin^2(t)cos(t)dt
∫_-p1/2^pi/2 sin^2(t)cos(t)dt = ∫_-1^1 u^2du
= 1/3u^3 |-1to1
= [1/3(1)^3] - [1/3(-1)^2]
= 1/3 + 1/3
= 2/3

==========================================

Two vectors u and v are orthogonal if their dot product equals zero:
u ⋅ v = 0
we denote this by u ⊥ v
e1 = [1,0] e2 = [0,1]
= 0
The zero vector is orthogonal to any vector since 0⋅u = 0

example:
u = [1,4,-2,3] which are orthogonal to u?
v = [2,-1,2,2] = 0
w = [-2,-3,3,5] = -5
x = [3,-2,1,2] = -1

example:
u = [-2,3,-7,1]
v = [-4,-3,1,7] = -1
w = [3,0,-1,-1] = 0
x = [1,4,2,-4] = -8

components in orthogonal vectors
example a ⊥ b, find the value of k:
a = [-3,k,2,-4]
b = [-2,k,-1,k]
k^2 - 4k + 4 = 0
(k - 2)^2 = 0
k = 2

example:
whats k?
u = [-5,4,-5,-3]
v = [1,k,5,6]
-48 + 4k = 0
k = 12

calculating dot products with orthogonal vectors
example (v⊥w):
u = [4,-2,1,-3]
v = [-4,3,5,4]
w = [2,-5,7,-3]
(u + 9v)⋅w + v⋅(u + 7w)
(u⋅w) + 9(v⋅w) + (v⋅u) + 7(v⋅w)
34 + -29
= 5

example:
(u⊥v)
u = [3,-2,1,-2]
v = [2,4,0,-1]
w = [-3,2,5,-2]
(v + w)⋅u + v⋅(u + w)
(ignoring 0's)
w⋅u + v⋅w
-4 + 4
= 0

determining components of a vector orthogonal to two given vectors
example:
(u⊥v) and (u⊥w)
u = [-3,a,b,-4]
v = [3,4,1,1]
w = [1,1,-3,-4]
{
	u⋅v = 0
	u⋅w = 0
}
calculating in terms of a and b
{
	4a + b = 13
	a - 3b = -13
} 3 * 1st {
	12a + 3b = 39
	a - 3b = -13
} + 1st to 2nd {
	12a + 3b = 39
	13a = 26
}
a = 2
first equation:
4(2) + b = 13
8 + b = 13
b = 5
a+b=7

example:
(u⊥v) and (u⊥w)
what is value of a+b
u = [a,3,b,-5]
v = [2,-5,3,-4]
w = [-4,-2,5,3]
{
	u⋅v = 0
	u⋅w = 0
}
calculating in terms of a and b
{
	2a + 3b = -5
	-4a + 5b = 21
} 1st * 2 {
	4a + 6b = -10
	-4a + 5b = 21
} 1st to 2nd {
	11b = 11
}
b = 1
substitution
4a + 6(1) = -10
4a = -16
a = -4
a+b = -3

The Cauchy-Schwarz Inequality (sometimes called the Cauchy-Bunyakovsky-Schwarz Inequality) provides a connection between dot products and norms. For any two vectors u and v in R^n we have the inequality:
|u ⋅ v| <= ||u||⋅||v||

The absolute value of the dot product is no more than the product of the norms.
example:
u = [3,-4], v = [5,12]
|-33| <= sqrt(3^2 + -4^2)sqrt(5^2 + 12^2)
33 <= 65

A trick to remember Cauchy-Schwarz inequality is to think of the formula for dot product:
u⋅v = ||u||⋅||v||cosθ
because |cosθ| <= 1:
|u⋅v| <= |u||⋅||v||
(this is only mnemonic trick)

Determining a minimum possible value using the Cauchy-Schwarz Inequality:
u = [sqrt(k), 1, 0, -1]
u⋅v = -2
||v|| = 1
given ^
||u|| = sqrt(2 + k)⋅(1)
|u⋅v| <= |u||⋅||v||
-2 <= sqrt(2 + k)
-2^2 <= sqrt(2 + k)^2
4 <= 2 + k
2 <= k

example:
given: ||u|| = 4, u⋅v = -16
find smallest possible value of ||2v||
16 <= 4⋅||v||
4 <= ||v||
2⋅4 <= 2⋅||v||
8 <= ||2v||
minumum possible value of ||2v|| is 8

Angle between two vectors
Using the dot product, we can define the acute angle θ between two vectors u and v in R^n where n > 3
cosθ will be equal to the ratio of the two sides of the Cauchy-Schwarz inequality:

cosθ = (u⋅v)/(||u||⋅||v||)

Analogous to the formula for the angle between two vectors in two-dimensional or three-dimensional space. It only works for non-zero vectos (otherwise we would be dividing by zero)

Cauchy-Schwarz inequality guarantees |u⋅v| <= ||u||⋅||v||
-1 <= (u⋅v)/(||u||⋅||v||) <= 1

finding the acute angle between two vectors:
v = [-1,2,-4,-2]
w = [-2,2,-2,2]
cosθ = (v⋅w)/(||v||⋅||w||)
= 10/sqrt(25)⋅sqrt(16)
= 10/5⋅4
= 1/2
θ = arcoss(1/2) = pi/3

example:
v = [3,-sqrt(5),7,1]
w = [-1,1,5,sqrt(5)]
cosθ = (v⋅w)/(||v||⋅||w||)
= 32/sqrt(64)sqrt(32)
= sqrt(32)/8
= 4sqrt(2)/8
= sqrt(2)/2
θ = arcoss(sqrt(2)/2) = pi/4

example:
v = [-3,1,-1,5]
w = [-2,2,-2,6]
cosθ = (v⋅w)/(||v||⋅||w||)
= 40/sqrt(36)sqrt(48)
= 40/6⋅4sqrt(3)
= 5/3sqrt(3)
θ = arcoss(5/3sqrt(3)) = 15.79 degrees

calculating cosine of an angle:
A(1,-1,2,0), B(1,0,0,-1), C(0,1,1,0). Find the cosine of angle ∠BAC
v = AB-> and w = AC->
v = b - a = [
	1
	0
	0
	-1
] - [
	1
	-1
	2
	0
] = [
	0
	1
	-2
	-1
]
w = c - a = [
	0
	1
	1
	0
] - [
	1
	-1
	2
	0
] = [
	-1
	2
	-1
	0
]
= 4/sqrt(6)sqrt(6)
= 4/6
= 2/3

example:
A(-2,1,0,-3), B(1,-1,1,-4) Find the cosine of angle ∠AOB where O(0,0,0,0) is the origin
OA-> [-2,1,0,-3]
OB-> [1,-1,1,-4]
= 9/sqrt(14)sqrt(19)
= 9/sqrt(266)

Proof of the Schwarz Inequality:
|u ⋅ v| <= ||u||⋅||v||

Let t be any number, consider the vector tu + v. Since ||tu + v|| >= 0:

||tu + v||^2 >= 0
(tu + v)⋅(tu + v) >= 0
t^2(u⋅u) + t(u⋅v) + t(v⋅u) + (v⋅v) >= 0
t^2||u||^2 + 2t(u⋅v) + ||v||^2 >= 0
a = ||u||^2, b = 2(u⋅v), c = ||v||^2

Since the quadratic is greater than or equal to zero, its discriminant must be less than or zero:

D <= 0
b^2 - 4ac <= 0
(2(u⋅v))^2 - 4⋅||u||^2 ⋅ ||v||^2 <= 0
4(u⋅v)^2 - 4||u||^2||v||^2 <= 0
(u⋅v)^2 <= ||u||^2||v||^2
sqrt((u⋅v)^2) <= sqrt(||u||^2||v||^2)
|u ⋅ v| <= ||u||||v||

===================================================

Calculating the components of a vector orthogonal to a set
given u = [a,b,c]
W = {[1,-3,1],[3,1,2]}
find value of c/(a+b)
{
	u⋅w1 = 0
	u⋅w2 = 0
} = {
	x1 - 3x2 + x3 = 0
	3x1 + x2 - 2x3 = 0
}
[
	1 -3  1   0
	3  1 -2 | 0
]
R2 := R2 + (-3)R1
[
	1 -3  1 | 0
	0 10 -5 | 0
]
10x2 - 5x3 = 0
x2 = 1/2x3
x1 - 3/2x3 + 2/2x3 = 0
x1 = 1/2x3
[
	1/2x3
	1/2x3
	x3
]
setting x3 to 2 we get: [1,1,2]
a = 1, b = 1, c = 2
2/1+1 = 1

example:
u = [a,b,c]
W = {[4,0,-3],[4,-1,2]}
{
	u⋅w1 = 0
	u⋅w2 = 0
} = {
	4x1 - 3x3 = 0
	4x1 - x2 + 2x3 = 0
}
[
	4  0 -3 | 0
	4 -1  2 | 0
]
R2 := R2 + (-1)R1
[
	4  0 -3 | 0
	0 -1  5 | 0
]
-x2 + 5x3 = 0
x2 = 5x3
4x1 - 3x3 = 0
x1 = 3/4x3
[
	3/4x3
	5x3
	x3
]
setting x3 to 4 we get [3,20,4]
b+c/a = 20+4/3 = 8

The set containing all vectors v that are orthogonal to H is called the orthogonal complement of H and its denoted as H^⊥

theorem:
A vector v belongs to H^⊥ if it is orthogonal to a set of vectors that spans H

(imagine a flat plane and H^⊥ passing straight down through the origin, this tells us what vectors are 90 degrees orthogonal to H^⊥ and sitting on the plane in 3 dimensional space)

If H Span{h1,h2,...hn}, then v ∈ H^⊥ iff v is orthogonal to {h1,h2,...hn}

H^⊥ is a subspace of R^n
(H^⊥)^⊥ = H
H ∩ H^⊥ = {0}

example:
subspace H = Span{[-2,-2,-2],[3,3,3]} which belong to H^⊥
v1 = [1,-2,1]
v2 = [1,3,-1]
v3 = [3,-6,3]

v1 -2 4 -2 = 0, 3 -6 3 = 0: v1⊥h1 v1⊥h1 = v1 ∈ H^⊥
v2 -2 -6 2
v3 -6 12 -6 = 0, 9 -18 9 = 0: v3⊥h1 v3⊥h1 = v3 ∈ H^⊥
v1 and v3 only

example:
subspace H = Span{[2,3,4],[2,1,2]}
v1 = [2,-1,2]
v2 = [2,4,-4]
v3 = [1,1,-1]

v1 4 -3 8
v2 4 12 -16 = 0, 4 4 -8 = 0 : v2⊥h1 v2⊥h1 = v2 ∈ H^⊥
v3 2 3 -4

find a basis of H^⊥
example:
H = Span{[5,-2,3]}
{
	5x1 - 2x2 + 3x3 = 0
}
(already in row echelon form)
[
	5 -2 3 | 0
]
x1 = 2/5x2 - 3/5x3
[
	2/5x2 - 3/5x3
		x2
		x3
]
= x2[2/5, 1, 0] + x3[-3/5, 0, 1]
{[2/5, 1, 0], [-3/5, 0, 1]} is a basis for H^⊥

example:
H = Span{[6,-9],[4,-6]}
{
	6x1 - 9x2 = 0
	4x1 - 6x2 = 0
}
[
	6 -9 | 0
	4 -6 | 0
]
R1 = 1/3R1
[
	2 -3 | 0
	4 -6 | 0
]
R1 = R2 + (-2)R1
[
	2 -3 | 0
	0  0 | 0
]
{
	2x1 - 3x2 = 0
}
x1 = 3/2x2
H^⊥ = {[3/2x2, x2] : x2 ∈ R}

checking if sets are orthogonal
example:
U = {[1,2,1,-2],[1,3,-1,3],[2,-1,2,1]}
V = {[2,2,-2],[1,-2,-1],[-6,0,-6]}
W = {[4,-1],[-2,-2]}
u1⋅u2 = 0
u1⋅u3 = 0
u2⋅u3 = 0
v1⋅v2 = 0
v1⋅v3 = 0
v2⋅v3 = 0
w1⋅w2 != 0

example:
U = {[5,-1],[1,5]}
V = {[1,-1],[1,0],[0,1]}
W = {[4,0,-1],[1,0,4]}
u1⋅u2 = 0
v1⋅v2 != 0 (dont need to check the rest)
w1⋅w2 = 0

u_i is a unit vector if ||u_i|| = 1 or equivalently u_i⋅u_i = 1

An orthonormal set that is also a basis, like the standard basis, is called the orthonormal basis
Any othogonal set of non-zero vectors is linearly independent.
An orthogonal set in R^n cannot contain more than n vectors. If V = {v1,v2,...vp} is orthogonal set of non-zero vectors and v_i ∈ R^n then we must have that p <= n

which of these sets are othonormal
example:
U = {[-3/sqrt(10), 1/sqrt(10)], [1/sqrt(10), 3/sqrt(10)]}
V = {[2,3,-1],[3,1,9],[28,-21,-7]}
W = {[3/5,4/5],[4/5,-3/5]}
{
	u1⋅u2 = 0
	v1⋅v2 = 0
	v1⋅v3 = 0
	v2⋅v3 = 0
	w1⋅w2 = 0
}
and
{
	u1⋅u1 = 1
	u2⋅u2 = 1
	v1⋅v1 = 1
	v2⋅v2 = 1
	v3⋅v3 = 1
	w1⋅w1 = 1
	w2⋅w2 = 1
}
(calculations were done in my head)
u1⋅u2 = 0
u1⋅u1 = 1
u2⋅u2 = 1
v1⋅v2 = 0
v1⋅v3 = 0
v2⋅v3 = 0
v1⋅v1 != 1
w1⋅w2 = 0
w1⋅w1 = 1
w2⋅w2 = 1
U and W only

example:
U = {[1,0],[0,1]}
V = {[0,0],[1,0]}
W = {[1/sqrt(2), -1/sqrt(2)], [0,1]}
{
	u1⋅u2 = 0
	v1⋅v2 = 0
	w1⋅w2 = 0
} and {
	u1⋅u1 = 1
	u2⋅u2 = 1
	v1⋅v1 = 1
	v2⋅v2 = 1
	w1⋅w1 = 1
	w2⋅w2 = 1
}
u1⋅u2 = 0
u1⋅u1 = 1
u2⋅u2 = 1
v1⋅v1 != 1
w1⋅w2 != 1
set U only

extending the set so that it forms an orthogonal basis of R^4 by adding a new vector x = [x1,x2,x3,x4]. What is the value of x1/x4
U = {[1,1,1,1],[1,-1,1,-1],[1,2,-1,-2]}
{
	x1 + x2 + x3 + x4 = 0
	x1 - x2 + x3 - x4 = 0
	x1 + 2x2 - x3 -2x4 = 0
}
[
	1  1 1  1 | 0
	1 -1 1 -1 | 0
	1 2 -1 -2 | 0
]
R2 := R2 + (-1)R1
[
	1  1 1  1 | 0
	0 -2 0 -2 | 0
	1 2 -1 -2 | 0
]
R3 := R3 + (-1)R1
[
	1  1 1  1 | 0
	0 -2 0 -2 | 0
	0 1 -2 -3 | 0
]
R2 := R2 + (-1/2)R2
[
	1 1  1  1 | 0
	0 1  0  1 | 0
	0 1 -2 -3 | 0
]
R3 := R3 + (-1)R2
[
	1 1  1  1 | 0
	0 1  0  1 | 0
	0 0 -2 -4 | 0
]
R3 := (-1/2)R3
[
	1 1  1  1 | 0
	0 1  0  1 | 0
	0 0  1  2 | 0
]
R1 := R1 + (-1)R3
[
	1 1  0 -1 | 0
	0 1  0  1 | 0
	0 0  1  2 | 0
]
{
	x1 + x2 - x4 = 0
	x2 + x4 = 0
	x3 + 2x4 = 0
}
x3 = -2x4
x2 = -x4
x1 - 2x4 = 0
x1 = 2x4
[
	2
	-x4
	-2x4
	x4
]
setting x4 to 1: [2, -1, -2, 1]
x1/x4 = 2/1 = 2

Extend the set so that it forms a orthogonal basis of R^3. What is the value of x1/x2
U = {[1,2,1],[0,-3,6]}
[
	1  2 1 | 0
	0 -3 6 | 0
]
(already in row echelon form)
x2 = 2x3
x1 + 2(2x3) + x3 = 0
x1 + 5x3 = 0
x1 = -5x3
[
	-5x3
	2x3
	x3
] = [
	-5
	2
	1
]
x1/x2 = -5/2

Extend the set so that it forms an orthonormal basis of R^3. What is the value of |x1|+|x2|+|x3|
U = {[1/sqrt(5),0,-2/sqrt(5)],[2/3,2/3,1/3]}
[
	1/sqrt(5) 0 -2/sqrt(5) | 0
		2/3   2/3   1/3     | 0
]
R1 := sqrt(5)R1
[
	 1   0  -2  | 0
	2/3 2/3 1/3 | 0
]
R2 := 3R2
[
	 1 0 -2 | 0
	 2 2  1 | 0
]
R2 := R2 - 2R1
[
	 1 0 -2 | 0
	 0 2  5 | 0
]
x2 = -5/2x3
x1 - 2x3 = 0
x1 = 2x3
[
	2x3
	-5/2x3
	x3
]
setting x3 = 2: [4, -5, 2]
= sqrt(16 + 25 + 4)
= sqrt(45) 45 -> 3 * 15 -> 3 * 3 * 5
= 3sqrt(5)
normalize x as
x/||x|| = [
	4/3sqrt(5)
	-5/3sqrt(5)
	2/3sqrt(5)
]
othonormal basis of R^2 is:
(same as U with our normalized vector inserted)
{[1/sqrt(5),0,-2/sqrt(5)], [2/3,2/3,1/3], [
	4/3sqrt(5)
	-5/3sqrt(5)
	2/3sqrt(5)
]}
|x1|+|x2|+|x3| 
= |4/3sqrt(5)| + |-5/3sqrt(5)| + |2/3sqrt(5)|
= 11/3sqrt(5)

example:
(extending the set so it forms an orthonormal basis R^2)
What is the value of |x1|+|x2|
U = {[3/sqrt(13), -2/sqrt(13)]}
[
	3/sqrt(13) -2/sqrt(13) | 0
]
R1 := sqrt(13)R1
[
	3 -2 | 0
]
3x1 - 2x2 = 0
x1 = 2/3x2
[
	2
	3
]
sqrt(4 + 9)
sqrt(13)
x/||x|| = [
	2/sqrt(13)
	3/sqrt(13)
]
|x1|+|x2| = |2/sqrt(13)| + |3/sqrt(13)|
= 5/sqrt(13)

===================================================

It can be a lot of work checking a matrix Q that form an orthonormal set. We have this theorem for easier calculations

Theorem:
A matrix Q is orthogonal iff Q⋅Q^T = Q^T⋅Q = I

Q^T is transpose of Q. The matrix we get when we interchange the rows and columns of Q
To verify that a matrix is orthogonal, we only need to check QQ^T = I

example:
A A^T [
	1/2 0  1
	 0  1  0
	 1  0 -1/2
][
	1/2 0  1
	 0  1  0
	 1  0 -1/2
] = [
	5/4 0 0
	 0  1 0
	 0  0 5/4
]
does not equal the identity matrix so this is not orthogonal.

example:
A⋅A^T [
	-1 0
	 0 1
][
	-1 0
	 0 1
] = [
	1 0
	0 1
]
This equals identity matrix (I) matrix is orthogonal

Every orthogonal matrix Q is invertible
The inverse of an orthogonal matrix Q is Q^T that is
Q^-1 = Q^T

Orthogonal matrices properties:
det(Q) = +/- (determinant of an orthogonal matrix is either 1 or -1)
If λ is an eigenvalue of an orthogonal matrix Q, then λ = +/- 1

these properties are necessary but not sufficient
Every orthogonal matrix has determinant equal to +/- 1, but not every matrix with dereminant +/- 1 is orthogonal
Every orthogonal matric has eigenvalues 1 or -1 only, but not every matrix with eigenvalues 1 or -1 is orthogonal.

computing the product (P^-1Q)⋅(P^-1Q)^T
(P^-1Q)⋅(P^-1Q)^T = (P^TQ)⋅(P^TQ)^T
= P^TQ⋅Q^T(P^T)^T
= P^T(QQ^T)P
= P^T(I)P
= P^TP
= I

computing the product (PQ^T)⋅(PQ^T)^T
(PQ^T)⋅(PQ^T)^T = PQ^T⋅(Q^T)^TP^T
= PQ^TQP^T
= P(Q^TQ)P^T
= P(I)P^T
= PP^T
= I

===================================================

The orthogonal projection of a vector x onto a Span{a} is the vector in Span{a} that is "closest" to x.

ka is called the orthogonal projection of x onto Span{a}
We can write the vector x as the sum
x = ka + z

k ∈ R and z⊥a

to compute the orthogonal projection ka, we can first multiply both sides of the equation above by a to get
x⋅a = (ka + z)⋅a
x⋅a = k(a⋅a) + z⋅a

Since a⊥z, z⋅a = 0

x⋅a = k(a⋅a)
k = x⋅a/a⋅a

consider these two vectors:
x = [5,-1,0,1], a = [3,-6,0,6]

k = (5⋅3 + (-1)⋅(-6) + 0⋅0 + 1⋅6)/(3^2 + (-6)^2 + 6^2)
k = 1/3
ka = 1/3[3,-6,0,6] = [1,-2,0,2]

Instead of going through this process each time, we can compute the orthogonal projection of x onto Span{a} using the following formula:

proj_a x = (a⋅x/a⋅a)a

example:
b = [2,4,-4,2], a = [6,2,-2,6]
Find the orthogonal projection of b onto Span{a}
proj_a b = (a⋅b/a⋅a)a
= 12 + 8 + 8 + 12/80
= 40/80
= 1/2a
= 1/2[6,2,-2,6]
= [3,1,-1,3]

example:
a = [3,2,-3,3], b = [2,0,-2,2]
proj_b a = (b⋅a/b⋅b)b
= 6 + 0 + 6 + 6/12
= 18/12 = 9/6 = 3/2
3/2[2,0,-2,2]
= [3,0,-3,3]

The distance between the vector x and the vector space Span{a} to be:
||x - proj_a x||

We can also define the angle θ between the vector x and the subspace Span{a} to be acute angle between the vector x and its othogonal projection proj_a x.

The vector x as a sum of two orthogonal vectors
x = proj_a x + (x - proj_a x)
      ^y              ^z
where y ∈ Span{a} and z⊥Span{a}
Thhe vector x - proj_a x whose norm ||x - proj_a x|| represents the distance from x to a is sometimes called the vector rejection of x from a

example:
x = [12,6,-6,12], a = [2,6,-6,2].
x = y + z where y ∈ Span{a} and z⊥Span{a}
whats z
proj_a x = (a⋅x/a⋅a)a
24 + 36 + 36 + 24
120/80 = 3/2
= 3/2[2,6,-6,2]
= [3,9,-9,3]
y = proj_a x = [3,9,-9,3]
z = x - proj_a x = [12,6,-6,12] - [3,9,-9,3]
= [9,-3,3,9]

example:
x = [4,4,-2,4], b = [1,2,-2,1]
whats the distance between x and Span{b}
proj_b x = (b⋅x/b⋅b)b
4 + 8 + 4 + 4
20/10
2[1,2,-2,1]
[4,4,-2,4] - [2,4,-4,2]
= [2,0,2,2]
= sqrt(4 + 4 + 4)
= sqrt(12) = 2⋅6 -> 2⋅2⋅3
= 2sqrt(3)

Finding orthogonal projection of a vector onto the solution space of a system of equations
find the projection of x onto the solution space of the system:
x = [-10,-15,28]
{
	x1 - 2x2 = 0
	2x1 + x2 - 3x3 = 0
}
[ 
	1 -2  0 | 0
	2  1 -3 | 0
]
R2 := R2 + (-2)R1
[ 
	1 -2  0 | 0
	0  5 -3 | 0
]
x2 = 3/5x3
x1 = 6/5x3
[
	6
	3
	5
]
35/70 = 1/2
1/2[6,3,5]
= [3,3/2,5/2]

example:
x = [11,3]
{
	x1 + x2 = 0
	2x1 + 2x2 = 0
}
[
	1 1 | 0
	2 2 | 0
]
R2 := R2 + (-2)R1
[
	1 1 | 0
	0 0 | 0
]
x1 = -x2
[
	-x2
	x2
] = [-1, 1]
-11 + 3
-8/2
-4[-1, 1]
= [4, -4]

orthogonal decomposition theorem
Let W be a subspace of R^n. Then each x ∈ R^n can be written uniquely in the form
x = y + z
where y ∈ W and z W^⊥

Since y is the orthogonal projection of x onto the subspace W, if {u1 ..., up} is any orthogonal basis of W then
y = proj_W x
= proj_u1 x + proj_u2 x + ... + proj_up x
= (x⋅u1/u1⋅u1)u1 + (x⋅u2/u2⋅u2)u2 + ... (x⋅up/up⋅up)up

===================================================

finding the orthogonal projection of a vector onto a subspace spanned by orthogonal vectors

find the orthogonal projection of x onto the subspace S given that the set {a1, a2} is orthogonal
example:
subspace S = Span{a1,a2}
a1 = [1,1,2], a2 = [1,1,-1], x = [3,1,6]

proj_a1 x = 8/3[1,1,2] = [8/3, 8/3, 16/3]
proj_a2 x = -2/3[1,1,-1] = [-2/3,-2/3,2/3]
=[8/3, 8/3, 16/3] + [-2/3,-2/3,2/3] = [2,2,6]

example:
a1 = [-1,1,1,1], a2 = [3,1,1,1], a3 = [0,-2,1,1]
x = [1,4,3,1]

proj_a1 x = 7/4[-1,1,1,1] = [-7/4,7/4,7/4,7/4]
proj_a2 x = 11/12[3,1,1,1] = [33/12,11/12,11/12,11/12]
proj_a3 x = -2/3[0,-2,1,1] = [0,4/3,-2/3,2/3]
[-7/4,7/4,7/4,7/4] + [33/12,11/12,11/12,11/12] + [0,4/3,-2/3,2/3]
[-21/12 + 33/12, 21/12 + 11/12 + 16/12, 21/12 + 11/12 + -8/12]
[1,4,2,2]

Consider the vector x and the plane r = sv + tw where s, t ∈ R
v = [-2,1,3], w = [2,-2,2], x = [10,6,14]
find the orthogonal projection of x onto the plane given v⊥w
proj_v x = 2[-2,1,3]
proj_w x = 3[2,-2,2]
= [-4,2,6] + [6,-6,6]
= [2,-4,12]

Consider the vector x and the plane r = sv + tw where s, t ∈ R
v = [1,1,1], w = [2,-1,-1], x = [3,3,0]
find the orthogonal projection of x onto the plane given v⊥w
proj_v x = 2[1,1,1]
proj_w x = 3/6[2,-1,-1]
= [2,2,2] + [1, -3/6, -3/6]
= [3, 3/2, 3/2]

Now we can calculate the distance between a vector and a subspace spanned by orthogonal vectors.
example:
a1 = [1,0,-1], a2 = [1,1,1], x = [4,1,2]
proj_a1 x = 1[1,0,-1] = [1,0,-1]
proj_a2 x = 7/3[1,1,1] = [7/3,7/3,7/3]
[1,0,-1] + [7/3,7/3,7/3] = [10/3, 7/3, 4/3]
sqrt((4 - 10/3)^2 + (1 - 7/3)^2 + (2 - 4/3)^2)
sqrt(4/9 + 16/9 + 4/9)
sqrt(24/9)
= sqrt((8 * 3/3 * 3))
= sqrt((8 * 3/3^2))
= sqrt(8 * 3)/3 = sqrt(24)/3 = sqrt(2 * 12)/3
= sqrt(2 * 2 * 6)/3 = sqrt(2 * 2 * 2 * 3)/3
= 2 * sqrt(2 * 3)/3
= 2 * sqrt(6)/3

example:
find the distance between x and the subspace S given that the set {a1, a2} is orthogonal
a1 = [1,-1,-1], a2 = [2,1,1], x = [2,1,3]
2 -1 -3
proj_a1 x = -2/3[1,-1,-1] = [-2/3, 2/3, 2/3]
proj_a2 x = 4/3[2,1,1] = [8/3, 4/3, 4/3]
[-2/3, 2/3, 2/3] + [8/3, 4/3, 4/3] = [2, 2, 2]
sqrt((2 - 2)^2 + (1 - 2)^2 + (3 - 2)^2)
sqrt(2)

* !IMPORTANT!
* when calculating cosine we can factor out a common fraction
* BELOW we factor out a 1/9 from: 1/9[1, 11, -7]
* we're not concerned in the magnitude but the direction
* the cosine of an angle depends only on directions.

Calculating the acute angle between a vector and a subspace spanned by orthogonal vectors.
example:
v = [1,1,1], w = [2,-1,-1], x = [3,3,0]
find the acute angle between x and subspace S given that the set {v,w} is orthogonal.
proj_v x = 2[1,1,1] = [2,2,2]
proj_w x = 1/2[2,-1,-1] = [1,-1/2,-1/2]
[2,2,2] + [1,-1/2,-1/2] = [2,3/2,3/2]
x = [3,3,0]
2/3proj_S x = [2, 1, 1]
cosθ = (x⋅proj_S x)/||x||⋅||proj_S x||
= (2)(3)+(1)(3)+(1)(0)/(sqrt(3^2 + 3^2 + 0^2)sqrt((2)^2 + (1)^2 + (1)^2))
= 9/sqrt(18)sqrt(6)
= 9/6sqrt(3)
= sqrt(3)/2
θ = arccos(sqrt(3)/2) = pi/6

example:
a1 = [0,1,-1], a2 = [1,2,2], x = [1,1,-1]
find acute angle between x and the subspace S given the set {a1,a2} is orthogonal.
proj_a1 x = 1[0,1,-1]
proj_a2 x = 1/9[1,2,2] = [1/9, 2/9, 2/9]
[0,1,-1] + [1/9, 2/9, 2/9] = 1/9[1, 11, -7]
x = [1,1,-1] 9proj_S x = [1,11,-7]
= 19/sqrt(3)sqrt(1^2 + 11^2 + (-7)^2)
= 19/sqrt(3)sqrt(171)
= 19/sqrt(3)⋅3sqrt(19)
= sqrt(57)/9
θ = arccos(sqrt(57)/9) = 32.98 degrees

justification for the formula:

Orthogonal projection of a vector x onto the subspaces Span{a1} and Span{a2} given that a1 ⊥ a2
write x as a sum of two vectors:
x = x_S + x_S⊥

where x_S = proj_S x is in Span{a1,a2}, and x_S⊥ is in the orthogonal complement of Span{a1,a2}. That is, x_S = k1a1 + k2a2 ∈ S and x_S⊥ ∈ S^⊥

x_S⊥ = x - x_S
= x - (k1a1 + k2a2)
= x - k1a1 - k2a2

Using the fact that x_S⊥ is orthogonal to S, and that a1⋅a2 = 0
{
	x_S⊥ ⋅ a1 = 0
	x_S⊥ ⋅ a2 = 0
} = {
	(x - k1a1 - k2a2)⋅a1 = 0
	(x - k1a1 - k2a2)⋅a2 = 0
} = {
	x⋅a1 - k1(a1⋅a1) - k2(a2⋅a1) = 0
	x⋅a2 - k1(a1⋅a2) - k2(a2⋅a2) = 0
} = {
	k1(a1⋅a1) = x⋅a1
	k2(a2⋅a2) = x⋅a2
} = {
	k1 = (x⋅a1)/(a1⋅a1)
	k2 = (x⋅a2)/(a2⋅a2)
}
proj_S x = k1a1 + k2a2
= (x⋅a1)/(a1⋅a1)a1 + (x⋅a2)/(a2⋅a2)a2
= proj_a1 x + proj_a2 x

===================================================
(careful a lot of calculations in this part)
(its easy to make a mistake, I am not a machine yet)

projection vectors onto subspaces in euclidean spaces (arbitrary bases)

same formula for othogonal bases
A^TAk = A^Tx  =>  k = (A^TA)^-1A^Tx 
proj_S x = Ak
= A(A^TA)^-1A^Tx

These vectos are not orthogonal
a1 = [1,2,1], a2 = [1,3,1], x = [2,4,0]
proj_Sx = [
	1 1
	2 3
	1 1
]⋅(
	[
		1 2 1
		1 3 1
	]⋅[
		1 1
		1 3
		1 1
	]
)^-1 ⋅ [
	1 2 1
	1 3 1
]⋅[
	2
	4
	0
]
= [
	1 1
	2 3
	1 1
]⋅([
	6 8
	8 11
])^-1 ⋅ [
	10
	14
]
= [
	1 1
	2 3
	1 1
]⋅1/2[
	11 -8
	-8 6
]⋅[
	10
	14
]
= [
	1 1
	2 3
	1 1
]⋅[
	-1
	2
]
= [1, 4, 1]

Do the operations in the same order as above, we will save ourselves a lot of work.

if {a1,a2,...,an} is a set of linearly independent vectors, then the orthogonal projection of a vector x onto the subspace S = span{a1,a2,...,an} is:
proj_S x = A(A^TA)^-1A^Tx
where A is the matrix whose columsn are the vectors a1,a2,...,an
[
	a1 a2 ... an
]
{a1,a2,...,an} must be linearly independent is important. If the vectors are linearly dependent, we need to ifnd the basis of the span before applying the formula.

find the orthogonal projection of the vector x onto the plane r = sv + tw where s,t ∈ R
v = [1,2,1], w = [2,1,-1], x = [6,3,6]
poj_S x = [
	1  2
	2  1
	1 -1
]
⋅
(
	[
		1 2  1
		2 1 -1
	] ⋅ [
		1  2
		2  1
		1 -1
	]
)^-1
⋅
[
	1 2  1
	2 1 -1
]
⋅ [6,3,6]
=
[
	1  2
	2  1
	1 -1
]
⋅
([
	6 4 
	3 6
])^-1
⋅
[
	18
	9
]
=
[
	1  2
	2  1
	1 -1
]
⋅
1/24[
	6 -4
	-3 6
]
⋅
[
	18
	9
]
=
[
	1  2
	2  1
	1 -1
] ⋅ [3, 0]
[
	3
	6
	3
]

find the orthogonal projection of the vector x onto the plane r = sv + tw where s,t ∈ R
example:
v = [1,1,1], w = [3,1,3], x = [3,4,5]
poj_S x = [
	1 3
	1 1
	1 3
]
⋅
(
	[
		1 1 1
		3 1 3
	] ⋅ [
		1 3
		1 1
		1 3
	]
)^-1
⋅
[
	1 1 1
	3 1 3
]
⋅ [3,4,5]
=
[
	1 3
	1 1
	1 3
]
⋅
[
	3  6
	7 19
]^-1
⋅
[
	12
	28
]
=
[
	1 3
	1 1
	1 3
]
⋅
1/15
[
	19 -6
	-7  3
]
⋅
[
	12
	28
]
=
[
	1 3
	1 1
	1 3
]
⋅
[
	4
	0
]
=
[
	4
	4
	4
]

finding the orthogonal projection of a vector onto a subspace (same process as a plane)
find the vector in the subspace spanned by {a1,a2} that is closest to the vector x
a1 = [1,1,0], a2 = [2,2,-2], x = [8,-4,8]
poj_S x = [
	1  2
	1  2
	0 -2
]
⋅
(
	[
		1 1  0
		2 2 -2
	] ⋅ [
		1  2
		1  2
		0 -2
	]
)^-1
⋅
[
	1 1  0
	2 2 -2
]
⋅ [8,-4,8]
=
[
	1  2
	1  2
	0 -2
]
⋅
[
	2 4
	4 12
]^-1
⋅
[
	4
	-8
]
=
[
	1  2
	1  2
	0 -2
]
⋅
1/4
[
	6 -2
	-2 1
]
⋅
[
	4
	-8
]
=
[
	1  2
	1  2
	0 -2
]
⋅
[
	10
	-4
]
=
[
	2
	2
	8
]

verify whether the vectors a1,a2,a3 are linearly independent, and find the orthogonal projection of x onto the subspace spanned by the vectors {a1,a2,a3}
a1 = [2,-2,-2], a2 = [2,2,2], a3 = [4,0,0]
x = [3,2,2]
[
	 2 2 4
	-2 2 0
	-2 2 0
]
R2 := R2 + R1
R3 := R3 + R1
[
	 2 2 4
	 0 4 0
	 0 4 0
]
R3 := R3 + (-1)R2
[
	 2 2 4
	 0 4 0
	 0 0 0
]
we can confirm from REF that the vectors a1 and a2 are linearly independent and form the basis of our span so we carry on our normal calculations:
poj_S x = [
	 2 2
	-2 2
	-2 2
]
⋅
(
	[
		2 -2 -2
		2  2  2
	]⋅[
		 2 2
		-2 2
		-2 2
	]
)^-1
⋅
[
	2 -2 -2
	2  2  2
]⋅[3,2,2]
=
[
	 2 2
	-2 2
	-2 2
]⋅[
	12 -4
	-4 12
]^-1 
⋅
[
	-2
	14
]
=
[
	 2 2
	-2 2
	-2 2
]
⋅ 1/32 [
	3 1
	1 3
]⋅[
	-2
	14
]
=
[
	 2 2
	-2 2
	-2 2
]⋅1/4[
	1
	5
]
= [3,2,2]

Notice: poj_S x = x. This means that x lies in the span.
(x ∈ Span{a1,a2} = Span{a1,a2,a3})

proj_a x = (a⋅x)/(a⋅a)a
= [(a^Tx)/(a^Ta)]a
= [(a^Ta)^-1a^Tx]a
= a(a^Ta)^-1a^Tx

lets compare this formula when we want to project onto an arbitrary subspace:
proj_s x = A(A^TA)^-1A^Tx

They are basically the same.

===================================================

Calculating the distance between a vector and a subspace
Find the distance between the vector x and the subspace spanned by the linearly independent vectors {a1,a2}
example:
a1 = [2,0,1], a2 = [0,-1,1], x = [3,-3,0]
proj_S x = [
	2  0
	0 -1
	1  1
]⋅(
	[
		2  0 1
		0 -1 1
	]⋅[
		2  0
		0 -1
		1  1
	]
)^-1
⋅
[
	2  0 1
	0 -1 1
]⋅[
	 3
	-3
	 0
]
=
[
	2  0
	0 -1
	1  1
]
1/9[
	 2 -1
	-1  5
]⋅[
	6
	3
]
=
[
	2  0
	0 -1
	1  1
]⋅ [
	1
	1
]
=
[
	2
	-1
	2
]
||x - ^x|| <-- (x hat)
sqrt((2 - 3)^2 + (-1 -(-3)^2 + (2 - 0)^2))
= sqrt(1 + 4 + 4) = sqrt(9) = 3

example:
a1 = [1,1,1,1], a2 = [-1,1,1,1]
x = [-6,3,0,3]
proj_S x = [
	1 -1
	1  1
	1  1
	1  1
]⋅(
	[
		 1 1 1 1
		-1 1 1 1
	]⋅[
		1 -1
		1  1
		1  1
		1  1
	]
)^-1
⋅[
	 1 1 1 1
	-1 1 1 1
]⋅[
	-6
	3
	0
	3
]
=
[
	1 -1
	1  1
	1  1
	1  1
]⋅ 1/12[
	4 -2
	-2 4
]⋅ [
	0
	12
]
=
[
	1 -1
	1  1
	1  1
	1  1
]
[
	-2
	4
]
=
[
	-6
	2
	2
	2
]
||x - ^x|| <-- (x hat)
sqrt((-6 -(-6))^2 + (3 - 2)^2 + (0 - 2)^2 + (3 - 2)^2)
= sqrt(1 + 4 + 1) = sqrt(6)

Calculating the acute angle between a vector and subspace
example:
subspace is spanned by the linearly independent vectors {a1,a2}
a1 = [1,1,1], a2 = [1,1,-1], x = [2,-1,1]
proj_S x = [
	1  1
	1  1
	1 -1
]⋅(
	[
		1 1  1
		1 1 -1
	]⋅[
		1  1
		1  1
		1 -1
	]
)^-1 ⋅ [
	1 1  1
	1 1 -1
]⋅[
	 2
	-1
	 1
]
=
[
	1  1
	1  1
	1 -1
]⋅ 1/8 [
	3 -1
	-1 3
]⋅ [
	2
	0
]
=
[
	1  1
	1  1
	1 -1
]⋅1/4[3, -1]
= [
	1
	1
	2
]
cosθ = 3/sqrt(6)sqrt(6)
= 1/2
θ = arccos(1/2) = pi/3

example:
find angle in degrees
a1 = [3,-4,0], a2 = [4,3,0], x = [7,-1,5]
proj_S x = [
	 3 4
	-4 3
	 0 0
]⋅(
	[
		3 -4 0
		4  3 0
	]⋅[
		 3 4
		-4 3
		 0 0
	]
)^-1 ⋅ [
	3 -4 0
	4  3 0
]⋅[
	 7
	-1
	 5
]
=
[
	 3 4
	-4 3
	 0 0
]⋅1/25[
	1 0
	0 1
]⋅[
	25
	25
]
=
[
	 3 4
	-4 3
	 0 0
]⋅[
	1
	1
]
= [
	 7
	-1
	 0
]
cosθ = x⋅proj_Sx/||x||⋅||proj_Sx||
= 50/sqrt(75)sqrt(50)
= 25*2/sqrt(25*3)sqrt(25*2)
= 2/sqrt(3)sqrt(2)
= sqrt(2)/sqrt(3)
θ = arccos(sqrt(2)/sqrt(3)) = 35.26 degrees

Finding the orthogonal projection of a vector onto the solution space of a linear system.
Given the vector x and the system of linear equations, find the projection of x onto the solution space of the system.
(Already calculated the dot product of the inverse for these already to save space)
x = [0,5,10],
{
	x1 - 3x3 = 0
	3x1 - 9x3 = 0
}
[
	1 0 -3 | 0
	3 0 -9 | 0
]
R2 = R2 + (-3)R1
[
	1 0 -3 | 0
	0 0  0 | 0
]
x1 = -3x3
[
	3x3
	x2
	x3
]
x2 = [
	0
	1
	0
]
= x3[
	3
	0
	1
]
proj_S x = [
	0 3
	1 0
	0 1
]
1/10
[
	10 0
	0 1
]⋅[
	 0 1 0
	 3 0 1
]⋅[
	0
	5
	10
]
=
[
	0 3
	1 0
	0 1
]
1/10
[
	10 0
	0 1
]⋅ [
	5
	10
]
=
[
	0 3
	1 0
	0 1
]⋅[
	5
	1
]
= [
	3
	5
	1
]

find the projection of x onto the solution space of the system:
x = [0,0,14]
{
	x1 - 3x2 + 2x3 = 0
	3x1 - 9x2 + 6x3 = 0
}
[
	1 -3 2 | 0
	3 -9 6 | 0
]
R2 := R2 + (-3)R1
[
	1 -3 2 | 0
	0  0 0 | 0
]
x1 = 3x2 - 2x3
[
	3x2 - 2x3
		x2
		x3
]
x2[
	3
	1
	0
]
x3[
	-2
	0
	1
]
proj_S x = [
	3 -2
	1  0
	0  1
]
1/14
[
	5 6
	6 10
]⋅[
	  3 1 0
	 -2 0 1
]⋅[
	0
	0
	14
]
=
[
	3 -2
	1  0
	0  1
]
1/14
[
	5 6
	6 10
]⋅[
	0
	14
]⋅[
	6
	10
]
=
[
 -2
  6
  10
]

===================================================

Review:
The Gram-Schmidt process is an algorithm that we can use to construct an orthogonal basis if we are given a non-orthogonal basis to start with

Basis {a1,a2} for subspace V = Span{a1,a2}
a1 = [1,-1], a2 = [4,6]

Step 1: set v1 = a1
Step 2: Find a vector v2 in V that's orthogonal to v1:
v2 = a2 - proj_v1 a2
= a2 - (a2⋅v1)/(v1⋅v1)v1
v2 = -((4 * 1) + (6 * (-1))/(1^2 + (-1)^2))
= [4,6] - (-1) ⋅ [1,-1]
= [5,5]
{v1,v2} = {[1,-1],[5,5]} is orthogonal basis for V

subspace V and an orthogonal basis B of V. B is derived form V using Gram-Schmidt process, what is the value of a/c
example:
V = Span{[2,-3,1],[4,-2,0]}
B = {[2,-3,1],[a,b,c]}
2 * 4 + (-3)(-2) + (1)(0)/2^2 + (-3)^2 + 1^2
= 14/14 = 1/1 = 1
= [4,-2,0] - 1/1[2,-3,1]
= [2,1,-1]
a = 2, c = -1, a/c = -2

example:
V = Span{[1,5,1],[-2,-5,0]}
B = {[1,5,1]}
B is derived form V using Gram-Schmidt process
what is the value of a/c
-2 + -25 + 1/27 = -27/27 = -1/1
[-2,-5,0] + [1,5,1]
= [-1, 0, 1]
a = -1, c = 1, a/c = -1

example:
Subspace Col(A) and an orthogonal basis B of Col(A), A and B are given above given that B is derived from Col(A) using the Gram-Schmidt process, what is the value of b/a
A = [
	 3 2
	-1 1
	 0 2
]
B = {[3,-1,0],[a,b,c]}
5/10 = 1/2
[2,1,2] - 1/2[3,-1,0]
= [1/2, 3/2, 2]
"clean up" (we can scale the factors by 2 since they will be on the same span anyways)
= [2, 2, 4]

example:
what is the value of c/a
A = [
	4  5
	4  5
	2 -2
]
B = {[4,4,2],[a,b,c]}
36/36 = 1/1 = 1
[5,5,-2] - [4,4,2]
= [1,1,-4]
a = 1, c = -4, c/a = -4

Using the Gram-Schmidt Process to find an orthonormal basis
given a basis {a1,a2} for a subspace V, we can use the Gram-Schmidt process to find an orthonormal basis of the subspace.

Calculate the Gram-Schmidt as before to obtain an orthogonal basis {v1,v2} then normalize each vector in this basis to get the orthonormal basis {u1,u2}
u1 = v1/||v1||, u2 = v2/||v2||

subspace V and on orthormal basis B of V. B is derived form V using the Gram-Schmidt process, what is the value of |a|+|b|+|c|
V = Span{[1,1,-1],[1,2,-3]}
B = {[sqrt(3)/3, sqrt(3)/3, -sqrt(3)/3],[a,b,c]}
6/3 = 2
[1,2,-3] - 2[1,1,-1]
= [-1,0,-1]
u1 = v1/||v1|| = 1/sqrt(1^2 + 1^1 + (-1)^2)
= 1/sqrt(3)[1,1,-1] = [1/sqrt(3),1/sqrt(3),-1/sqrt(3)]
u2 = v2/||v2|| = 1/sqrt((-1)^2 + 0^2 + (-2)^2)
= 1/sqrt(2)[-1,0,-1] = [-sqrt(2)/2,0,-sqrt(2)/2]
a = -sqrt(2)/2, b = 0, c = sqrt(2)/2
|a|+|b|+|c| = |-sqrt(2)/2|+|0|+|sqrt(2)/2| = sqrt(2)

example:
subspace Col(A) and an orthonormal basis V of Col(A). B is derived from Col(A) using the Gram-Schmidt process whats |a|+|b|+|c|
A = [
	-1  3
	-2  0
	 2 -3
]
B = {u1,u2} = {[-1/3,-2/3,2/3]}
-9/9 = -1/1 = -1
[3,0,-3] + [-1,-2,2]
= [2, -2, -1]

u1 = v1/||v1|| = 1/sqrt(9) = 1/3[-1,-2,2]
= [-1/3, -2/3, 2/3]
u2 = v2/||v2|| = 1/sqrt(9) = 1/3[2,-2,-1]
= [2/3,-2/3,-1/3]
|a|+|b|+|c| = |2/3|+|-2/3|+|-1/3| = 5/3

===================================================

Review:
(no commas inbetween entries as its a row vector)
Linear form f(x) = ax where a = [0 -2 3]. Evaluate the function when x = [1, 3, 5]
[0 -2 3]⋅[1, 3, 5]
0⋅1 + (-2)⋅3 + 3⋅5 = 9

example:
a = [1 -3], x = [-1, 2]
= -7

Bilinear forms:
B(x,y) = x^TAy
example:
A = [
	1 2
	3 4
]
x = [1, 2] (column vector)
y = [-1, 1] (column vector)
B(x,y) = x^TAy
= [1 2][
	1 2
	3 4
][-1,1]
= [7 10][-1,1]
= 3

example:
B(x,y) = x1y1 - 3x1y3 - x2y3 + x3y3
x = [1,0,-4], y = [2,-1,1]
= -5

example:
B(x,y) = x^TAy x = [2, -3], y = [1, 1]
A = [
	-2 4
	 4 1
]
=
[2 -3][
	-2 4
	 4 1
]
[-16 5][1, 1]
= -11

writing down a bilinear form from a matrix
A = [
	2 -1
	3 -2
]
x^TAy components x = [x1,x2], y = [y1,y2]

method1: each entry a_ij of A corresponds to the term a_ijx_iy_j in the expression of the bilinear form:
x^TAy = 2x1y1 - x1y2 + 3x2y1 - 2x2y2

method2: bilinear form in terms of vectors:
x^TAy = [x1 x2][
	2 -1
	3 -2
][y1,y2]
= [x1 x2][
	2y1 - y2
	3y1 - 2y2
]
= x1(2y1 - y2) + x2(3y1 - 2y2)
= 2x1y1 - x1y2 + 3x2y1 - 2x2y2

B(x,y) = x^TAy, x,y ∈ R^n
is linear in each argument separately. Thats the reason why we call them bilinear.

For y fixed, all u, v ∈ R^n, and any real constant c, linearity in the first argument means the following:
B(u + v,y) = B(u,y) + B(v,y)
B(cu,y) = cB(u,y)

For x fixed all w,z ∈ R^n, and any real constant c linearity in the second argument means the following:
B(x,w + z) = B(x,w) + B(x,z)
B(x, cw) = cB(x,w)

In matrix notion a quadratic form can be given as
Q(x) = x^TAx
The matrix of a quadratic form must be symmetric
Q(x) = B(x,x)

Q(x1,x2) = [x1 x2][
	1 2
	2 1
][x1, x2]
= [x1 x2][
	x1 + 2x2
	2x1 + x2
]
= x1^2 + 4x1x2 + x2^2
In quadratic form all terms must be degree two
!IMPORTANT SYMMETRIC

example:
A = [
	-2 4
	 4 1 
]
x = [2,3]
=
[2 3][
	-2 4
	 4 1
][2,3]
= [8 11][2,3]

example:
express the quadratic form Q(x) = x1^2 - 3x1x2 + x2^2 in terms of the componets y
[x1,x2] = [
	1 -2
	0  1
][y1,y2]
x1 = y1 - 2y2
x2 = y2
substitute
= (y1 - 2y2)^2 - 3(y1 - 2y2)y2 + y2^2
= y1^2 - 4y1y2 + 4y2^2 - 3y1y2 + 6y2^2 + y2^2
= y1^2 - 7y1y2 + 11y2^2

example:
Q(x) = x1^2 + 4x1x2
[x1,x2]=[
	2 -1
	1  1
][y1,y2]
x1 = 2y1 - y2
x2 = y1 + y2
= (2y1 - y2)^2 + 4(2y1 - y2)(y1 + y2)
= 4y1^2 - 4y1y2 + y2^2 + 8y1^2 + 4y1y2 - 4y2^2
= 12y1^2 - 3y2^2

example:
express Q(x) = x1^2 + 3x1x2 + x2^2 in terms of y
[y1,y2]=[
	1  2
	0 -1
][x1,x2]

y1 = x1 + 2x2
y2 = -x2

solving the second equation x2 and substituting it into the first equation:
x1 = y1 + 2y2(I got -2y2, filling in what the answer gave me)
x2 = -y2

(y1 + 2y2)^2 + 3(y1 + 2y2)(-y2) + (-y2)^2
y1^2 + 4y1y2 + 4y^2 - 3y1y2 - 6y2^2 + y2^2
= y1^2 + y1y2 - y2^2
 
example
express Q(x) = x1^2 + x1x2 - 2x2^2 in terms of y
[y1,y2]=[
	 0 1
	-1 0
][x1,x2]

y1 = x2
y2 = -x1
solving for x1, x2:
x1 = -y2
x2 = y1

(-y2)^2 + (-y2)(y1) - 2(y1)^2
= y2^2 - y2y1 - 2y1^2
= -2y1^2 - y2y1 + y2^2

we are given a quadratic form:
Q(x) = x^TAx

and an invertible matrix P that defines the change of variable
x = Py

If this change of variable is applied to the quadratic form Q(x), what wil be the new matrix of Q

By making the necessary substitutions
Q(x) = x^TAx
= (Py)^TA(Py)
= y^TP^TAPy
= y^T(P^TAP)y
the new matrix of our quadratic form is:
B = P^TAP

Find the matrix of the quadratic form x^TAx in the basis that consists of the columns of P
P^TAP
A = [
	-2 1 
	 1 5
]
P = [
	1 -1
	1  2
]
=
[
	 1 1
	-1 2
]⋅[
	-2 1
	 1 5
]⋅[
	1 -1
	1  2
]
=
[
  -1 6
	4 9
]⋅[
	1 -1
	1  2
]
=
[
	5  13
	13 14
]

Find the matrix of the quadratic form x^TAx in the basis that consists of the columns of P
P^TAP
A = [
	-1  2
	 2 -1
]
P = [
	2 -2
	1  1
]
=
[
	 2 1
	-2 1
]⋅[
	-1  2
	 2 -1
]⋅[
	2 -2
	1  1
]
=
[
	0 3
	4 -5
]⋅[
	2 -2
	1  1
]
=
[
	3 3
	3 -13
]

Two quadradic forms Q1 and Q2 on R^n are said to be equivalent if they can be obtained from each other through a change of variable

theorem:
The two quadratic forms with matricies A and B are equivalent iff there exists a matrix P such that B = P^TAP

Equivalent quadratic forms represent the same function from R^n to R

===================================================

Sometimes its important to know if a particular quadratic Q(x) is always postive or negative.

Q(x) is positive-definite if Q(x) > 0 for all ∈ R^n
for example: S(x) = 8x1^2 + 3x2^2 > 0

Q(x) is negative-definite if Q(x) < 0 for all ∈ R^n
example: T(x) = -x1^2 - 3x2^2 < 0

A quadratic form Q(x) is indefinite if
Q(x1) > 0 for some x1
Q(x2) < 0 for some x2
example: R(x) = x1^2 + 2x^2 - 6x1x2
is indefinite, substituting x1 [1,0] we get
= 1^2 + 2*0^2 - 6*1*0
= 1 + 0 + 0
R(x1) = 1 > 0
substituting x2 [1,1]
= 1^2 + 2*1^2 - 6*1*1
= 1 + 2 - 6
= -3 < 0

example:
x = [1,-2]
determine which of these cannot be positive-definite
R(x) = x1^2 + x2^2 + 5x1x2
S(x) = x^T[
	 1 -2
	-2  9
]x
T(x) = x1^2 + x2^2 + x1x2
substitute
R(x) = 1^2 + (-2)^2 + 5*1*-2
= -5 < 0
S(x) = [1 -2][
	 1 -2
	-2  9
][1,-2]
= [5 -20][1,-2]
= 45 > 0
T(x) = 1^2 + (-2)^2 + 1*(-2)
= 1 + 4 - 2
= 3 > 0
R(x) only

A quadratic form Q(x) is positive semi-definite if Q(x) >= 0
A(x) = x1^2 this is not positive definite since it gives 0 on the nonzero vector: [0,1]

A quadratic form Q(x) is negative semi-definite if Q(x) <= 0
B(x) = -x1^2 this gives 0 on the nonzero vector [0,1]

any postive-definite quadratic form is positive semi-definite
any negative-definite quadratic form is negative semi-definite

Using eigenvalues to classify quadratic forms
Q(x) = x^TAx we have the following theorem:
Q is positive-definite iff all the eigenvalues of A are positive.
Q is positive semi-definite iff all the eigenvalues of A are non-negative
Q is negative-definite iff all the eigenvalues of A are negative
Q is negative semi-definite iff all the eigenvalues of A are non-positive
Q is indefinite iff A has both positive and negative eigenvalues


example:
find what theorem defines the characteristic
A = [
	9  4
	4 -6
]
|9-λ  4|
|4 -6-λ|
(9 - λ)(-6 - λ) - (4)(4)
-54 - 9λ + 6λ + 1λ^2 - 16
λ^2 - 3λ - 70
(λ - 10)(λ + 7)
λ = 10, λ = -7
(indefinite both positive and negative)

The principal minors of an nxn matrix are the n determinants formed by the first r row and first r columns of A for r = 1,2,3,...n. The principal of A are denoted ∆_i

The first principal minor ∆_1 equals a_11
The nth principal minor ∆_n equals det(A)

example of principal minors
A = [
	1 0  6
	2 3 -4
	6 9  8
]
A is 3x3 it has 3 principal minors
The first principal minor is the determinant formed by the first row and first column of A:
M_1 = |a_11| = |1| = 1
The second principal minor is determinant formed by the first 2 rows and first 2 columns of A:
M_2 = |1 0|
		|2 3| = 3
The third principal minor is the determinant formed by the first 3 rows and first 3 columns of A:
M_3 = |1 0  6|
      |2 3 -4| = 60
      |6 9  8|
60 comes from the det
(det(A) = 1⋅(3⋅8−(−4)⋅9)−0⋅(2⋅8−(−4)⋅6)+6⋅(2⋅9−3⋅6))

Let ∆_i for i = 1,2,...n denote principal minors fo an nxn matrix A
According Sylvester's criterion the quadratic form x^TAx is
positive-definite iff ∆_i > 0 for all i = 1,2,...n
negative-definite iff for all i = 1,2,...,n we have ∆_i < 0 when i is odd, and ∆_i > 0 when i is even.

If A is negative-definite, then the principal minors alternate in sign, with the first principal minor being negative.

For example:
A = [
	2 1 0
	1 1 0
	0 0 5
]
principal minors:
∆_1 = a11 = 2 > 0
∆_2 = |2 1|
      |1 1| = 1 > 0
∆_3 = |A|
 |2 1 0|
=|1 1 0|
 |0 0 5|
5⋅|2 1|
  |1 1|
= 5 > 0
all principal minors are positive. By Sylvester's criterion, the quadratic form x^TAx is positive-definite.

example:
Sylvester Criterions
A = [
	9 6
	6 6
]
a11 = 9 and det(A) = 18
which makes this positive-definite

example:
Sylvester Criterions
A = [
	-1  2  0
	 2 -6  0
	 0  0 -2
]
a11 = -1
det(A) = -1(12) - 2(-4) + 0((2)(0)(-6)(0))
-12 + 8 = -4

∆_1 < 0
∆_2 = |-1 2|
      |2 -6| = 2 > 0
∆_3 = |A| -4 < 0

∆_1 (odd) < 0, ∆_2 (even) > 0, ∆_3 (odd) < 0
therefore by Sylvester's criterion the form x^TAx
and the matrix A are negative-definite

===================================================

theorem:
Let A be the matrix corresponding to our quadratic form Q(x)
The maximum value of Q(x), when ||x|| = 1 is equal to the largest eigenvalue of A:
max{Q(x)} | ||x|| = 1} = λ_max
The max is attained on the corresponding unit eigenvector u_max
The minimum value of Q(x) when ||x|| = 1 is equal to the smallest eigenvalue of A:
min{Q(x) | ||x|| = 1} = λ_min
The minimum is attained on the corresponding unit eigenvector u_min

Suppose we are given the quadratic form:
Q(x) = 4x1^2 - 2x1x2 + 4x2^2
How can we find a unit vector u_max at which the maximum of Q(x) is attained?
First we write down the matrix of our quadratic form
A = [
	 4 -1
	-1  4
]
next calculate the eigenvalues:
|A-λI| = 0
|4-λ  1|
| 1 4-λ| = 0
(4 - λ)(4 - λ) - 1 = 0
λ^2 - 8λ + 15 = 0
(λ - 5)(λ - 3) = 0
λ = 5,3
therefore we have
max{Q(x) | ||x|| = 1} = λ_max = 5

To find u_max we need a unit eigenvector that corresponds to the eigenvalue λ_max = 5.
A - 5I = [
	 4 -1
	-1  4
] - 5[
	1 0
	0 1
] = [
	-1 -1
	-1 -1
]
gives us the eigenvector v = [1,-1]
divide v by its norm ||v|| = sqrt(2)
u_max = v/||v|| = 1/sqrt(2)[1, -1]

Find minimum value of Q(x) = x1^2 - 2x2^2 - 2x3^2 + 2x2x3
given that ||x|| = 1
A = [
	1  0  0
	0 -2  1
	0  1 -2
]
|A-λI| = 0
|1-λ  0   0|
|0  -2-λ  1| = 0
|0   1 -2-λ|
(1-λ)|-2-λ  1|
     | 1 -2-λ| = 0
(1 - λ)((-2 - λ)(-2 - λ) - 1) = 0
(1 - λ)(λ^2 + 4λ + 3) = 0
(1 - λ)(λ + 3)(λ + 1) = 0
λ = 1,-1,3
min{Q(x) | ||x|| = 1} = -3

example:
find minimum value of
Q(x) = 7x1^2 + 8x2^2 + 5x3^2 - 4x2x3 given ||x|| = 1
A = [
  7-λ  0  0
  0   8-λ -2
  0   -2  5-λ
]
(7-λ)|8-λ   2|
     |-2  5-λ| = 0
(7 - λ)((8 - λ)(5 - λ) - 4)
(7 - λ)(λ^2 - 13λ + 36)
(7 - λ)(λ - 9)(λ - 4)
λ = 7, 9, 4

Finding where the extrema of a quadratic form is attained on the unit circle
example we are given
Q(x) = x1^2 - 10x1x2 + x2^2
when ||x|| = 1, lets find a unit vector u_max
A = [
	 1 -5
	-5  1
]
Find the unit eigenvector that corresponds to the eigenvalue λ_max = 6
A - 6I = [
	 1 -5
	-5  1
] - 6[
	1 0
	0 1
] = [
	-5 -5
	-5 -5
]
(A-5I)x = 0 gives the eigen vector v = [-1,1]
dividing v by norm ||v|| = sqrt(2)
u_max = v/||v|| = 1/sqrt(2)[-1,1]
Now we can check this is the vector we are looking for
Q(u_max) = (-1/sqrt(2)^2) -10(-1/sqrt(2))(1/sqrt(2)) + (1/sqrt(2))^2
= 1/2 + 10/2 + 1/2
= 6

example:
The max value of Q(x) = x1^2 - 4x1x2 + 4x2^2 when ||x|| = 1 is equal to 5. What is the vector when max is attained?
A = [
	 1 -2
	-2  4
]
A - 5I = [
	 1 -2
	-2  4
] - 5[
	1 0
	0 1
] = [
	-4 -2
	-2 -1
]
(-4 - λ)(-1 - λ) - (-2)(-2)
4 + 4λ + λ + λ^2 - 4
λ^2 + 5λ
λ(λ + 5)
eigenvalues
λ = 0, -5 
eigenvector
{
	-4x1 - 2x2 = 0
	-2x1 - x2 = 0
}
first equation we can express x1 in terms of x2
-4x1 = 2x2  =>  x1 = -2/4x2  =>  x1 = -1/2x2
x1 = -1/2t, x2 = t
eigenvector λ = 0
[-1/2, t] setting t = 2 we get
[-1, 2]
u_max = v/||v|| = 1/sqrt(5)[-1, 2]

example:
the min value Q(x) = 3x1^2 - 2x1x2 + 3x2^2 when ||x|| = 1, is equal to 2. find the vector x when min is attained
A = [
	 3 -1
	-1  3
]
A-2I = [
	 3 -1
	-1  3
] - 2[
	1 0
	0 1
] = [
	 1 -1
	-1  1
]
(A - 2I)x = 0
eigenvector v = [1, 1]
||v|| = sqrt(2)
u_min = v/||v|| = 1/sqrt(2)[1,1]

Finding extrema of quadratic form is attained on unit SPHERE for a 3x3 matrix
the max value of quadratic form Q(x) = -x1^2 + 2x2^2 + 6x3^2 + 4x1x2, when ||x|| = 1, is equal to 6. find vector x where max is attained.
example:
A = [
	-1 2 0
	 2 2 0
	 0 0 6
]
A - 6I = [
	-1 2 0
	 2 2 0
	 0 0 6
] - 6[
	1 0 0
	0 1 0
	0 0 1
] = [
	-7  2 0
	 2 -4 0
	 0  0 0
]
{
	-7x1 + 2x2 = 0
	2x1 - 4x2 = 0
	0 = 0
}
2x2 = 7x1
x1 = 7/2
setting x1 to 0 to simplify
x2 = 0
x3 = 1
(A - 6I)x = 0 eigenvector v = [0,0,1]
since v is already normalized ||v|| = 1
u_max = [0,0,1]

example:
the max value of the quadratic form Q(x) = -x1^2 + 2x2^2 + x3^2 + 4x1x2 when ||x|| = 1, is equal to 3. What is the max vector x?
A = [
	-1 2 0
	 2 2 0
	 0 0 1
] 
A - 3I = [
	-1 2 0
	 2 2 0
	 0 0 1
] - 3[
	1 0 0
	0 1 0
	0 0 1
] = [
	-4  2  0
	 2 -1  0
	 0  0 -2
]
{
	-4x1 + 2x2 = 0
	2x1 - x2 = 0
	-2x3 = 0
}
x1 = 1/2x2
2(1/2x2) - x2
x2 - x2 = 0
x2 = x2
x3 = 0
setting x2 to 2 we get [1, 2, 0]
u_max = v/||v|| = 1/sqrt(5)[1,2,0]

example:
min value Q(x) = -x1^2 - x2^2 + 2x3^2 + 2x1x2 ||x|| = 1, is equal to -2. What is the vector x at the minimum is attained?
A = [
	-1  1 0
	 1 -1 0
	 0  0 2
]
A - 2I = [
	-1  1 0
	 1 -1 0
	 0  0 2
] + 2 [
	1 0 0
	0 1 0
	0 0 1
] = [
	1 1 0
	1 1 0
	0 0 4
]
{
	x1 + x2 = 0
	x1 + 3x2 = 0
	4x3 = 0
}
x1 = -x2
setting x2 to 1: [-1,1,0]
u_min = v/||v|| = 1/sqrt(2)[-1,1,0]

===================================================

The general form of an ellipse centered at the origin is:
x1^2/a^2 + x2^2/b^2 = 1
where a and b are the lengths of the semi-major and semi-minor axes.

---------------------------------------------------------
Constrained Optimization of Quadratic Forms Over Ellipses
---------------------------------------------------------
START:

Q(x) = x1^2 - 2x1x2 + x2^2
over the ellipse 4x1^2 + x2^2 = 4 equals 5. How can we find a vector at which max is attained?

By applying suitable change of variable. This change of variable should transform our problem into one where we need to maximize a related quadratic form Q' on the unit circle

First, we write down the matrix of our quadratic form:
A = [
	 1 -1
	-1  1
]
write the constraint equation:
x1^2 + x2^2/2^2 = 1
which represents an ellipse with semiaxes a = 1 (along the x1-axis) and b = 2 (along the x2-axis)
if we introduce new variables:
y1 = x1, y2 = x2/2
the equation of the ellipse will transform into y1^2 + y2^2 = 1 or ||y|| = 1
so, introducing a change-of-coordinate matrix
P = [
	1 0
	0 2
]
we obtain that x = Py. substituting this into the expression of the quadratic form, we get
Q(x) = x^TAx = (Py)^TA(Py) = y^T(P^TAP)y = Q'(y)
since P^T = P the matrix of our quadratic form in the new coordinates is
B = P^TAP
= PAP
= [
	1 0
	0 2
][
	 1 -1
	-1  1
][
	1 0
	0 2
]
= [
	 1 -1
	-2  2
][
	1 0
	0 2
]
=
[
	 1 -2
	-2  4
]
Now to maximize Q'(y) when ||y|| = 1
since we are told the max value of of the quadratic form is 5:
max{Q'(y) | ||y|| = 1} = λ_max = 5
next we find the unit eigenvector that corresponds to the eigenvalue λ_max = 5
B - 5I = [
	 1 -2
	-2  4
] - 5[
	1 0
	0 1
] = [
	-4 -2
	-2 -1
]
= eigenvector v = [-1, 2], dividing v by its norm ||v|| = sqrt(5)
u_max = v/||v|| = 1/sqrt(5)[-1, 2]
finally, we map this vector back to the initial coordinate system using the change-of-coordinates matrix:
x_max = Pu_max
= [
	1 0
	0 2
]⋅ 1/sqrt(5)[-1, 2]
= 1/sqrt(5)[-1, 4]

FINISHED: (many steps)
---------------------------------------------------------
Finding Where the Extrema of Quadratic Form is Attained Over an Ellipse
---------------------------------------------------------
START: (also many steps)
these steps are very similar to contrained optimization

Find the min value of Q(x) = 16x1^2 + 16x1x2 + x2^2 given that 16x1^2 + x2^2 = 16 find the vector x at which the min is obtained.
First the matrix of our quadratic form:
A = [
	16 8
	 8 1
]
the contraint equation:
x1^2/1^2 + x2^2/4^2 = 1
This is an ellipse with semiaxes a = 1 (along the x1-axis) and b = 4 (along the x2-axis)
if we introduce new variables:
y1 = x1, and y2 = x2/4
the equation of the ellipse will transform into y1^2 + y2^2 = 1, or ||y|| = 1
change-of-coordinates matrix
P = [
	1 0
	0 4
]
we obtain that x = Py. substituting this into the expression of the quadratic form
Q(x) = x^TAx = (Py)^TA(Py) = y^T(P^TAP)y = Q'(y)
since P^T = P the matrix of our quadratic form in new coordinates is
B = P^TAP
= PAP
= [
	1 0
	0 4
][
	16 8
	 8 1
][
	1 0
	0 4
]
= [
	16 32
	32 16
]
now we maximize Q'(y) when ||y|| = 1. Calculating the eigenvalues B, we obtain the following:
|B - λI| = 0
|16-λ 32|
|32 16-λ| = 0
(16 - λ)(16 - λ) - 1024 = 0
λ^2 - 32λ - 768 = 0
(λ - 48)(λ + 16) = 0
λ = 48, -16
therefore we have
min{Q'(y) | ||y|| = 1} = λ_min = -16
unit eigenvector that corresponds to the eigenvalue λ_min = -16
B-(-16I) = [
	16 32
	32 16
] + 16[
	1 0
	0 1
] = [
	32 32
	32 32
]
seeking a non-zero solution of (B-(-16I))y = 0 gives the eigenvector v = [1, 1] dividing v by its norm ||v|| = sqrt(2):
u_min = v/||v|| = 1/sqrt(2)[1,1]
finally map this vector back to the initial coordinate system using the change-of-coordinates matrix:
x_min = Pu_min
= [
	1 0
	0 4
] ⋅ 1/sqrt(2)[1, 1]
= 1/sqrt(2)[1, 4]
FINISHED: (many steps)
---------------------------------------------------------
