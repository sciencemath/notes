FTI architecture
Feature Training and Inference

Feature Pipeline
Takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference, saved to a feature store

Training Pipeline
Takes the features and labels from the features stored as input and outputs a train model or models and puts them into a model registry

Inference Pipeline
Takes as input the features and labels from the feature store and trained model from the model registry to make predictions

Benefits of FTI:
- Just three components easy to understand
- Each component can be written into its own tech stack, so we can adapt them to specific needs, allows us to pick the best tools for the job.
- There is a transparent interface between the three components, each one can be developed by a different team
- every component can be monitored and deployed independently.

RAG (retrieval-augmented generation)
With any RAG-based system, one of the central peices of the infrastructure is a vector DB

LLM encapsulates and automates all the following steps:
- Data collection
- Data preprocessing
- Data storage, versioning, and retrieval
- LLM fine-tuning
- RAG
- Content generation evaluation
The key is to be data-centric and architecture model-agnostic so we can use different models on specific data.

run `pyenv local 3.11.8` then pyenv will always know to use that python version it creates a `.python-version` file

Poetry is a dependency and vitrual environment manager.
it saves all its requirements in `pyproject.toml`
creates a .lock file just like node package manager.
had used venv before but lacks the dependency management option

Poe the Poet is a plugin on top of Poetry
helps you define and run tasks within your Python project, simplifying automation and script execution.
Just like how we have commands in package.json example:

we've defined:
[tool.poe.tasks]
test = "pytest"

then run:
`poetry poe test`

ZenML acts as the bridge between ML and MLOps. It handles transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment.
ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.

An orchestrator is a system that automates, schedules, and coordinates all your ML pipelines.

ZenML works as an orchestrator via piplelines and steps, example:

from zenml import pipeline
from steps.etl import crawl_links, get_or_create_user
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> None:
	user = get_or_create_user(user_full_name)
	crawl_links(user=user, links=links)

To integrate ZenML with your code you have to write modular code, where each function does one thing (makes it easy to use `@step` decorators):

from loguru import logger
from typing_extensions import Annotated
from zenml import step
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument

@step
def get_or_create_user(user_full_name: str) -> Annotated[UserDocument, "user"]:
    logger.info(f"Getting or creating user: {user_full_name}")
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    return user

Have a directory of steps alongside pipelines on the root so we can swap orchestrators if needed to use a REST API

In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. We can transform anything into an artifact.

For web scraping its good to keep a config folder and a specific .yaml file that have all the URLs associated to (in this case) all posts/code from the same person. 

Keeping everything in pipeline config .yaml files is ideal

We can use Comet to track metrics and visualize them, track different config between experiements

Opik: prompt monitoring
Qdrant: vector database

SageMaker provides a comprehensive platform for building, training, and deploying machine learning models vs Amazon Bedrock which is just pretrained models accessed directly through an API an "out-of-box" solution.
Even SageMaker isn’t fully customizable. If you want complete control over your deployment use EKS AWS Kubernetes self-managed service.

We use an ETL pipline to extract data, transforming and cleaning data into a suitable format for storage and analysis, and load into our warehouse or db.

ETL and the feature pipeline strictly communicate trough the Mongo data warehouse. Feature will read and the ETL process will write acting independently.

using MongoDB as a data warehouse is uncommon, but we're using it for a small amount of data and its fine for our unstrucutred data (internet web articles), for production or dealing with tons of data we would use Snowflake or BigQuery

Our crawling logic/step:
===========================================================
from urllib.parse import urlparse
from loguru import logger
from tqdm import tqdm
from typing_extensions import Annotated
from zenml import get_step_context, step
from llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher
from llm_engineering.domain.documents import UserDocument

@step
def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], "crawled_links"]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(f"Starting to crawl {len(links)} link(s).")
    metadata = {}
    successfull_crawls = 0
    for link in tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
        step_context = get_step_context()
    step_context.add_output_metadata(output_name="crawled_links", metadata=metadata)
    logger.info(f"Successfully crawled {successfull_crawls} / {len(links)} links.")
    return links

def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    try:
        crawler.extract(link=link, user=user)
        return (True, crawler_domain)
    except Exception as e:
        logger.error(f"An error occurred while crawling: {e!s}")
        return (False, crawler_domain)

def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:
	if domain not in metadata:
	    metadata[domain] = {}
	metadata[domain]["successful"] = metadata.get(domain, {}).get("successful", 0) + successfull_crawl
	metadata[domain]["total"] = metadata.get(domain, {}).get("total", 0) + 1
	return metadata
===========================================================
CrawlerDispatcher:
===========================================================
import re
from urllib.parse import urlparse
from loguru import logger
from .base import BaseCrawler
from .custom_article import CustomArticleCrawler
from .github import GithubCrawler
from .linkedin import LinkedInCrawler
from .medium import MediumCrawler

class CrawlerDispatcher:
    def __init__(self) -> None:
        self._crawlers = {}

    @classmethod
    def build(cls) -> "CrawlerDispatcher":
        dispatcher = cls()
        return dispatcher

    # CrawlerDispatcher.build().register_linkedin().register_medium()
    def register_medium(self) -> "CrawlerDispatcher":
        self.register("https://medium.com", MediumCrawler)
        return self
    def register_linkedin(self) -> "CrawlerDispatcher":
        self.register("https://linkedin.com", LinkedInCrawler)
        return self
    def register_github(self) -> "CrawlerDispatcher":
        self.register("https://github.com", GithubCrawler)
        return self


    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[r"https://(www\.)?{}/*".format(re.escape(domain))] = crawler


    def get_crawler(self, url: str) -> BaseCrawler:
        for pattern, crawler in self._crawlers.items():
            if re.match(pattern, url):
                return crawler()
        else:
            logger.warning(f"No crawler found for {url}. Defaulting to CustomArticleCrawler.")
            return CustomArticleCrawler()
===========================================================
BaseCrawler
===========================================================
from abc import ABC, abstractmethod
class BaseCrawler(ABC):
    model: type[NoSQLBaseDocument]
    @abstractmethod
    def extract(self, link: str, **kwargs) -> None: ...

Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave.
we can use this tool to login, scroll websites, click on different elements etc
===========================================================
BaseSeleniumCrawler
===========================================================
import time
from tempfile import mkdtemp
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from llm_engineering.domain.documents import NoSQLBaseDocument
# Check if the current version of chromedriver exists
# and if it doesn't exist, download it automatically,
# then add chromedriver to path
chromedriver_autoinstaller.install()

class BaseSeleniumCrawler(BaseCrawler, ABC):
    def __init__(self, scroll_limit: int = 5) -> None:
        options = webdriver.ChromeOptions()
       
        options.add_argument("--no-sandbox")
        options.add_argument("--headless=new")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-background-networking")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument(f"--user-data-dir={mkdtemp()}")
        options.add_argument(f"--data-path={mkdtemp()}")
        options.add_argument(f"--disk-cache-dir={mkdtemp()}")
        options.add_argument("--remote-debugging-port=9226")

        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )

    # placeholders
    def set_extra_driver_options(self, options: Options) -> None:
        pass
    def login(self) -> None:
        pass

    def scroll_page(self) -> None:
        """Scroll through the LinkedIn page based on the scroll limit."""
        current_scroll = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):
                break
            last_height = new_height
            current_scroll += 1

===========================================================
next we can look at the concrete crawlers, in our case theres three
GitHubCrawler(BaseCrawler)
CustomArticleCrawler(BaseCrawler)
MediumCrawler(BaseSeleniumCrawler)

We don't have to log in to GitHub through the browser, as we can leverage Git clone functionality. this means no Selenium functionality. 


GithubCrawler
===========================================================
class GithubCrawler(BaseCrawler):
    model = RepositoryDocument
    def __init__(self, ignore=(".git", ".toml", ".lock", ".png")) -> None:
        super().__init__()
        self._ignore = ignore

    def extract(self, link: str, **kwargs) -> None:
    	old_model = self.model.find(link=link)
    	if old_model is not None:
        	logger.info(f"Repository already exists in the database: {link}")
        	return
        logger.info(f"Starting scrapping GitHub repository: {link}")
    	repo_name = link.rstrip("/").split("/")[-1]
    	local_temp = tempfile.mkdtemp()

	    try:
        	os.chdir(local_temp)
        	subprocess.run(["git", "clone", link])
        	repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])
	        tree = {}
	        for root, _, files in os.walk(repo_path):
	            dir = root.replace(repo_path, "").lstrip("/")
	            if dir.startswith(self._ignore):
	                continue
	            for file in files:
	                if file.endswith(self._ignore):
	                    continue
	                file_path = os.path.join(dir, file)
	                with open(os.path.join(root, file), "r", errors="ignore") as f:
	                    tree[file_path] = f.read().replace(" ", "")

	        user = kwargs["user"]
	        instance = self.model(
	            content=tree,
	            name=repo_name,
	            link=link,
	            platform="github",
	            author_id=user.id,
	            author_full_name=user.full_name,
	        )
	        instance.save()