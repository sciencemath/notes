FTI architecture
Feature Training and Inference

Feature Pipeline
Takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference, saved to a feature store

Training Pipeline
Takes the features and labels from the features stored as input and outputs a train model or models and puts them into a model registry

Inference Pipeline
Takes as input the features and labels from the feature store and trained model from the model registry to make predictions

Benefits of FTI:
- Just three components easy to understand
- Each component can be written into its own tech stack, so we can adapt them to specific needs, allows us to pick the best tools for the job.
- There is a transparent interface between the three components, each one can be developed by a different team
- every component can be monitored and deployed independently.

RAG (retrieval-augmented generation)
With any RAG-based system, one of the central peices of the infrastructure is a vector DB

LLM encapsulates and automates all the following steps:
- Data collection
- Data preprocessing
- Data storage, versioning, and retrieval
- LLM fine-tuning
- RAG
- Content generation evaluation
The key is to be data-centric and architecture model-agnostic so we can use different models on specific data.

run `pyenv local 3.11.8` then pyenv will always know to use that python version it creates a `.python-version` file

Poetry is a dependency and vitrual environment manager.
it saves all its requirements in `pyproject.toml`
creates a .lock file just like node package manager.
had used venv before but lacks the dependency management option

Poe the Poet is a plugin on top of Poetry
helps you define and run tasks within your Python project, simplifying automation and script execution.
Just like how we have commands in package.json example:

we've defined:
[tool.poe.tasks]
test = "pytest"

then run:
`poetry poe test`

ZenML acts as the bridge between ML and MLOps. It handles transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment.
ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.

An orchestrator is a system that automates, schedules, and coordinates all your ML pipelines.

ZenML works as an orchestrator via piplelines and steps, example:

from zenml import pipeline
from steps.etl import crawl_links, get_or_create_user
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> None:
	user = get_or_create_user(user_full_name)
	crawl_links(user=user, links=links)

To integrate ZenML with your code you have to write modular code, where each function does one thing (makes it easy to use `@step` decorators):

from loguru import logger
from typing_extensions import Annotated
from zenml import step
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument

@step
def get_or_create_user(user_full_name: str) -> Annotated[UserDocument, "user"]:
    logger.info(f"Getting or creating user: {user_full_name}")
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    return user

Have a directory of steps alongside pipelines on the root so we can swap orchestrators if needed to use a REST API

In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. We can transform anything into an artifact.

For web scraping its good to keep a config folder and a specific .yaml file that have all the URLs associated to (in this case) all posts/code from the same person. 

Keeping everything in pipeline config .yaml files is ideal

We can use Comet to track metrics and visualize them, track different config between experiements

Opik: prompt monitoring
Qdrant: vector database

SageMaker provides a comprehensive platform for building, training, and deploying machine learning models vs Amazon Bedrock which is just pretrained models accessed directly through an API an "out-of-box" solution.
Even SageMaker isn’t fully customizable. If you want complete control over your deployment use EKS AWS Kubernetes self-managed service.

We use an ETL pipline to extract data, transforming and cleaning data into a suitable format for storage and analysis, and load into our warehouse or db.

ETL and the feature pipeline strictly communicate trough the Mongo data warehouse. Feature will read and the ETL process will write acting independently.

using MongoDB as a data warehouse is uncommon, but we're using it for a small amount of data and its fine for our unstrucutred data (internet web articles), for production or dealing with tons of data we would use Snowflake or BigQuery

Our crawling logic/step:
===========================================================
from urllib.parse import urlparse
from loguru import logger
from tqdm import tqdm
from typing_extensions import Annotated
from zenml import get_step_context, step
from llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher
from llm_engineering.domain.documents import UserDocument

@step
def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], "crawled_links"]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(f"Starting to crawl {len(links)} link(s).")
    metadata = {}
    successfull_crawls = 0
    for link in tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
        step_context = get_step_context()
    step_context.add_output_metadata(output_name="crawled_links", metadata=metadata)
    logger.info(f"Successfully crawled {successfull_crawls} / {len(links)} links.")
    return links

def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    try:
        crawler.extract(link=link, user=user)
        return (True, crawler_domain)
    except Exception as e:
        logger.error(f"An error occurred while crawling: {e!s}")
        return (False, crawler_domain)

def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:
	if domain not in metadata:
	    metadata[domain] = {}
	metadata[domain]["successful"] = metadata.get(domain, {}).get("successful", 0) + successfull_crawl
	metadata[domain]["total"] = metadata.get(domain, {}).get("total", 0) + 1
	return metadata
===========================================================
CrawlerDispatcher:
===========================================================
import re
from urllib.parse import urlparse
from loguru import logger
from .base import BaseCrawler
from .custom_article import CustomArticleCrawler
from .github import GithubCrawler
from .linkedin import LinkedInCrawler
from .medium import MediumCrawler

class CrawlerDispatcher:
    def __init__(self) -> None:
        self._crawlers = {}

    @classmethod
    def build(cls) -> "CrawlerDispatcher":
        dispatcher = cls()
        return dispatcher

    # CrawlerDispatcher.build().register_linkedin().register_medium()
    def register_medium(self) -> "CrawlerDispatcher":
        self.register("https://medium.com", MediumCrawler)
        return self
    def register_linkedin(self) -> "CrawlerDispatcher":
        self.register("https://linkedin.com", LinkedInCrawler)
        return self
    def register_github(self) -> "CrawlerDispatcher":
        self.register("https://github.com", GithubCrawler)
        return self


    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[r"https://(www\.)?{}/*".format(re.escape(domain))] = crawler


    def get_crawler(self, url: str) -> BaseCrawler:
        for pattern, crawler in self._crawlers.items():
            if re.match(pattern, url):
                return crawler()
        else:
            logger.warning(f"No crawler found for {url}. Defaulting to CustomArticleCrawler.")
            return CustomArticleCrawler()
===========================================================
BaseCrawler
===========================================================
from abc import ABC, abstractmethod
class BaseCrawler(ABC):
    model: type[NoSQLBaseDocument]
    @abstractmethod
    def extract(self, link: str, **kwargs) -> None: ...

Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave.
we can use this tool to login, scroll websites, click on different elements etc
===========================================================
BaseSeleniumCrawler
===========================================================
import time
from tempfile import mkdtemp
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from llm_engineering.domain.documents import NoSQLBaseDocument
# Check if the current version of chromedriver exists
# and if it doesn't exist, download it automatically,
# then add chromedriver to path
chromedriver_autoinstaller.install()

class BaseSeleniumCrawler(BaseCrawler, ABC):
    def __init__(self, scroll_limit: int = 5) -> None:
        options = webdriver.ChromeOptions()
       
        options.add_argument("--no-sandbox")
        options.add_argument("--headless=new")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-background-networking")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument(f"--user-data-dir={mkdtemp()}")
        options.add_argument(f"--data-path={mkdtemp()}")
        options.add_argument(f"--disk-cache-dir={mkdtemp()}")
        options.add_argument("--remote-debugging-port=9226")

        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )

    # placeholders
    def set_extra_driver_options(self, options: Options) -> None:
        pass
    def login(self) -> None:
        pass

    def scroll_page(self) -> None:
        """Scroll through the LinkedIn page based on the scroll limit."""
        current_scroll = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):
                break
            last_height = new_height
            current_scroll += 1

===========================================================
next we can look at the concrete crawlers, in our case theres three
GitHubCrawler(BaseCrawler)
CustomArticleCrawler(BaseCrawler)
MediumCrawler(BaseSeleniumCrawler)

We don't have to log in to GitHub through the browser, as we can leverage Git clone functionality. this means no Selenium functionality. 


GithubCrawler

===========================================================
class GithubCrawler(BaseCrawler):
    model = RepositoryDocument
    def __init__(self, ignore=(".git", ".toml", ".lock", ".png")) -> None:
        super().__init__()
        self._ignore = ignore

    def extract(self, link: str, **kwargs) -> None:
    	old_model = self.model.find(link=link)
    	if old_model is not None:
        	logger.info(f"Repository already exists in the database: {link}")
        	return
        logger.info(f"Starting scrapping GitHub repository: {link}")
    	repo_name = link.rstrip("/").split("/")[-1]
    	local_temp = tempfile.mkdtemp()

	    try:
        	os.chdir(local_temp)
        	subprocess.run(["git", "clone", link])

        	# walks dir tree, skipping any ignore patterns
        	# removes spaces

        	repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])
	        tree = {}
	        for root, _, files in os.walk(repo_path):
	            dir = root.replace(repo_path, "").lstrip("/")
	            if dir.startswith(self._ignore):
	                continue
	            for file in files:
	                if file.endswith(self._ignore):
	                    continue
	                file_path = os.path.join(dir, file)
	                with open(os.path.join(root, file), "r", errors="ignore") as f:
	                    tree[file_path] = f.read().replace(" ", "")
	        # creates a new instance of RepositoryDocument model
	        # then saved to MongoDB
	        user = kwargs["user"]
	        instance = self.model(
	            content=tree,
	            name=repo_name,
	            link=link,
	            platform="github",
	            author_id=user.id,
	            author_full_name=user.full_name,
	        )
	        instance.save()
	    # clean up the temp directory
	    except Exception:
	        raise
	    finally:
	        shutil.rmtree(local_temp)
	    logger.info(f"Finished scrapping GitHub repository: {link}")
===========================================================
CustomArticleCrawler
This will use AsyncHtmlLoader and Html2TextTransformer
both from the langchain community

some developers avoid using LangChain in production

===========================================================
from urllib.parse import urlparse
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers.html2text import Html2TextTransformer
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseCrawler

class CustomArticleCrawler(BaseCrawler):
    model = ArticleDocument
    def extract(self, link: str, **kwargs) -> None:
        old_model = self.model.find(link=link)
        if old_model is not None:
            logger.info(f"Article already exists in the database: {link}")
            return
        # here we use both
        # AsyncHtmlLoader, and Html2TextTransformer
        # we are not in control is extracted and parsed
        # fallback where we don’t have anything custom implemented

	    logger.info(f"Starting scrapping article: {link}")
	    loader = AsyncHtmlLoader([link])
	    docs = loader.load()
	    html2text = Html2TextTransformer()
	    docs_transformed = html2text.transform_documents(docs)
	    doc_transformed = docs_transformed[0]
	    # page content and meta data
        content = {
            "Title": doc_transformed.metadata.get("title"),
            "Subtitle": doc_transformed.metadata.get("description"),
            "Content": doc_transformed.page_content,
            "language": doc_transformed.metadata.get("language"),
        }

        # This parses URL to determine the platform (domain) 
        # from which article was scrapped

        parsed_url = urlparse(link)
        platform = parsed_url.netloc

        # Next save everything to MongoDB

        user = kwargs["user"]
        instance = self.model(
            content=content,
            link=link,
            platform=platform,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Finished scrapping custom article: {link}")       
===========================================================
MediumCrawler

NOTE: since we use extract and a conditional at the beginnning
for each of these crawlers this should be abstracted out as well.
===========================================================
from bs4 import BeautifulSoup
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseSeleniumCrawler
class MediumCrawler(BaseSeleniumCrawler):
    model = ArticleDocument

    def set_extra_driver_options(self, options) -> None:
    	options.add_argument(r"--profile-directory=Profile 2")

    def extract(self, link: str, **kwargs) -> None:
	    old_model = self.model.find(link=link)
	    if old_model is not None:
	        logger.info(f"Article already exists in the database: {link}")
	        return
	    logger.info(f"Starting scrapping Medium article: {link}")
	    self.driver.get(link)
	    self.scroll_page()
	    # The only crawler using BeautifulSoup
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        title = soup.find_all("h1", class_="pw-post-title")
        subtitle = soup.find_all("h2", class_="pw-subtitle-paragraph")
        data = {
            "Title": title[0].string if title else None,
            "Subtitle": subtitle[0].string if subtitle else None,
            "Content": soup.get_text(),
        }

        # save to DB

        self.driver.close()
        user = kwargs["user"]
        instance = self.model(
            platform="medium",
            content=data,
            link=link,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Successfully scraped and saved article: {link}")

Linked in is a bit longer then the other two scrapers as it uses Beautiful soup
does a bulk_insert for all posts, logins() the user, find elements based on classes which in fact may change over time but the same idea applies for scrapping setting up model and saving

Since LLM core features is scrapping there are two other popular scrapping data:
Scrapy: https://github.com/scrapy/scrapy
Crawl4Ai: https://github.com/unclecode/crawl4ai
ORMs:
FastAPI SQLModel: https://github.com/fastapi/sqlmodel
SQLAlchemy: https://www.sqlalchemy.org/

It is best practice to structure data in classes instead of dictionaries, each item is more verbose, reducing run errors. 

ODM pattern is similar to ORM but instead it simply works with NoSQL

===========================================================
NoSQLBaseDocument

Nothing special in these methods just a wrapper for
DB management, the class methods here are self explaintory
similar to mongoengine
===========================================================
import uuid
from abc import ABC
from typing import Generic, Type, TypeVar
from loguru import logger
from pydantic import UUID4, BaseModel, Field
from pymongo import errors
from llm_engineering.domain.exceptions import ImproperlyConfigured
from llm_engineering.infrastructure.db.mongo import connection
from llm_engineering.settings import settings

_database = connection.get_database(settings.DATABASE_NAME)
T = TypeVar("T", bound="NoSQLBaseDocument")

# id field is defined as a UUID4, with a default factory generating a unique UUID
class NoSQLBaseDocument(BaseModel, Generic[T], ABC):
	id: UUID4 = Field(default_factory=uuid.uuid4)
	def __eq__(self, value: object) -> bool:
	    if not isinstance(value, self.__class__):
	        return False
	    return self.id == value.id
	def __hash__(self) -> int:
	    return hash(self.id)

	# The from_mongo() and to_mongo()
	# are what it says mongoDB->class, dictionary->mongoDB
	@classmethod
	def from_mongo(cls: Type[T], data: dict) -> T:
	    if not data:
	        raise ValueError("Data is empty.")
	    id = data.pop("_id")
	    return cls(**dict(data, id=id))
	def to_mongo(self: T, **kwargs) -> dict:
	    exclude_unset = kwargs.pop("exclude_unset", False)
	    by_alias = kwargs.pop("by_alias", True)
	    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
	    if "_id" not in parsed and "id" in parsed:
	        parsed["_id"] = str(parsed.pop("id"))
	    for key, value in parsed.items():
	        if isinstance(value, uuid.UUID):
	            parsed[key] = str(value)
	    return parsed

	def save(self: T, **kwargs) -> T | None:
	    collection = _database[self.get_collection_name()]
	    try:
	        collection.insert_one(self.to_mongo(**kwargs))
	        return self
	    except errors.WriteError:
	        logger.exception("Failed to insert document.")
	        return None

	@classmethod
	def get_or_create(cls: Type[T], **filter_options) -> T:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        new_instance = cls(**filter_options)
	        new_instance = new_instance.save()
	        return new_instance
	    except errors.OperationFailure:
	        logger.exception(f"Failed to retrieve document with filter options: {filter_options}")
	        raise

	@classmethod
	def bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:
	    collection = _database[cls.get_collection_name()]
	    try:
	        collection.insert_many([doc.to_mongo(**kwargs) for doc in documents])
	        return True
	    except (errors.WriteError, errors.BulkWriteError):
	logger.error(f"Failed to insert documents of type {cls.__name__}")
	        return False

	@classmethod
	def find(cls: Type[T], **filter_options) -> T | None:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        return None
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return None

	@classmethod
	def bulk_find(cls: Type[T], **filter_options) -> list[T]:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instances = collection.find(filter_options)
	        return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return []

	@classmethod
	def get_collection_name(cls: Type[T]) -> str:
	    if not hasattr(cls, "Settings") or not hasattr(cls.Settings, "name"):
	        raise ImproperlyConfigured(
	            "Document should define an Settings configuration class with the name of the collection."
	        )
	    return cls.Settings.name

=========================================================
Lastly concrete classes that define our data categories
=========================================================
from abc import ABC
from typing import Optional
from pydantic import UUID4, Field
from .base import NoSQLBaseDocument
from .types import DataCategory
from enum import StrEnum

class DataCategory(StrEnum):
    PROMPT = "prompt"
    QUERIES = "queries"
    INSTRUCT_DATASET_SAMPLES = "instruct_dataset_samples"
    INSTRUCT_DATASET = "instruct_dataset"
    PREFERENCE_DATASET_SAMPLES = "preference_dataset_samples"
    PREFERENCE_DATASET = "preference_dataset"
    POSTS = "posts"
    ARTICLES = "articles"
	REPOSITORIES = "repositories"

class Document(NoSQLBaseDocument, ABC):
    content: dict
    platform: str
    author_id: UUID4 = Field(alias="author_id")
    author_full_name: str = Field(alias="author_full_name")

class RepositoryDocument(Document):
    name: str
    link: str
    class Settings:
        name = DataCategory.REPOSITORIES
class PostDocument(Document):
    image: Optional[str] = None
    link: str | None = None
    class Settings:
        name = DataCategory.POSTS
class ArticleDocument(Document):
    link: str
    class Settings:
        name = DataCategory.ARTICLES

class UserDocument(NoSQLBaseDocument):
    first_name: str
    last_name: str
    class Settings:
        name = "users"
    @property
    def full_name(self):
        return f"{self.first_name} {self.last_name}"

-------------------------------------------------------------------------------
You often want to use the LLM on data it wasn’t trained on
Retrieval augmented generation (RAG)
- used to inject custom data into the LLM to perform a given action
- fine tuning LLM is highly costly, RAG bypasses need for constant fine-tuning
- summarize, reformulate, and extract the injected data

Retrieval: Search for relevant data
Augmented: Add data as context to the prompt
Generation: Augmented prompt with an LLM for generation

LLM is bound to the data it was trained on (parameterized knowledge)
GPT-4o trained data up to Oct 2023, RAG overcomes these limits and provides access to external or latest data and prevents hallucinations.

RAG Solves:
Hallucinations (could tell us something that isn't true)
Old or private information (new data is generated every second!)

RAG system (3 modules):
Ingestion pipeline: batch or streaming pipeline to populate vectorDB
Retrieval pipeline: queries vector DB and retrieves relevant users input
Generation pipeline: uses retrieved data to augment the prompt and LLM to generate answers

ingestion pipeline (constantly updating)
user input->retrieval->generation->LLM->answer

Ingestion pipeline
---------------------------------------
gets data from many sources then cleans, chunks, and embeds
loads the embedded chunks into a vector DB

first data extraction, from DB APIs or webpages
second cleaning layer, depending on your data cleaning varies such as invalid/unwanted characters 
third chunking, splits cleaned documents into smaller ones. ensure it doesn't exceed the models input max size.
fourth embedding, takes the chunks content and project it into a dense vector packed with semantic value.
fifth loading, takes embedded chunks along with metadata the embedding is used as an index to query similar chnks. metadata is used to access the information added to augment the prompt

Retrieval pipeline
---------------------------------------
take user input (text, image, audio) embed it and query vector DB for similar vectors
project users input into the same vector space as the embeddings used as an index in the vector DB. We can then find the top K's most similar entried by comparing embeddings from the vector storage with the user's input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer

distance formula cosine distance is popular (others Euclidean, Manhatten)
1 - cos(θ) = 1 - (A ⋅ B)/(||A||⋅||B||)
ranges from -1 (opposite directions) to 1 (same direction), 0 (orthogonal)
distance between two vectors depends on data and embedding model used.
To avoid training-serving skew you must clean, chunk and embed the users input using the same functions, models, and hyperparameters, input and embeddings must be in the same vector space.

Generation pipeline
---------------------------------------
Last step take users input retrieve data, pass to LLM, generate answer. Usually all the prompt engineering is done at the prompt template level.
The final prompt populated with user's query and retrieved context

As the prompt templates evolve, each change should be tracked and versioned using machine learning operations (MLOps) best practices.
could use git or prompt management tools like LangFuse etc.

Emedddings is like a map where words with similar meanings are clustered together with numerical represenation encoded as vectors in continuous vector space.
Could be used in words, images or recommendation systems.

NLP (natrual language processing) embeddings translate words into vectors where semantically similar words are positioned close in vector space.

embeddings have more than 2 or 3 dimensions usually 64 to 2048 you must project them again to 2D or 3D.
t-SNE or UMAP uses transform high-dimensional reduction keeping geometrical properties between points, making it easier to visualize, interpret, and process while minimizing the loss of important information.
embeddings encode any categorical variable and you can feed it to an ML model.

One-hot can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.

Feature hashing (hash trick) maps categories into a fixed number of bins or buckets. risk of collisions, where different categories might map to the same bin, leading to a loss of information. It's also difficult to understand the relationship between the original categories and the hashed features.

Embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. Similar to techniques of a CNN

====================================================
example using Sentence Transformers Python package
(also in hugging face)
====================================================
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
sentences = [
"The cat sits outside waiting for a mouse.",
"I am going swimming.",
"The cat is swimming."
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# Output: [3, 384]
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# Output:
# tensor([[ 1.0000, -0.0389, 0.2692],
# [-0.0389, 1.0000, 0.3837],
# [ 0.2692, 0.3837, 1.0000]])
#
# similarities[0, 0] = The similarity between the first sentence and itself.
# similarities[0, 1] = The similarity between the first and second sentence.
# similarities[2, 1] = The similarity between the third and second sentence.
====================================================
you can pick between best-performing model or one with smallest memory footprint, descisions should be based on requirements, best to experiment

Models like CLIP, lets you embed a piece of text and an image in the same vector. This allows you to find similar images using sentence as input and vice versa
example:
====================================================
from io import BytesIO
import requests
from PIL import Image
from sentence_transformers import SentenceTransformer

response = requests.get(
"https://i.etsystatic.com/9632921/r/il/a3e46a/3070971627/il_1588xN.3070971627_5jbv.jpg"
)
image = Image.open(BytesIO(response.content))
model = SentenceTransformer("clip-ViT-B-32")
img_emb = model.encode(image)
text_emb = model.encode(
	[
		"A crazy cat smiling.",
		"A white and brown cat with a yellow bandana.",
		"A man eating in the garden."
	]
)

print(text_emb.shape)
# Output: (3, 512)
similarity_scores = model.similarity(img_emb, text_emb)
print(similarity_scores)
# Output: tensor([[0.3068, 0.3300, 0.1719]])

====================================================

A Vector db look for the closest neighbors of the query vector, it uses approximate nearest neighbor (ANN) algorithms using only approximations of the top matches for a given input query works well enough trade-off between accuracy and latency

workflow of a vector DB
1. indexing vectors, vectors are indexed using data structures optimized for high-dimensional data.
	- Hierarchiacal navigable small world (HNSW)
		- multi-layer graph where each node represents a set of vectors.
	- random projection
		reduces high-dimensonility of vectors by projecting into a lower dimensional space using a random matrix
	- product quantization (PQ)
		- dividing vectors into smaller sub-vectors and then quantizing these sub-vectors into representative codes.
	- locality-sensitive hashing (LSH)
		- maps similar vectors into buckets, fast approximate nearest neighbor searches by focusing on a subset of the data
2. querying for similarity (distance formulas)
3. post-processing results: refine accuracy, most relevant vectors are returned

Vector DB share common characteristics of a standard DB
- Sharding and replication
- Monitoring
- Access Control
- Backups

There are some limitiations in the vanilla RAG framework, we can improve this with an advanced RAG in these stages:
- Pre-retrieval index optimizations as well as query optimization
- Retrieval improving the embedding models and metadata filtering
- Post-retrieval filter out noise from retrieved documents and compress the prompt before feeding it to an LLM

Pre-retrieval
-----------------------------
- Data indexing cleaning or chunking modules to preprocess data for better indexing
- Query Optimization, algorithm performed directly on users's query before embedding it and retrieving chunks from vector DB

improve better preprocessing and structuring data:
- Sliding window
	- overlap between text chunks, ensuring important context near chunk bondaries is reatined, legal documents, scientific papers, customer support logs, medical records, critical information spans multiple sections. (maintains context)
- Enhance data granularity 
	- removing irrelevant details, factual accuracy, updating outdated information
- Metadata
	- dates, URLs, chapter markers (filter)
- Optimizing index structures
	- chunk sizes and multi-indexing strategies
- Small-to-big
	- decouples chunks for retrieval and context used in prompt for final answer generation, uses small sequence of text to compute the embedding while preserving the sequence and a wider window around it in the metadata, smaller chunks enhances the retrievals accuracy, the larger context adds more context info to the LLM
We do this to reduce the noise, or the text could contain multiple topics
- Query routing
	- Natural language is used to route queries if user input has multiple categories
- Query rewriting (reformulating question to match indexed information better)
	- Paraphrasing (e.g. "are there a lot of people that die from snakes?" could be written as "How many people die each year from snakes")
	- Synonym substitution less common words with synonyms to broaden the search (e.g. "gleeful" to "hppy")
	- Sub-queries break down long queries to mroe focused sub-queries
- Hypothetical document embeddings (HyDE) LLM creates hypotheticals, both original query and LLMs response are fed into retrieval stage
- Query expansion adding terms or concepts resulting in different perspectives for example you could search for "Zelda" but also include "Nintendo" or "Master Sword"
- Self-Query unstructured queries to structured ones

You'll have to experiment with each of these because it depends on your data

Retrieval
-----------------------------
- imporving the embedding models
- Leveraging the DB's filter and search features
both enhance the vector search step by finding similarity between query and indexed data

Instructor Models: (instead of fine-tuning the embedding model)
Tailoring your embedding network to your data using such a model can be a good option fine-tuning a model consumes more computing and human resources.

====================================================
from InstructorEmbedding import INSTRUCTOR

model = INSTRUCTOR("hkunlp/instructor-base")
sentence = "RAG Fundamentals First"
instruction = "Represent the title of an article about AI:"
embeddings = model.encode([[instruction, sentence]])

print(embeddings.shape)
# Output: (1, 768)
====================================================
python3 -m venv instructor_venv && source instructor_venv/bin/activate
pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1

classic filter and search DB filters

- Hybrid search A vector and keyword-based search blend, you have an alpha that controls weight between the two methods, two independent searches, later normalized and unified.
- Filtered vector search, metadata index to filter for specific keywords within metadata.


Post-Retrieval
-----------------------------
This step is to limit irrelevant information, or the context can be too large which can distract the LLM
- Prompt compression eliminate unnecessary details
- re-ranking use a cross-encoder ML model to give a matching score between the user's input and every retrieved chunk. Retrieved items are sorted based on this score. its costly to do this initial retrieval step.

Any RAG system is split into two independent components
- The ingestion pipeline
	- raw data, cleans, chunks, embeds, loads into vector DB
- The inference pipeline
	- queries vector db for relevant context generates by leveraging an LLM

