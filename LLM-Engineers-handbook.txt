FTI architecture
Feature Training and Inference

Feature Pipeline
Takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference, saved to a feature store

Training Pipeline
Takes the features and labels from the features stored as input and outputs a train model or models and puts them into a model registry

Inference Pipeline
Takes as input the features and labels from the feature store and trained model from the model registry to make predictions

Benefits of FTI:
- Just three components easy to understand
- Each component can be written into its own tech stack, so we can adapt them to specific needs, allows us to pick the best tools for the job.
- There is a transparent interface between the three components, each one can be developed by a different team
- every component can be monitored and deployed independently.

RAG (retrieval-augmented generation)
With any RAG-based system, one of the central peices of the infrastructure is a vector DB

LLM encapsulates and automates all the following steps:
- Data collection
- Data preprocessing
- Data storage, versioning, and retrieval
- LLM fine-tuning
- RAG
- Content generation evaluation
The key is to be data-centric and architecture model-agnostic so we can use different models on specific data.

run `pyenv local 3.11.8` then pyenv will always know to use that python version it creates a `.python-version` file

Poetry is a dependency and vitrual environment manager.
it saves all its requirements in `pyproject.toml`
creates a .lock file just like node package manager.
had used venv before but lacks the dependency management option

Poe the Poet is a plugin on top of Poetry
helps you define and run tasks within your Python project, simplifying automation and script execution.
Just like how we have commands in package.json example:

we've defined:
[tool.poe.tasks]
test = "pytest"

then run:
`poetry poe test`

ZenML acts as the bridge between ML and MLOps. It handles transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment.
ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.

An orchestrator is a system that automates, schedules, and coordinates all your ML pipelines.

ZenML works as an orchestrator via piplelines and steps, example:

from zenml import pipeline
from steps.etl import crawl_links, get_or_create_user
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> None:
	user = get_or_create_user(user_full_name)
	crawl_links(user=user, links=links)

To integrate ZenML with your code you have to write modular code, where each function does one thing (makes it easy to use `@step` decorators):

from loguru import logger
from typing_extensions import Annotated
from zenml import step
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument

@step
def get_or_create_user(user_full_name: str) -> Annotated[UserDocument, "user"]:
    logger.info(f"Getting or creating user: {user_full_name}")
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    return user

Have a directory of steps alongside pipelines on the root so we can swap orchestrators if needed to use a REST API

In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. We can transform anything into an artifact.

For web scraping its good to keep a config folder and a specific .yaml file that have all the URLs associated to (in this case) all posts/code from the same person. 

Keeping everything in pipeline config .yaml files is ideal

We can use Comet to track metrics and visualize them, track different config between experiements

Opik: prompt monitoring
Qdrant: vector database

SageMaker provides a comprehensive platform for building, training, and deploying machine learning models vs Amazon Bedrock which is just pretrained models accessed directly through an API an "out-of-box" solution.
Even SageMaker isn’t fully customizable. If you want complete control over your deployment use EKS AWS Kubernetes self-managed service.

We use an ETL pipline to extract data, transforming and cleaning data into a suitable format for storage and analysis, and load into our warehouse or db.

ETL and the feature pipeline strictly communicate trough the Mongo data warehouse. Feature will read and the ETL process will write acting independently.

using MongoDB as a data warehouse is uncommon, but we're using it for a small amount of data and its fine for our unstrucutred data (internet web articles), for production or dealing with tons of data we would use Snowflake or BigQuery

Our crawling logic/step:
===========================================================
from urllib.parse import urlparse
from loguru import logger
from tqdm import tqdm
from typing_extensions import Annotated
from zenml import get_step_context, step
from llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher
from llm_engineering.domain.documents import UserDocument

@step
def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], "crawled_links"]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(f"Starting to crawl {len(links)} link(s).")
    metadata = {}
    successfull_crawls = 0
    for link in tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
        step_context = get_step_context()
    step_context.add_output_metadata(output_name="crawled_links", metadata=metadata)
    logger.info(f"Successfully crawled {successfull_crawls} / {len(links)} links.")
    return links

def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    try:
        crawler.extract(link=link, user=user)
        return (True, crawler_domain)
    except Exception as e:
        logger.error(f"An error occurred while crawling: {e!s}")
        return (False, crawler_domain)

def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:
	if domain not in metadata:
	    metadata[domain] = {}
	metadata[domain]["successful"] = metadata.get(domain, {}).get("successful", 0) + successfull_crawl
	metadata[domain]["total"] = metadata.get(domain, {}).get("total", 0) + 1
	return metadata
===========================================================
CrawlerDispatcher:
===========================================================
import re
from urllib.parse import urlparse
from loguru import logger
from .base import BaseCrawler
from .custom_article import CustomArticleCrawler
from .github import GithubCrawler
from .linkedin import LinkedInCrawler
from .medium import MediumCrawler

class CrawlerDispatcher:
    def __init__(self) -> None:
        self._crawlers = {}

    @classmethod
    def build(cls) -> "CrawlerDispatcher":
        dispatcher = cls()
        return dispatcher

    # CrawlerDispatcher.build().register_linkedin().register_medium()
    def register_medium(self) -> "CrawlerDispatcher":
        self.register("https://medium.com", MediumCrawler)
        return self
    def register_linkedin(self) -> "CrawlerDispatcher":
        self.register("https://linkedin.com", LinkedInCrawler)
        return self
    def register_github(self) -> "CrawlerDispatcher":
        self.register("https://github.com", GithubCrawler)
        return self


    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[r"https://(www\.)?{}/*".format(re.escape(domain))] = crawler


    def get_crawler(self, url: str) -> BaseCrawler:
        for pattern, crawler in self._crawlers.items():
            if re.match(pattern, url):
                return crawler()
        else:
            logger.warning(f"No crawler found for {url}. Defaulting to CustomArticleCrawler.")
            return CustomArticleCrawler()
===========================================================
BaseCrawler
===========================================================
from abc import ABC, abstractmethod
class BaseCrawler(ABC):
    model: type[NoSQLBaseDocument]
    @abstractmethod
    def extract(self, link: str, **kwargs) -> None: ...

Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave.
we can use this tool to login, scroll websites, click on different elements etc
===========================================================
BaseSeleniumCrawler
===========================================================
import time
from tempfile import mkdtemp
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from llm_engineering.domain.documents import NoSQLBaseDocument
# Check if the current version of chromedriver exists
# and if it doesn't exist, download it automatically,
# then add chromedriver to path
chromedriver_autoinstaller.install()

class BaseSeleniumCrawler(BaseCrawler, ABC):
    def __init__(self, scroll_limit: int = 5) -> None:
        options = webdriver.ChromeOptions()
       
        options.add_argument("--no-sandbox")
        options.add_argument("--headless=new")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-background-networking")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument(f"--user-data-dir={mkdtemp()}")
        options.add_argument(f"--data-path={mkdtemp()}")
        options.add_argument(f"--disk-cache-dir={mkdtemp()}")
        options.add_argument("--remote-debugging-port=9226")

        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )

    # placeholders
    def set_extra_driver_options(self, options: Options) -> None:
        pass
    def login(self) -> None:
        pass

    def scroll_page(self) -> None:
        """Scroll through the LinkedIn page based on the scroll limit."""
        current_scroll = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):
                break
            last_height = new_height
            current_scroll += 1

===========================================================
next we can look at the concrete crawlers, in our case theres three
GitHubCrawler(BaseCrawler)
CustomArticleCrawler(BaseCrawler)
MediumCrawler(BaseSeleniumCrawler)

We don't have to log in to GitHub through the browser, as we can leverage Git clone functionality. this means no Selenium functionality. 


GithubCrawler

===========================================================
class GithubCrawler(BaseCrawler):
    model = RepositoryDocument
    def __init__(self, ignore=(".git", ".toml", ".lock", ".png")) -> None:
        super().__init__()
        self._ignore = ignore

    def extract(self, link: str, **kwargs) -> None:
    	old_model = self.model.find(link=link)
    	if old_model is not None:
        	logger.info(f"Repository already exists in the database: {link}")
        	return
        logger.info(f"Starting scrapping GitHub repository: {link}")
    	repo_name = link.rstrip("/").split("/")[-1]
    	local_temp = tempfile.mkdtemp()

	    try:
        	os.chdir(local_temp)
        	subprocess.run(["git", "clone", link])

        	# walks dir tree, skipping any ignore patterns
        	# removes spaces

        	repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])
	        tree = {}
	        for root, _, files in os.walk(repo_path):
	            dir = root.replace(repo_path, "").lstrip("/")
	            if dir.startswith(self._ignore):
	                continue
	            for file in files:
	                if file.endswith(self._ignore):
	                    continue
	                file_path = os.path.join(dir, file)
	                with open(os.path.join(root, file), "r", errors="ignore") as f:
	                    tree[file_path] = f.read().replace(" ", "")
	        # creates a new instance of RepositoryDocument model
	        # then saved to MongoDB
	        user = kwargs["user"]
	        instance = self.model(
	            content=tree,
	            name=repo_name,
	            link=link,
	            platform="github",
	            author_id=user.id,
	            author_full_name=user.full_name,
	        )
	        instance.save()
	    # clean up the temp directory
	    except Exception:
	        raise
	    finally:
	        shutil.rmtree(local_temp)
	    logger.info(f"Finished scrapping GitHub repository: {link}")
===========================================================
CustomArticleCrawler
This will use AsyncHtmlLoader and Html2TextTransformer
both from the langchain community

some developers avoid using LangChain in production

===========================================================
from urllib.parse import urlparse
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers.html2text import Html2TextTransformer
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseCrawler

class CustomArticleCrawler(BaseCrawler):
    model = ArticleDocument
    def extract(self, link: str, **kwargs) -> None:
        old_model = self.model.find(link=link)
        if old_model is not None:
            logger.info(f"Article already exists in the database: {link}")
            return
        # here we use both
        # AsyncHtmlLoader, and Html2TextTransformer
        # we are not in control is extracted and parsed
        # fallback where we don’t have anything custom implemented

	    logger.info(f"Starting scrapping article: {link}")
	    loader = AsyncHtmlLoader([link])
	    docs = loader.load()
	    html2text = Html2TextTransformer()
	    docs_transformed = html2text.transform_documents(docs)
	    doc_transformed = docs_transformed[0]
	    # page content and meta data
        content = {
            "Title": doc_transformed.metadata.get("title"),
            "Subtitle": doc_transformed.metadata.get("description"),
            "Content": doc_transformed.page_content,
            "language": doc_transformed.metadata.get("language"),
        }

        # This parses URL to determine the platform (domain) 
        # from which article was scrapped

        parsed_url = urlparse(link)
        platform = parsed_url.netloc

        # Next save everything to MongoDB

        user = kwargs["user"]
        instance = self.model(
            content=content,
            link=link,
            platform=platform,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Finished scrapping custom article: {link}")       
===========================================================
MediumCrawler

NOTE: since we use extract and a conditional at the beginnning
for each of these crawlers this should be abstracted out as well.
===========================================================
from bs4 import BeautifulSoup
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseSeleniumCrawler
class MediumCrawler(BaseSeleniumCrawler):
    model = ArticleDocument

    def set_extra_driver_options(self, options) -> None:
    	options.add_argument(r"--profile-directory=Profile 2")

    def extract(self, link: str, **kwargs) -> None:
	    old_model = self.model.find(link=link)
	    if old_model is not None:
	        logger.info(f"Article already exists in the database: {link}")
	        return
	    logger.info(f"Starting scrapping Medium article: {link}")
	    self.driver.get(link)
	    self.scroll_page()
	    # The only crawler using BeautifulSoup
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        title = soup.find_all("h1", class_="pw-post-title")
        subtitle = soup.find_all("h2", class_="pw-subtitle-paragraph")
        data = {
            "Title": title[0].string if title else None,
            "Subtitle": subtitle[0].string if subtitle else None,
            "Content": soup.get_text(),
        }

        # save to DB

        self.driver.close()
        user = kwargs["user"]
        instance = self.model(
            platform="medium",
            content=data,
            link=link,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Successfully scraped and saved article: {link}")

Linked in is a bit longer then the other two scrapers as it uses Beautiful soup
does a bulk_insert for all posts, logins() the user, find elements based on classes which in fact may change over time but the same idea applies for scrapping setting up model and saving

Since LLM core features is scrapping there are two other popular scrapping data:
Scrapy: https://github.com/scrapy/scrapy
Crawl4Ai: https://github.com/unclecode/crawl4ai
ORMs:
FastAPI SQLModel: https://github.com/fastapi/sqlmodel
SQLAlchemy: https://www.sqlalchemy.org/

It is best practice to structure data in classes instead of dictionaries, each item is more verbose, reducing run errors. 

ODM pattern is similar to ORM but instead it simply works with NoSQL

===========================================================
NoSQLBaseDocument

Nothing special in these methods just a wrapper for
DB management, the class methods here are self explaintory
similar to mongoengine
===========================================================
import uuid
from abc import ABC
from typing import Generic, Type, TypeVar
from loguru import logger
from pydantic import UUID4, BaseModel, Field
from pymongo import errors
from llm_engineering.domain.exceptions import ImproperlyConfigured
from llm_engineering.infrastructure.db.mongo import connection
from llm_engineering.settings import settings

_database = connection.get_database(settings.DATABASE_NAME)
T = TypeVar("T", bound="NoSQLBaseDocument")

# id field is defined as a UUID4, with a default factory generating a unique UUID
class NoSQLBaseDocument(BaseModel, Generic[T], ABC):
	id: UUID4 = Field(default_factory=uuid.uuid4)
	def __eq__(self, value: object) -> bool:
	    if not isinstance(value, self.__class__):
	        return False
	    return self.id == value.id
	def __hash__(self) -> int:
	    return hash(self.id)

	# The from_mongo() and to_mongo()
	# are what it says mongoDB->class, dictionary->mongoDB
	@classmethod
	def from_mongo(cls: Type[T], data: dict) -> T:
	    if not data:
	        raise ValueError("Data is empty.")
	    id = data.pop("_id")
	    return cls(**dict(data, id=id))
	def to_mongo(self: T, **kwargs) -> dict:
	    exclude_unset = kwargs.pop("exclude_unset", False)
	    by_alias = kwargs.pop("by_alias", True)
	    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
	    if "_id" not in parsed and "id" in parsed:
	        parsed["_id"] = str(parsed.pop("id"))
	    for key, value in parsed.items():
	        if isinstance(value, uuid.UUID):
	            parsed[key] = str(value)
	    return parsed

	def save(self: T, **kwargs) -> T | None:
	    collection = _database[self.get_collection_name()]
	    try:
	        collection.insert_one(self.to_mongo(**kwargs))
	        return self
	    except errors.WriteError:
	        logger.exception("Failed to insert document.")
	        return None

	@classmethod
	def get_or_create(cls: Type[T], **filter_options) -> T:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        new_instance = cls(**filter_options)
	        new_instance = new_instance.save()
	        return new_instance
	    except errors.OperationFailure:
	        logger.exception(f"Failed to retrieve document with filter options: {filter_options}")
	        raise

	@classmethod
	def bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:
	    collection = _database[cls.get_collection_name()]
	    try:
	        collection.insert_many([doc.to_mongo(**kwargs) for doc in documents])
	        return True
	    except (errors.WriteError, errors.BulkWriteError):
	logger.error(f"Failed to insert documents of type {cls.__name__}")
	        return False

	@classmethod
	def find(cls: Type[T], **filter_options) -> T | None:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        return None
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return None

	@classmethod
	def bulk_find(cls: Type[T], **filter_options) -> list[T]:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instances = collection.find(filter_options)
	        return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return []

	@classmethod
	def get_collection_name(cls: Type[T]) -> str:
	    if not hasattr(cls, "Settings") or not hasattr(cls.Settings, "name"):
	        raise ImproperlyConfigured(
	            "Document should define an Settings configuration class with the name of the collection."
	        )
	    return cls.Settings.name

=========================================================
Lastly concrete classes that define our data categories
=========================================================
from abc import ABC
from typing import Optional
from pydantic import UUID4, Field
from .base import NoSQLBaseDocument
from .types import DataCategory
from enum import StrEnum

class DataCategory(StrEnum):
    PROMPT = "prompt"
    QUERIES = "queries"
    INSTRUCT_DATASET_SAMPLES = "instruct_dataset_samples"
    INSTRUCT_DATASET = "instruct_dataset"
    PREFERENCE_DATASET_SAMPLES = "preference_dataset_samples"
    PREFERENCE_DATASET = "preference_dataset"
    POSTS = "posts"
    ARTICLES = "articles"
	REPOSITORIES = "repositories"

class Document(NoSQLBaseDocument, ABC):
    content: dict
    platform: str
    author_id: UUID4 = Field(alias="author_id")
    author_full_name: str = Field(alias="author_full_name")

class RepositoryDocument(Document):
    name: str
    link: str
    class Settings:
        name = DataCategory.REPOSITORIES
class PostDocument(Document):
    image: Optional[str] = None
    link: str | None = None
    class Settings:
        name = DataCategory.POSTS
class ArticleDocument(Document):
    link: str
    class Settings:
        name = DataCategory.ARTICLES

class UserDocument(NoSQLBaseDocument):
    first_name: str
    last_name: str
    class Settings:
        name = "users"
    @property
    def full_name(self):
        return f"{self.first_name} {self.last_name}"

-------------------------------------------------------------------------------
You often want to use the LLM on data it wasn’t trained on
Retrieval augmented generation (RAG)
- used to inject custom data into the LLM to perform a given action
- fine tuning LLM is highly costly, RAG bypasses need for constant fine-tuning
- summarize, reformulate, and extract the injected data

Retrieval: Search for relevant data
Augmented: Add data as context to the prompt
Generation: Augmented prompt with an LLM for generation
