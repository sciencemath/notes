FTI architecture
Feature Training and Inference

Feature Pipeline
Takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference, saved to a feature store

Training Pipeline
Takes the features and labels from the features stored as input and outputs a train model or models and puts them into a model registry

Inference Pipeline
Takes as input the features and labels from the feature store and trained model from the model registry to make predictions

Benefits of FTI:
- Just three components easy to understand
- Each component can be written into its own tech stack, so we can adapt them to specific needs, allows us to pick the best tools for the job.
- There is a transparent interface between the three components, each one can be developed by a different team
- every component can be monitored and deployed independently.

RAG (retrieval-augmented generation)
With any RAG-based system, one of the central peices of the infrastructure is a vector DB

LLM encapsulates and automates all the following steps:
- Data collection
- Data preprocessing
- Data storage, versioning, and retrieval
- LLM fine-tuning
- RAG
- Content generation evaluation
The key is to be data-centric and architecture model-agnostic so we can use different models on specific data.

run `pyenv local 3.11.8` then pyenv will always know to use that python version it creates a `.python-version` file

Poetry is a dependency and vitrual environment manager.
it saves all its requirements in `pyproject.toml`
creates a .lock file just like node package manager.
had used venv before but lacks the dependency management option

Poe the Poet is a plugin on top of Poetry
helps you define and run tasks within your Python project, simplifying automation and script execution.
Just like how we have commands in package.json example:

we've defined:
[tool.poe.tasks]
test = "pytest"

then run:
`poetry poe test`

ZenML acts as the bridge between ML and MLOps. It handles transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment.
ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.

An orchestrator is a system that automates, schedules, and coordinates all your ML pipelines.

ZenML works as an orchestrator via piplelines and steps, example:

from zenml import pipeline
from steps.etl import crawl_links, get_or_create_user
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> None:
	user = get_or_create_user(user_full_name)
	crawl_links(user=user, links=links)

To integrate ZenML with your code you have to write modular code, where each function does one thing (makes it easy to use `@step` decorators):

from loguru import logger
from typing_extensions import Annotated
from zenml import step
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument

@step
def get_or_create_user(user_full_name: str) -> Annotated[UserDocument, "user"]:
    logger.info(f"Getting or creating user: {user_full_name}")
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    return user

Have a directory of steps alongside pipelines on the root so we can swap orchestrators if needed to use a REST API

In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. We can transform anything into an artifact.

For web scraping its good to keep a config folder and a specific .yaml file that have all the URLs associated to (in this case) all posts/code from the same person. 

Keeping everything in pipeline config .yaml files is ideal

We can use Comet to track metrics and visualize them, track different config between experiements

Opik: prompt monitoring
Qdrant: vector database

SageMaker provides a comprehensive platform for building, training, and deploying machine learning models vs Amazon Bedrock which is just pretrained models accessed directly through an API an "out-of-box" solution.
Even SageMaker isn’t fully customizable. If you want complete control over your deployment use EKS AWS Kubernetes self-managed service.

We use an ETL pipline to extract data, transforming and cleaning data into a suitable format for storage and analysis, and load into our warehouse or db.

ETL and the feature pipeline strictly communicate trough the Mongo data warehouse. Feature will read and the ETL process will write acting independently.

using MongoDB as a data warehouse is uncommon, but we're using it for a small amount of data and its fine for our unstrucutred data (internet web articles), for production or dealing with tons of data we would use Snowflake or BigQuery

*Refer to code file*

Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave.
we can use this tool to login, scroll websites, click on different elements etc

*Refer to code file*

-------------------------------------------------------------------------------
You often want to use the LLM on data it wasn’t trained on
Retrieval augmented generation (RAG)
- used to inject custom data into the LLM to perform a given action
- fine tuning LLM is highly costly, RAG bypasses need for constant fine-tuning
- summarize, reformulate, and extract the injected data

Retrieval: Search for relevant data
Augmented: Add data as context to the prompt
Generation: Augmented prompt with an LLM for generation

LLM is bound to the data it was trained on (parameterized knowledge)
GPT-4o trained data up to Oct 2023, RAG overcomes these limits and provides access to external or latest data and prevents hallucinations.

RAG Solves:
Hallucinations (could tell us something that isn't true)
Old or private information (new data is generated every second!)

RAG system (3 modules):
Ingestion pipeline: batch or streaming pipeline to populate vectorDB
Retrieval pipeline: queries vector DB and retrieves relevant users input
Generation pipeline: uses retrieved data to augment the prompt and LLM to generate answers

ingestion pipeline (constantly updating)
user input->retrieval->generation->LLM->answer

Ingestion pipeline
---------------------------------------
gets data from many sources then cleans, chunks, and embeds
loads the embedded chunks into a vector DB

first data extraction, from DB APIs or webpages
second cleaning layer, depending on your data cleaning varies such as invalid/unwanted characters 
third chunking, splits cleaned documents into smaller ones. ensure it doesn't exceed the models input max size.
fourth embedding, takes the chunks content and project it into a dense vector packed with semantic value.
fifth loading, takes embedded chunks along with metadata the embedding is used as an index to query similar chnks. metadata is used to access the information added to augment the prompt

Retrieval pipeline
---------------------------------------
take user input (text, image, audio) embed it and query vector DB for similar vectors
project users input into the same vector space as the embeddings used as an index in the vector DB. We can then find the top K's most similar entried by comparing embeddings from the vector storage with the user's input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer

distance formula cosine distance is popular (others Euclidean, Manhatten)
1 - cos(θ) = 1 - (A ⋅ B)/(||A||⋅||B||)
ranges from -1 (opposite directions) to 1 (same direction), 0 (orthogonal)
distance between two vectors depends on data and embedding model used.
To avoid training-serving skew you must clean, chunk and embed the users input using the same functions, models, and hyperparameters, input and embeddings must be in the same vector space.

Generation pipeline
---------------------------------------
Last step take users input retrieve data, pass to LLM, generate answer. Usually all the prompt engineering is done at the prompt template level.
The final prompt populated with user's query and retrieved context

As the prompt templates evolve, each change should be tracked and versioned using machine learning operations (MLOps) best practices.
could use git or prompt management tools like LangFuse etc.

Emedddings is like a map where words with similar meanings are clustered together with numerical represenation encoded as vectors in continuous vector space.
Could be used in words, images or recommendation systems.

NLP (natrual language processing) embeddings translate words into vectors where semantically similar words are positioned close in vector space.

embeddings have more than 2 or 3 dimensions usually 64 to 2048 you must project them again to 2D or 3D.
t-SNE or UMAP uses transform high-dimensional reduction keeping geometrical properties between points, making it easier to visualize, interpret, and process while minimizing the loss of important information.
embeddings encode any categorical variable and you can feed it to an ML model.

One-hot can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.

Feature hashing (hash trick) maps categories into a fixed number of bins or buckets. risk of collisions, where different categories might map to the same bin, leading to a loss of information. It's also difficult to understand the relationship between the original categories and the hashed features.

Embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. Similar to techniques of a CNN

====================================================
example using Sentence Transformers Python package
(also in hugging face)
====================================================
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
sentences = [
"The cat sits outside waiting for a mouse.",
"I am going swimming.",
"The cat is swimming."
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# Output: [3, 384]
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# Output:
# tensor([[ 1.0000, -0.0389, 0.2692],
# [-0.0389, 1.0000, 0.3837],
# [ 0.2692, 0.3837, 1.0000]])
#
# similarities[0, 0] = The similarity between the first sentence and itself.
# similarities[0, 1] = The similarity between the first and second sentence.
# similarities[2, 1] = The similarity between the third and second sentence.
====================================================
you can pick between best-performing model or one with smallest memory footprint, descisions should be based on requirements, best to experiment

Models like CLIP, lets you embed a piece of text and an image in the same vector. This allows you to find similar images using sentence as input and vice versa
example:
====================================================
from io import BytesIO
import requests
from PIL import Image
from sentence_transformers import SentenceTransformer

response = requests.get(
"https://i.etsystatic.com/9632921/r/il/a3e46a/3070971627/il_1588xN.3070971627_5jbv.jpg"
)
image = Image.open(BytesIO(response.content))
model = SentenceTransformer("clip-ViT-B-32")
img_emb = model.encode(image)
text_emb = model.encode(
	[
		"A crazy cat smiling.",
		"A white and brown cat with a yellow bandana.",
		"A man eating in the garden."
	]
)

print(text_emb.shape)
# Output: (3, 512)
similarity_scores = model.similarity(img_emb, text_emb)
print(similarity_scores)
# Output: tensor([[0.3068, 0.3300, 0.1719]])

====================================================

A Vector db look for the closest neighbors of the query vector, it uses approximate nearest neighbor (ANN) algorithms using only approximations of the top matches for a given input query works well enough trade-off between accuracy and latency

workflow of a vector DB
1. indexing vectors, vectors are indexed using data structures optimized for high-dimensional data.
	- Hierarchiacal navigable small world (HNSW)
		- multi-layer graph where each node represents a set of vectors.
	- random projection
		reduces high-dimensonility of vectors by projecting into a lower dimensional space using a random matrix
	- product quantization (PQ)
		- dividing vectors into smaller sub-vectors and then quantizing these sub-vectors into representative codes.
	- locality-sensitive hashing (LSH)
		- maps similar vectors into buckets, fast approximate nearest neighbor searches by focusing on a subset of the data
2. querying for similarity (distance formulas)
3. post-processing results: refine accuracy, most relevant vectors are returned

Vector DB share common characteristics of a standard DB
- Sharding and replication
- Monitoring
- Access Control
- Backups

There are some limitiations in the vanilla RAG framework, we can improve this with an advanced RAG in these stages:
- Pre-retrieval index optimizations as well as query optimization
- Retrieval improving the embedding models and metadata filtering
- Post-retrieval filter out noise from retrieved documents and compress the prompt before feeding it to an LLM

Pre-retrieval
-----------------------------
- Data indexing cleaning or chunking modules to preprocess data for better indexing
- Query Optimization, algorithm performed directly on users's query before embedding it and retrieving chunks from vector DB

improve better preprocessing and structuring data:
- Sliding window
	- overlap between text chunks, ensuring important context near chunk bondaries is reatined, legal documents, scientific papers, customer support logs, medical records, critical information spans multiple sections. (maintains context)
- Enhance data granularity 
	- removing irrelevant details, factual accuracy, updating outdated information
- Metadata
	- dates, URLs, chapter markers (filter)
- Optimizing index structures
	- chunk sizes and multi-indexing strategies
- Small-to-big
	- decouples chunks for retrieval and context used in prompt for final answer generation, uses small sequence of text to compute the embedding while preserving the sequence and a wider window around it in the metadata, smaller chunks enhances the retrievals accuracy, the larger context adds more context info to the LLM
We do this to reduce the noise, or the text could contain multiple topics
- Query routing
	- Natural language is used to route queries if user input has multiple categories
- Query rewriting (reformulating question to match indexed information better)
	- Paraphrasing (e.g. "are there a lot of people that die from snakes?" could be written as "How many people die each year from snakes")
	- Synonym substitution less common words with synonyms to broaden the search (e.g. "gleeful" to "hppy")
	- Sub-queries break down long queries to mroe focused sub-queries
- Hypothetical document embeddings (HyDE) LLM creates hypotheticals, both original query and LLMs response are fed into retrieval stage
- Query expansion adding terms or concepts resulting in different perspectives for example you could search for "Zelda" but also include "Nintendo" or "Master Sword"
- Self-Query unstructured queries to structured ones

You'll have to experiment with each of these because it depends on your data

Retrieval
-----------------------------
- imporving the embedding models
- Leveraging the DB's filter and search features
both enhance the vector search step by finding similarity between query and indexed data

Instructor Models: (instead of fine-tuning the embedding model)
Tailoring your embedding network to your data using such a model can be a good option fine-tuning a model consumes more computing and human resources.

====================================================
from InstructorEmbedding import INSTRUCTOR

model = INSTRUCTOR("hkunlp/instructor-base")
sentence = "RAG Fundamentals First"
instruction = "Represent the title of an article about AI:"
embeddings = model.encode([[instruction, sentence]])

print(embeddings.shape)
# Output: (1, 768)
====================================================
python3 -m venv instructor_venv && source instructor_venv/bin/activate
pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1

classic filter and search DB filters

- Hybrid search A vector and keyword-based search blend, you have an alpha that controls weight between the two methods, two independent searches, later normalized and unified.
- Filtered vector search, metadata index to filter for specific keywords within metadata.


Post-Retrieval
-----------------------------
This step is to limit irrelevant information, or the context can be too large which can distract the LLM
- Prompt compression eliminate unnecessary details
- re-ranking use a cross-encoder ML model to give a matching score between the user's input and every retrieved chunk. Retrieved items are sorted based on this score. its costly to do this initial retrieval step.

Any RAG system is split into two independent components
- The ingestion pipeline
	- raw data, cleans, chunks, embeds, loads into vector DB
- The inference pipeline
	- queries vector db for relevant context generates by leveraging an LLM

Batch Pipelines (refer to data engineer notes)
-----------------------------
data is collected processed, and stored in intervals
- Data collection
	- data from DB, logs, files, other sources
- Scheduled processing
	- intervals hourly, daily, processed in bulk
	- cleansing, transformation, aggregation, and other operations
- Data loading
	- loaded into a target system, db, warehouse, data lake, feature store
	- available for analysis, querying, or further processing

Batching does not need immediate processing
- Efficiency
	- optimize resources, and parallel processing
- Complex processing
	- data transformation, and aggregations (which might not be suitable for real-time processing)
- Simplicity
	- simpler than real time, making them easier to implement and maintain

Streaming is well suited for social media, where behavior changes frequently. For example your looking at cats for 10 minutes on social media but you get bored and want to watch something more engaging so you switch to look at videos about educative content, the recommender system had to capture behavior system in real-time without delay. You couldn't do this with a batch system
- Fraud detection
- trading platforms

An ETL pipeline is a good approach for a batch system

Start with a batch architecture, after gradually move to a streaming design

Change data capture (CDC) allows to sync two or more data types in sync without computing and I/O overhead, it captures any CRUD operation done on the source DB and replicated on a target DB. Example: how to sync data warehouse with feature store
- Push
	- near instantaneous updates to the target, a messaging system is typically used as a buffer because data loss can occur
- Pull
	- passive role to the source db, only records changes, also needs a messaging system is essential to prevent data loss
Push is ideal for immediate data access, pull for large-scale data transffers where real-time updates aren't critical.

CDC patterns:
- Timestamp-based
	- add last_modified or last_updated time column, limited to tracking changes not deletions, imposes performance overhead (table scan)
- Trigger-based
	- tigger based approach, records data modification to a separate table upon INSERT, UPDATE, DELETE operations (event table), can impact db performance
- Log-based
	- transaction logs for all modifications w/ timestamps used for recovery, requires no schema modification, propagate changes to target system in real-time, minimizes performance impact on the source DB, lacks standardized log formats.

* Please note the following code is very step by step process *
its only to analyse the bigger picture, I put them here just to understand
a base example.

*Refer to code file*
----------------------------------------------------------
Supervised Fine-Tuning (SFT)
refines the model's capabilities using carefully curated pairs of instructions and corresponding answers. This teaches the model to understand and follow a specific chat format transforming it into a conversation, the model adapts its broad knowledge base to excel in targeted tasks or specialized domains.

Shapes LLMs behavior to align with specific goals.

Instruction datasets are defined as pairs of instructions and answers.

Here is an example from the SlimOrca dataset, with "system" and "instruction":
System
You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.

Instruction
Concepts: building, shop, town
Write a sentence that includes all these words.

Output
In our little town, there is a shop inside a big building where people go to buy their favorite toys and candies.

Once we have enough samples we only keep the high-quality:
- Accuracy
	- making sure responses are factually accurate
	- relevant to instructions
	- unlike FOX, CNBC, any media, we on the other hand want to be trustworthy
- Diversity
	- span different topics
	- contexts
	- writing styles
- Complexity
	- multi-step reasoning
	- complex real world problems

Hugging Face Hub contains numerous instruction datasets
The quality of the data is a crucial factor, and a high number of samples is always desirable.

Two types of finetunes: general-purpose, domain specific
Some fields, like medicine or law, may require as much data as general-purpose fine-tuning due to their vast technical corpora.

task specific models: collecting examples of the desired task from existing datasets or creating new ones.

Domain specific models: requires collaboration with subject matter experts to gather and validate relevant texts, research papers, technical documents, and other domain specific content.

few shot prompting: providing a few examples of the desired task within the input prompt.

Sometimes a blury line between task/domain models

rule based filtering relies on explicit predefined rules to filter data
- length filtering setting thresholds for length of responses in the dataset
	this can vary significantly depending on task/domain
- keyword exclusion focuses on content rather than their structure
	filter inappropriate content, low quality terms (profanities, spam, off topic).
- Format checking structured data follow specific formatting to maintain consistency
	code samples that are syntactiaclly correct and follow a specific style

Predefined rules may lack the nuance required to capture full complexity of language and context could remove valid but unusual samples, rules need to always be reviewed because of the nature of data and quality standards evolve

Data deduplication
----------------------------------------------------------
issues with duplicates or near-duplicates:
	- Overfitting
	- Biased performance
	- Inefficient training
	- Inflated evaluation metrics

Exact deduplication removes identifical sample through
	- data normalization
	- hash generation
	- duplicate remove

Fuzzy deduplication is MinHash deduplication (most popular)
generates compact representations or signatures for each data item, transforms data items into sets of shingles, applies multiple hash functions then selects the min has values to form signature vectors. These signatures are compared using similarity measures like Jaccard similarity to find near-duplicates

Semantic similarity
focuses on meaning of text for dedup, words -> vector representations using natural language processing, Word2Vec, GloVe, and FastText, words -> dense vectors

Context-aware representations like BERT, sentence transformers, or cross-encoders can generate embedding for entire sentences or documents. dedup can be performed by comparing similarity between vectors (cosine distance). Clustering techniques may be applied to group similar vectors (K-means and others).

Data decontamination
----------------------------------------------------------
Preventing overfitting or memorization of test data by ensuring that the training dataset does not contain identicals or highly similar to those in the evaluation or test sets.

Remove similar training samples that are similar to evaluation samples could involve using MinHash or computing scores based on n-grames or embeddings

Add your evaluation set to the instruction dataset during the data deduplication stage. Automatically add your evaluation sets in the data deduplication stage to fully automate this process.

----------------------------------------------------------
LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. Example:

====================================================
Instruction
You are a data quality evaluator. Your goal is to assess an instruction and its corresponding answer, determining how effectively the answer addresses the given task.
In your evaluation, you will provide feedback detailing the strengths and weaknesses of the answer, followed by a score on a scale of 1 to 4.
A score of 1 means that the answer is terrible and irrelevant to the instruction.
A score of 2 means that the answer is not helpful and misses important aspects of the instruction.
A score of 3 means that the answer is helpful but could be improved in terms of relevance, accuracy, and depth.
A score of 4 means that the answer is excellent and fully addresses the task.
Provide your evaluation as follows:
Feedback: (strengths and weaknesses you find relevant)
Score: (number between 1 and 4)
====================================================
LLM-as-a-judge is known to have several biases
- Position bias in comparative scoring
	Favors first answer, randomize the order of answers A and B
- Favors long answers
	length normalizations can be applied
- intra-model favoritism
	- prefer models from the same family
Using multiple LLMs as a jury reduced bias and improve consistency.
Reward models are another way to re-purpose LLMs for data quality take an instruction and answer pair and return a score as output. Score could be multiple scores helpfulness, correctness, coherence, complexity etc.

Hugging Face is a good resource for comparing different reward models.

Classifiers or encoder-only models can be trained to perform data quality evaluation.
At smaller scale, encoder-only models are still valuable to filter out outliers or as part of an automated data pipeline, which requires faster processing.

Manual dataset exploration, is time consuming (important step). It reveals errors and inconsistencies that automated processes might miss, including formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies.

Argilla A collaborative platform for manual data quality evaluation and exploration.

Statisitcal analysis reveals vocab diversity, potential biases, concept representation. Utilizes natrual language processing libraries for tokenization and analysis of large text volumes. Visualization tools such as matplotlib or Seaborn uses histograms and word clouds.

Topic clustering reveals patterns

Synthetic data generation using LLMs offers a more efficient and scalable alternative than crowdsourcing. Process begins with preparation of a set of carefully designed promprts (taxonomy).
Useful for addressing biases and gaps in existing datasets. its possible to create more balanced and inclusive datasets that represent a wider range of perspectives, topics, and language styles.

Data augmentation
---------------------------------------------------------
In-depth evolving enchancing the complexity of existing instructions
	- Constraints
		introducing additional requirements or limitations to the original instruction
	- Deepening
		finds more deep questions, requiring more comprehensive repsonses
	- Concretizing
		replaces general concepts with more specific ones, adding detail to the instructtion
	- Increasing reasoning steps
		explicity requires multiple-step reasoning promoting more complex problem solving
	- Complicated input
		adding more complex data formats or structures to the instruction code snippets, JSON, LaTeX

In-breadth evolving expands the diversity of the instruction dataset

UltraFeedback method focused on answer quality instead of instruction quality. 

LLMs are not reliable when it comes to producing structured output.
Structured generation is an effective method to force an LLM to follow a predefined template, such as JSON, pydantic classes, or regular expressions.


====================================================
Hugging Face Hub provides a convenient dataset viewer
Instruction  |  Output

SFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instructions and answers. 
	- Low-Rank Adaptation (LoRA)
	- Quantization-aware Low-Rand Adaptation (QLoRA)

Instead of building applications around a chatbot, fine-tuning allows developers to create more diverse interactions with LLMs, like tool analytics, moderation, and additional context. 

Everything has limitations
fine-tuning also has limitations
 - knowledge that is too distant from what has been learned in the pre-training set (such as an unknown or rare language) can be difficult to learn effectively.

fine-tuning a model on new knowledge could result in more frequent hallucinations

Chat templates offer a unified way to present the instructions and answers to the model. Special tokens to identify the beginning and the end of a message, and author of the message. Base models are not designed to follow instructions so you can choose any template. Not recommended to fine-tune an instruct model

example:
====================================================
<|im_start|>system
You are a helpful assistant, who always provide explanation. Think like you are answering to a nine year old.<|im_end|>
<|im_start|>user
Concepts: cat, bed, play
Write a sentence that includes all these words.<|im_end|>
<|im_start|>assistant
Before bed I like to play with my cat.<|im_end|>
====================================================
Now understands that the next tokens should be an answer relevant to the user instruction and guided by the system prompt.
This is how fine-tuned models acquire instruction following capabilities.

issue with chat templates:
Whitespace and line break is very important. Adding or removing any character would result in a wrong tokenization, which negatively impacts the performance of the model. Use something like Jinja. different types of templates
	- Alpaca
	- ChatML
	- Llama3
	- Phi-3
	- Gemma

Full fine-tuning
---------------------------------------------------------
Re-training every parameter in the base model.
SFT uses next-token prediction as its training objective.
This method often provides the best results but requires significant computational resources.

Memory = Parameters + Gradients + Optimizer States + Activations

- Paramters
	Learnable weights and biases within a neural network
- Gradients
	Partial derivatives of the loss function with respect to each model parameter
	minimize loss
	computed for each parameter backwards propagation
- Optimizer States
	Additional values maintained by optimization algorithms (Adam or AdamW)
- Activations
	outputs of each layer during foward pass
	kept in memory to compute gradients in backward pass

How to reduce memory usuage during LLM fine-tuning?
- Model parallelism (adds some overhead)
- Gradient accumlation (enables larger batch sizes)
- Optimizers like 8-bit Adam
- Activation checkpointing (trades computation for memory)

Full fine-tuning directly modifies the pre-training weights, which makes it destructive by nature. If training doesn’t behave as expected, it might erase previous knowledge and skills "catastrophic forgetting."

LoRA (fine-tuning)
---------------------------------------------------------
Enabling fine-tuning of LLMs with significanlty reduced computational resources.
trainable low-rank matrices that modify behavior of the model without changing original parameters
- Dramatically reduced memory during training
- faster fine-tuning
- preservation of pre-trained model weights (non-descructive)
- ability to switch between tasks efficiently by swapping LoRA weights

W' = W + BA
W is original weight matrix
B and A are LoRA matrices
W' effective weight matrix used during inference

A and B are chosen to have the same shape as W but with lower rank
this rank is denoted as r
original W remains frozen, and only A and B are updated

Hyperparameters:
- Rank (r)
	size of LoRA matrices common to start r=8
- Alpha (a)
	requires experimentation, this is a scaling factor a/r. Its common to set this valuie twice the value of r

Can add a drop out layer between 0 - 0.1 as an optional regularization factor

LoRA is primarily focused on query (Q) and value (V) in the transformer layers
additional targets:
- Key (K) matrices in attention layers
- Output projection layers (O)
- Feed-forward or multi-layer perceptron (MLP) between attention layers
- linear output layers

LoRA could produce better results than full-tuning
LoRAX multiple-LoRA serving
A feature supported by Hugging Face's Text Generation Inference (TGI) and Nvidia Inference Microservices (NM)

QLoRA (fine-tuning)
---------------------------------------------------------
fine-tune models on relatively small, widely available GPUs

quantizing the base model parameters to a custom 4-bit NormalFloat (NF4) data type
QLoRA indroduces small, trainable low-rank matrices (adapters) to specific layers
only these adapters are updating during training.
double quantization, quantizes the quantization constants
uses page optimizer to manage memory spikes during training Nvidia's unified memory feature

significant memory saves compared to LoRA
usually benefical when memory constraints are a primary concern

Difference between the two should be on the specific requirements of the project, available hardware, memory usage, speed, performance.

Learning rate and scheduler
---------------------------------------------------------
A hyperparameter, it controls how much the model's parameters are updated during training, ranges from small values: 1e-6 to 1e-3 a common starting point is 1e-5, if learning rate to low it may get stuck, to high it might diverge

Using a learning rate schedule often leads to faster convergence and better performance than using a fixed learning rate.

Learning rate schedule adjusts learning rate throughout the training process, start with higher learning rate then gradually decreases in later stages, two common types of schedulers:
- Linear
	decreases the learning rate steadily over time
- Cosine
	decresing slowly at first then more rapid toward end of training
both have same level of performance

Warmup and Decay:
During the initial stages, the model is far from optimal. A small or gradually increasing learning rate helps the model explore the solution space without instability.

Later in training, the model is closer to an optimal solution. A smaller learning rate allows for more precise adjustments to the parameters.

(LR: Learning Rate)
Warmup:
for 5% of training steps learning rate increases linearly:
LR = (stepnumber/warmupsteps) x initial LR

Decay:
remaining 95% the learning rate descreases following a chosen schedule (e.g. exponential decay):
LR = initial LR x exp(-(stepnumber - warmupsteps/decay constant))

used in large-scale pretraining (e.g. transformers)
finetuning on specific tasks or datasets

Batch size
---------------------------------------------------------
Number of samples processed before the model's weights are updated
range from 1-32 common values 1,2,4,8 or 16 larger batch sizes lead to more stable gradient estimates and can improve training speed, as they provide a better approximation of the true gradient of the entire dataset, requires more memory.

gradient accumulation can be used to mitigate memory constraints from larger batch sizes. Performs multiple forward and backward passes with smaller mini-batches, accumulating the gradients. More accumulation steps allow for larger effective batch sizes but increase the time required for each update.

Effective Batch Size = Batch Size x #GPUs x Gradient Accumulation Steps
say your using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accumulation steps you get 4 * 2 * 4 = 32 samples

Max length and packing
---------------------------------------------------------
longest input the model can process, this is set between 512 and 4096 tokens but can go up to 128,000+ max length of 2,048 tokens is common, RAG applications can go up to 8,192+

Longer input are truncated, this can happen to the left or right

Packing combines multiple smaller smaples into a single batch, increasing the amount of data processed in each iteration. If max sequence length is 1024 tokens but many of your samples are only 200-300 tokens long packing allows you to fit 3-4 samples into each batch slot. This improves training efficiency.

Packing requires careful implementation to ensure that model attention doesn't cross between packed samples. This can be mitigated by attention masks from attending to toakes from different sampels within the same packed sequence

Number of Epochs
---------------------------------------------------------
Number of complete passes through the entire training dataset.
LLM fine-tuning range 1-10 epochs, too few underfit, to much overfit.
this will depend on model, datasize, complexity

Optimizers
---------------------------------------------------------
Adjusts the model's parameters to minimize the loss function,
for LLM AdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended
combines adaptive learning rates with weight decay

AdaFactor is designed for memory efficency, it may not always match AdamW performance
if max performace is needed the non-quantized adamw_torch optimizer may be the best.

Weight decay
---------------------------------------------------------
You'll have to experiment with values but this works by adding a penalty for large weights to the loss function, encouraging the model to learn simpler, more generalizable features. Can improve performance on unseen data. 0.01 to 0.1

Gradient checkpointing
---------------------------------------------------------
Reduces memory consumption during training by storing a subset of intermediate activations by forward pass. its standard that all intermediate activations are retained in memory to facilitate gradient calculation during the backward pass.

selectively saving activation at specific layers within the network, if not saved they are re-computed during the backward pass for gradient computation. Has a trade-off between computation time and memory usage.

It may increase overall computation but reduces memory

---------------------------------------------------------

To select the most relevant LLM, we need to consider:
- License
- Budget
- Performance

Specialized tools and libraries to fine-tune models:
- TRL (popular)
	Hugging Face to train LLMs using SFT and perference alignment
- Axolotl
	fine-tune LLM with reusable YAML config files
- Unsloth
	uses custom kernels to speed up training (2-5x) and reduce memory by 80%
	only available for single-GPU settings.

====================================================
Using Unsloth
====================================================
import os
import torch
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets
from transformers import TrainingArguments, TextStreamerfrom unsloth import FastLanguageModel, is_bfloat16_supported

max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Meta-Llama-3.1-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=False, # using LoRA instead of QLoRA
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    lora_alpha=32,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)
# we have to load in more samples and only take 10000 samples
dataset1 = load_dataset("mlabonne/llmtwin")
dataset2 = load_dataset("mlabonne/FineTome-Alpaca-100k", split="train[:10000]")
dataset = concatenate_datasets([dataset1, dataset2])

# format data using a chat template, we manually add EOS (end of sentence)
# to ensure that the model learns to output it, otherwise it will keep generating answers without stopping
alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{}
### Response:
{}"""
EOS_TOKEN = tokenizer.eos_token
dataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)

# training 95%, 5% test
dataset = dataset.train_test_split(test_size=0.05)

# SFTTrainer stores all hyperparameters
trainer = SFTTrainer(
    model=model,
   tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=True,
    args=TrainingArguments(
        learning_rate=3e-4,
        lr_scheduler_type="linear",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        warmup_steps=10,
        output_dir="output",
        report_to="comet_ml",
        seed=0,
    ),
)
trainer.train()

# empty answer format() this forces model to answer the instruction instead of completing it
FastLanguageModel.for_inference(model)
message = alpaca_prompt.format("Write a paragraph to introduce supervised fine-tuning.", "")
inputs = tokenizer([message], return_tensors="pt").to("cuda")
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)
====================================================
Answer:
====================================================
Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's 
responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.
====================================================
# we can save our model locally and/or push to Hugging Face hub
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
model.push_to_hub_merged("MemoryOverload/TwinLlama-3.1-8B", tokenizer, save_method="merged_16bit")

You can use comet to visulaize different metrics
- Training Loss
	How well is the model doing on the task its being trained for
- Validation Loss
	measures the loss using the validation set instead of training set
	if taining loss continues to decrease while validation loss increase we overfitted
	vise versa underfitting 
- Gradient norm
	magnitude of the gradient vector during training
	A stable or decreasing gradient norm means the model is converging towards local optimum
	to mitigate large gradient norms use gradient clipping
---------------------------------------------------------

Preference alignment addresses shortcomings of SFT by adding direct human or Ai feedback into the training process

DPO datasets (Direct preference optimization)
Each instruction is paried with one preferred answer and one rejected answer, the objective is to train the model to generate the perferred response.

- Chatbots
	responses depends on subjective factors like naturalness, engagement, and context.
- Content moderation
	is content appropriate? Preference dataset can learn borderline cases
- Summarization
	is it concise, relevant, and coherent. Preference can learn to generate summaries
	SFT might result in summaries taht are correct but less perferable to humans
- Code generation
	multiple correct solutions, some more readable. Preference help model learn code quality
- Creative writting
	this is highly subjective, Preference datasets can capture human judgements about style, creativity, and emotional impact
- Translations
	Preference datasets can help produce translations that native speakers prefer

This is not only technically accurate but also better aligned with huiman judgment and preferences in complex open-ended tasks.

Data quantity
---------------------------------------------------------
Large number of preference pairs is beneficial for data quality
Nvidia and Meta are converging on similar post-training pipelines involving multiple rounds of perference alignment and extensive use of synthetic data.

DPO is less destructive than SFT and has a milder impact on the final model.

When creating preference datasets, data generation and evaluation are closely linked. We create answers and then rate them to make final dataset

Generating preferences
---------------------------------------------------------
its good to look at relevant open-source datasets before making new preference data
a well known one is the Anthropic HH-RLHF dataset, which has human preferences for helpful and harmless Ai responses and the OpenAI Summarize from Human Feedback dataset focuses on article summaries

DPO datasets can be created using various methods
- Human generated, human-evaluate datasets
	extermely resource-intensive and difficult to scale, involves hiteing people, but ideal for complex tasks
- Human generated, LLM-evaluated datasets
	useful if you have a lot of existing human-generated content, requires significant human input for response generation
- LMM generated, human-evaluated datasets
	good balance between quality and efficiency. LLMs generate responses to prompts and humans rank these. This is perferred method, humans are better judging answers than writing from scratch, may not provide creative or unexpected responses
- LLM generated, LLM-evauluated
	fully synthetic datasets, increasingly common due to their scalablility and cost-effectiveness. massive datasets quickly and improves as LLM capabilities advance

`Intel/orca_dpo_pairs` dataset on hugging face hub is an instruction-following Ai. Datasets like this are key to building models similar to OpenAI’s ChatGPT or Anthropic’s Claude, where human preferences play a crucial role in defining the quality of the output.

`Intel/orca_dpo_pairs` provides a foundational building block for preference-based tuning!

Introducing variability in the outputs is another crucial aspect of generating synthetic preference datasets. You can do this by manipulating the temperature settings or trying other sampling methods in the LLM. Higher tempature settings tend to produce more creative and diverse responses, lower settings result in more focused and deterministic outputs which creates a trade-off, depends also on the data you want to generate

Using multiple LLMs to generate samples can be better than just one model introduces variety. used by datasets like `argilla/Capybara-Preferences` combining GPT-4 with open weight models. The evaluation process selected the chosen and rejected answers

Pairwise ranking involves presenting the LLM with two responses and asking it to choose the better one or rank them
example of two approaches:
====================================================
"Rate the following response on a scale of 1-5 based on relevance, coherence, and helpfulness: [INSERT RESPONSE]." For pairwise ranking, the prompt could be: "Compare the following two responses. Which one is better in terms of relevance, coherence, and helpfulness? Response A: [INSERT RESPONSE A] Response B: [INSERT RESPONSE B]."
====================================================
Pairwise ranking is an ideal approach more closely correlated to human judgment than absolute scoring.

chain-of-thought reasoning encourges evaluating LLM to condsider multiple aspect of responses and articulate its decision making process leading to more thorough evaluations.

LLM-as-a-judge prompt to perform pairwise ranking:
====================================================
Instruction

You are an answer judge. Your goal is to compare answer A and answer B. I want to know which answer does a better job of answering the instruction in terms of relevance, accuracy, completeness, clarity, structure, and conciseness.
Instruction: {instruction}
Answer A: {answer_a}
Answer B: {answer_b}
Explain your reasoning step by step and output the letter of the best answer using the following structure:
Reasoning: (compare the two answers)
Best answer: (A or B)
====================================================
introduces bias
- Position bias
	LLM judges prefer first answer
- Length bias
	LLM prefers longer answers (overlooking shorter concise responses)
- Family bias
	LLM favor responses generated by themselves or models from same family

You can randomize the order, provide few-shot examples that demonstrate a balance distrubition of scores also can employ multiple models as a jury rather than relying on a single LLM judge

*Refer to code file*

Preference alignment regroups techniques to fine-tune models on preference data.

Reinforcement Learning from Human Feedback (RLHF) combines reinforcement learning (RL) with human input to align models with human preference and values.

RLHF works iteratively improving both a reward model and policy
- Reward model learning
	usually uses a Bradley-Terry model to train reward model. Instead of using a pre-defined reward function RLHF learns a reward model from human feedback
- Policy optimization
	standard RL algorithms can be used to optimize a policy generates new behaviors to maximize the predicted rewards
- Iterative improvement
	As policy improves it generates new behaviors

A popular RLHF algorithm is Proximal Policy Optimization (PPO) algorithm
Reward is regularized by Killback-Leibler (KL) divergence ensuring distribution of tokens stays similar to the model before training (frozen model)

Direct Preference Optimization: Your Language Model is Secretly a Reward Model (2023 paper)

DPO expresses the prefernece learning problem directly in terms of policy, eliminating the need for a separate reward model or complex reinforcement learning algorithms

DPO can be implemented as a simple binary cross-entropy loss function that operates directly on the language model's output probabilities. The model assigns higher prob to preferred responses and vise versa. The reference model is controlled via beta parameter between 0 and 1. This allows optimization using gradient descent, wihtout the need for sampling from model during training or implementing complex RL algorithms

DPO matches performance of more complex RLHF methods
Both RLHF and DPO benefit significantly from the integration of synthetic data
As LLMs become more capable they can generate data the surpasses human-created content in quality and diversity.
There may be scenarios where the added flexibility of RLHF is beneficial such as complex tasks.

DPO is ideal in most cases

Implementing DPO
====================================================
from unsloth import PatchDPOTrainer

PatchDPOTrainer()

import os
import torch
from datasets import load_dataset
from transformers import TrainingArguments, TextStreamer
from unsloth import FastLanguageModel, is_bfloat16_supportedfrom trl import DPOConfig, DPOTrainer

max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="mlabonne/TwinLlama-3.1-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=False,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    lora_alpha=32,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)

dataset = load_dataset("mlabonne/llmtwin-dpo", split="train")

alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{}
### Response:
"""
EOS_TOKEN = tokenizer.eos_token
def format_samples(example):
    example["prompt"] = alpaca_template.format(example["prompt"])
    example["chosen"] = example['chosen'] + EOS_TOKEN
    example["rejected"] = example['rejected'] + EOS_TOKEN
    return {"prompt": example["prompt"], "chosen": example["chosen"], "rejected": example["rejected"]}
dataset = dataset.map(format_samples)
dataset = dataset.train_test_split(test_size=0.05)

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    tokenizer=tokenizer,
    beta=0.5,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    max_length=max_seq_length//2,
    max_prompt_length=max_seq_length//2,
    args=DPOConfig(
        learning_rate=2e-6,
        lr_scheduler_type="linear",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=8,
        num_train_epochs=1,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        optim="adamw_8bit",
        weight_decay=0.01,
        warmup_steps=10,
        output_dir="output",
        eval_strategy="steps",
        eval_steps=0.2,
        logging_steps=1,
        report_to="comet_ml",
        seed=0,
    ),
)
trainer.train()

FastLanguageModel.for_inference(model)
message = alpaca_template.format("Write a paragraph to introduce supervised fine-tuning.", "")
inputs = tokenizer([message], return_tensors="pt").to("cuda")
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)
====================================================
Response
====================================================
Supervised fine-tuning is a method used to enhance the performance of pre-trained language models by utilizing labeled data. This technique involves taking a pre-trained model and refining it on a specific task, such as content creation or customer service. By providing the model with relevant data and guidance, it can learn to generate outputs that align more closely with the desired outcomes. This approach allows for the creation of more specialized models that can tackle complex tasks with greater accuracy and efficiency.
====================================================
SFT model:
====================================================
Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.
====================================================

DPO is more accurate and closer to the desired writting style
its less formal, closer to preference

save model:
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")

we want to measure the same metrics as before but with two additional ones
- Rewards
	chosen and rejected, mean difference between the log probabilities output by the trained and reference models, the gap between chosen answers and rejected answers should increase this is tracked by the margins metric, should quickly increase then plateau
- Accuracies
	percentage of times the model correctly identifities the chosen answers, if this has an accuracy of 100% the dataset might be too easy for the model, add more challenging examples!
---------------------------------------------------------

There is no unified approach to measure LLM model's performance, we can use multiple-choice question answering, open-ended instructions and feedback from real users.

General purpose evaluations are most popular ones
- Massive Multi-Task Language Understanding (MMLU)
- LMSYS Chatbot Arena

Model evaluation is to asses the capabilities of a single model without any prompt engineering, RAG pipeline, etc. This is important to make sure the fine-tuning process improved the model.

ML is concerned how accurately and efficiently a model can process structured data, tasks like prediction, classification and regression
LLM is concerned with a well a model understands and generates language

ML models are designed for narrowly defined problems which involve numerical or categorical data.
LLM interprets lanugage which adds a layer of subjectivity to the evaluation process, incorporates qualitative assessments.

Three key differences that impact evaluation
- Numerical metrics
	ML: accuracy, percision, recall, mean square error (MSE)
	LLM: rarely rely on the same numerical metrics
- Feature engineering
	ML: manually selecting and transforming data features before training
	LLM: handles raw text directly
- Interpretability
	ML: easy to interpret why a model made certain predictions
	LLM: have to request explanations during the generation process
task-specific LLMs are more closely aligned with traditional ML

We can categorize general purpose evaluations:
- pre-trianing
- after pre-training
- after fine-tuning

During pre-training:
- training loss
- validation loss
- perplexity (how "surprised" the model is by the data)
- gradient norm
Could include benchmarks liek HellaSwag (common sense reasoning) but could overfit

After pre-training (public pre-training evaluations)
- MMLU (knowledge)
	tests models on mutliple-choice questions across 57 subjects
- HellaSwag (reasoning)
	Complete a given situation with the most plausible ending from multiple choices
- ARC-C (reasoning)
	grade-school-level multiple choice science questions requiring causal reasoning
- Winogrande (reasoning)
	assesses common sense reasoning in carefully crafted sentences
- PIQA (reasoning)
	measures physical common sense understanding through questions about everyday physical interactions

improving an MMLU score of a base model by 10 during fine-tuning is unlikely, means the data might be contaiminated.

instruction-following, multi-turn conversation
- IFEval (instruction following)
	assesses model's ability to follow instructions w/ particular constraints, like not outputting hyphens in your answer
- Chatbot Arena (conversation)
	humans vote for the best answer to an instruction, comparing two models in head-to-head conversations
- AlpacaEval (instruction following)
	automatics evaluation for fine-tuned models
- MT-Bench (conversation)
	multi-turn conversation, testing ability to maintain context and provide coherent responses
- GAIA (agentic)
	tests wide range of abilities liek tool use and web browsing
if you want to fine-tune a model you want the best base model in terms of knowledge and reasoning

Benchmarks are a good way to compare different instruct models, not a single source of truth but should be used as signals

Domain-specific are more fine-grained
evaluations on the Hugging Face Hub:
- Open Medical LLM
	Evaluates the performance of LLMs in medical question-answering tasks collection of well known medical, clinical, college, biology sources
- BigCodeBench
	Evaluates the performance of LLMs for code completion based on docstrings covers a wide range of programming scenarios
- Hallucinations
	Evaluates LLMs' tendency to produce false or unsupported information
- Enterprise Scenarios
	Evaluates LLMs on six real-world enterprise use cases, specific capabilites.
	PII, customer service, legal, financial, creative writting, toxic

Domain-specific, and general-purpose evaluations should be complex and challenge models to distinguish good and bad outputs, they should be diverse and cover many topics and scenarios as possible, and should be easy to run.

run your benchmarks with these:
Eleuther AI and lighteval

Domain-specific and general-purpose evaluations idicate a strong base or instruct model but not insights into how well these models work for a given task

Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric measures the overlap between the generated text and reference text using n-grams

Classification tasks
- Accuracy
	is there a clear distinction between right and wrong answers, named entity recognition (NER)
- Precision
	ratio of true positive predicitions to total positive predictions
- Recall
	ratio of true positive predicitions to total actual positive instances
- F1 Score
	harmonic mean of precision and recall used to balance both metrics

When a task cannot be directly mapped to a traditional ML task, it is possible to create a custom benchmark. A common pattern is multiple choice
The model generatea a letter (A,B,C, or D) which is checked against correct answer.
Or have it as probabilities based evaluations

LLM-as-a-judge can be used to evaluate the quality of the answers
====================================================
You are an evaluator who assesses the quality of an answer to an instruction.
Your goal is to provide a score that represents how well the answer addresses the instruction.
You will use a scale of 1 to 4, where each number represents the following:
1. The answer is not relevant to the instruction.
2. The answer is relevant but not helpful.
3. The answer is relevant and helpful but could be more detailed.
4. The answer is relevant, helpful, and detailed.
Please provide your evaluation as follows:
##Evaluation##
Explanation: (analyze the relevant, helpfulness, and complexity of the answer)
Total rating: (final score as a number between 1 and 4)
Instruction:
{instruction}
Answer:
{answer}
##Evaluation##
Explanation:
====================================================

RAG evaluation is more comprehensive because it considers external information and models generative abilities
- Retrieval accuracy
	how well fetch relevant information
- Integration quality
	how well is the retrieved info incorporated into the response
- Factuality and relevance
	does final output address the query while blending retrieved and generated content?

Retrival-Augmented Generation Assessment (Ragas)
open-source toolkit designed to provide developers with tools for RAG evaluation and optimization, track impact over time, its designed around the idea of metrics-driven development (MDD) 

Manually creating hundereds of questions, answers and contexts in time consuming and labor intensive. Ragas ability is to generate diverse and complex test dataset. It uses an evolutionary approach, can generate conversational samples

Ragas metrics:
- Faithfullness
	score is calculated as the ratio of verifiable claims to the total number of claims in the answer
- Answer Relevancy
	generates multiple questions based on the answer and then calculates the mean cosine similarity between these generated questions and the origin questinos, it helps identify answers that may be correct but off-topic
- Context percision
	evaluates all the ground-truth relevant items present in the contexts are ranked
- Context recall
	measures extent to which the retrieved context aligns with the annotated answer (ground truth)

ARES (an automated evaluation framework for RAG systems)
A tool design to evaluate RAG systems
- synthetic data generation (mimics real-world scenarios)
- classifier training
- RAG evaluation
offers flexible model execution options, supporting both cloud-based and local runs through vLLM integration.

Ragas and ARES compliment each other. Ragas strength in production monitoring and LLM-assisted metrics combined with ARES's highly configurable evaluation process and classifier based assessments.
Combining offers quick iterations with Ragas and in-depth, customized with ARES at key stages.

Generate Answers
*Refer to code file*
reviewing answers is not scalable it is a crucial step that can help to identify a lot of common mistakes

Manual review of generated answers we can detect mistakes and identify areas for improvement, iterating over different datasets and models will allow us to significantly outperform our baseline and create the best possible model

---------------------------------------------------------

decoder-only architecture is designed for text-generation tasks. Predicts the next word in a sequence based on preceding words.

encoder-only architecture like BERT, focuses on understanding and representing the input text with detailed embeddings. comprehensive context understanding such as text classification and named entity recognition

the dual encoder-decoder is powerful for sequence-to-sequence tasks like translation and summarization.

inference process for decoder-only model:
- Tokenizing
	passing input prompt through an embedding layer and positional encoding.
- Computing
	key/value pairs for each input token using multi-head attention
- Generating
	tokens are sequnetially generated one at a time, using the computed key/values
Steps 1 & 2 are computational expensive, highly parallelizable matrix multiplication can utilize GPU/TPUs hardware accelerators

KV cache
LLMs generate text token by token which is slow because each new prediction depends on the entire previous context. When predicting the 100th token in a sequence the model needs the context of tokens 1-99 when predicting the 101th token it again needs the information 1-99 plus 100

key-value (KV) cache addresses this issue by storying key-value pairs produced by self-attention layers. instead of re-calculation we fetch from cache

KV cache is implemented in every popular tool and library
size(KV cache) = (2ntokens)(nlayers)(nheads)(dimhead)(nbytes)

Configure a model to use a static KV cache with the transformers lib
====================================================
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "google/gemma-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id) 
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

model.generation_config.cache_implementation = "static"
compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer("What is 2+2?", return_tensors="pt").to(device)

outputs = model.generate(**inputs, do_sample=True, temperature=0.7, max_new_tokens=64)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['What is 2+2?\n\nThe answer is 4. 2+2 = 4.']
====================================================

batching or processing multiple inference requests simultaneously is standard approach to achieve high throughput

in-flight batching is continuous batching it prevents idle time, this is implemented natively is most frameworks

speculative decoding (assisted generation) uses spare compute capacity to predict multiple tokens simultaneoulsy using smaller proxy model

- Apply a smaller model like a pruned version of the main model to predict mutliple token completions in parallel, 5-10 tokens predicted in a single step
- Put sepculative completions into full model to validate which predictions match what the large model would have generated
- Discard any incorrect tokens and retain longest matching prefix from the speculative completions

If small model predicts large model well, mutliple tokens can be generated in a single step, this saves compute time on large expensive model, both models have to use the same tokenizer.

Speculative decoding
(small example might not be noticable)
====================================================
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id) 
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
draft_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", device_map="auto")

device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer("What is 2+2?", return_tensors="pt").to(device)

outputs = model.generate(**inputs, do_sample=True, assistant_model=draft_model, temperature=0.7, max_new_tokens=64)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['What is 2+2? 2 + 2 equals 4!']

outputs = model.generate(**inputs, prompt_lookup_num_tokens=4)
====================================================

Speculation heads are components in a neural language model designed to predict multiple possible next tokens or outcomes rather than focusing solely on the most probable one. They encourage the model to explore diverse paths during prediction, which can be valuable for applications where uncertainty, creativity, or ambiguity is essential.
- Diversity in Output
- Risk-Aware Predictions
- Uncertainty Representation

Medusa freezes main model, and has dedicated speculation heads

Transformer architecture is based on the attention mechanism, which scales quadratically with the number of input tokens

PagedAttention supports memory sharing across multiple output sequences generated from the same prompt.

FlashAttention-2 divides input and output matrices into smaller blocks that can firt into the GPUs on-chip SRAM, it can calculate attention probabilities without needing to store large intermediate matrices, enables block-wise processing, reducing memory requirements, important during training where the recomputation of intermediate values from quadratic to linear

FlashAttention-2
(attn_implementation param)

pip install flash-attn --no-build-isolation
====================================================
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3",
    attn_implementation="flash_attention_2",
)
====================================================
3 types of model parallelism, each splits model weights and computation differently
- data parallelism (DP) (training step)
	copies model across different GPUs, each GPU processes a subset of the data simultaneously, gradients calc on each GPU are averaged. Typically used for training
- pipeline parallelism (PP) (inference step)
	distributs the computational load of training and running large neural networks across multiple GPUs, "pipeline bubbles" may arise when GPUs are idle waiting for acitivations from preceding layers. Micro batching mitigates pipeline bubbles ensuring GPUs remain busier as the next sub-batch can begin processing before the previous one is fully completed
- tensor parallelism (TP) (inference step)
	splits the weight matrices found in individual layers, the weight matrix is divided so that each GPU process only a subset of the weights, inputs are braodcast to all GPUs and independently compute outputs, partial results are aggregated through an all-reduce operation.

If using dropout or layernorm TP is not applicable due it being dependent of the entire input.

These three techniques can be combined, pipeline is the best but might sacrifice efficiency due to pipeline bubbles

Quantization is the process of representing weights and activations of a neural network using lower-precision data types, for LLMS it means reducing the precision of the model's weights and activations
Two Types:
- Post-Training Quantization (PTQ)
	the weighrts of a pre-trained model are directly converted to a lower precision format without retraining
- Quantization-Aware Training (QAT)
	happens during training or fine-tuning, better performance than PTQ requires additional computational resources

Data type is important when quantization, floats FP32, FP16, BF16
(-1)^sign x base^exponent x significand

absmax quantization maps the original weights x to the range [-127,127] by dividing them by the absolute max value of x and scaling them
Xquant = round(127 * X/max|X|)
if our absoulte max value is 3.2 a weight of 0.1 would be quantized to:
= round(127 * 0.1/3.2) = 4
to dequantize it we do the inverse
Xdequant = (max|X| * Xquant/127)
= (3.2 * 4/127) = 0.1008

import torch
def absmax_quantize(X):
    # Calculate scale
    scale = 127 / torch.max(torch.abs(X))
    # Quantize
    X_quant = (scale * X).round()
    return X_quant.to(torch.int8)

Zero-point quantization considers asymmetric input distributions maps weights to the range [-128,127] by introduction a zero-point offset:
Xquant = round(scale * X + zeropoint)
scale = 255/max(X)-min(X) and zeropoint = -round(scale * min(X)) - 128
if we use the same example as above
255/3.2+3.0 ~= 41.13 zero-point -round(255/3.2+3.0 * -3.0) - 128 = -5
round(41.13 * 0.1 - 5) = -1 (unlike 4 provided by absmax)
inverse
Xdequant = (Xquant - zeropoint/scale)

def zeropoint_quantize(X):
    # Calculate value range (denominator)
    x_range = torch.max(X) - torch.min(X)
    x_range = 1 if x_range == 0 else x_range
    # Calculate scale
    scale = 255 / x_range
    # Shift by zero-point
    zeropoint = (-scale * torch.min(X) - 128).round()
    # Scale and round the inputs
    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)
   
    return X_quant.to(torch.int8)

Be aware that quantization can have limitations when dealing with outliers
we need more advanced quantization techniques `LLM.int8()` this may introduce additional computational overhead

from transformers import AutoModelForCausalLM

model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

its also integrated into transformers:

from transformers import AutoModelForCausalLM

model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)

`llama.cpp` and `GGUF` is also a popular quantization technique offers CPU inference with GPU offloading
`GPTQ` and EXL2` are two quantizations formats dedicated to GPUs, process quantization of weights in batches 
Theres also `QuIP#` and `HQQ` that better perserves the performance could take up less space.

---------------------------------------------------------
RAG inference pipeline
Feature pipeline runs on a different machine on a schedule to populate the vector DB, the retrieval module is called on demand within the inference pipeline.
1) User query
2) Query expansion
3) Self-querying
4) Filered vector search
5) Collecting results
6) Reranking
7) Build the prompt and call the LLM
8) Answer

*Refer to code file*

Increasing the number of query expansion (using multiple LLMs) will create various perspectives but the more we add the more latency we get, in the code we can parallelize them.


An issue w/ plain vector search is retrieving semantically similar but contextually irrelavant documents. For example searching for the word "Python" could retrieve documents about the snake instead of the code

plain vector search can suffer from scalability issues.

filtered vector search is a solution with this it tragets metadata tags or categories. Additonally it improves latency by reducing comparisons the algorithm needs to perform

Metadata for author_id
====================
from qdrant_client.models import FieldCondition, Filter, MatchValue

records = qdrant_connection.search(
            collection_name="articles",
            query_vector=query_embedding,
            limit=3,
            with_payload=True,
            query_filter= Filter(
                    must=[
                        FieldCondition(
                            key="author_id",
                            match=MatchValue(
                                value=str("1234"),
                            ),
                        )
                    ]
                ),
        )
====================
Retrieved context may contain irrelevant chunks that only:
- Add noise
- Make the prompt bigger
- Unaligned with question
All of these are irrelevant, cluttering information

The solution is to use reranking to order all the N x K retrieved chunks based on their relevance relative to the inital question, first chunk is most relevant, and last is least.
N = number of searches after query expansion
K = number of chunks retrieved per search

neural cross-encoders evaluate the semantic similarity between the query and each chunk more accurately than initial retrieval methods based on embeddings and the cosine similarity distance

Reranking works well when combined with query expansion:
1) Seach for N x K chunks: retrieve multiple sets of chunks using the expanded queries
2) Reordering using rerank: rerank all retrieved chunks based on their relevance
3) Take top K: select most relevant chunks for final prompt

*Refer to code file*
(rerank)

We can optimize the retrieval phase using a hybrid search algorithm that combines the vector search (based on embeddings) with a keyword search algorithm such as BM25. Hybrid search follow these properties:
1) Parallel processing
	search is simultaneously processed through both the vector search and BM25 algorithms
2) Score normalization
	This step is important because vector search and BM25 scoring work at different scales.
3) Result merging
	normalized scores are combined, through a weighted sum

Instead of using embeddings of a single field to do the vector search, we can combine multiple fields (multi-index vector structures)
Superlinked is a framework that makes multi-indexing easy.
====================
from superlinked.framework.common.schema.id_schema_object import IdField
from superlinked.framework.common.schema.schema import schema
from superlinked.framework.common.schema.schema_object import String
… # Other Superlinked imports. 
@schema
class ArticleSchema:
    id: IdField
    platform: String
    content: String
article = ArticleSchema()
articles_space_content = TextSimilaritySpace(
    text=chunk(article.content, chunk_size=500, chunk_overlap=50),
    model=settings.EMBEDDING_MODEL_ID,
)
articles_space_plaform = CategoricalSimilaritySpace(
    category_input=article.platform,
    categories=["medium", "substack", "wordpress"],
    negative_filter=-5.0,
)
article_index = Index(
    [articles_space_content, articles_space_plaform],
    fields=[article.author_id],
)
---------------------------------------------------------
Three requirements when deploying ML models
- throughput
- latency
- data
- infrastructure
Tradeoffs for all

Throughput measured in requests per second (RPS) number of inference requests a system can process in a given period

Latency is the time it takes for a system to process a single inference request from when it is recieved until the result is returned. Sum of the network I/O, serialization and deserialization, and LLM's inference time.
Even when batching requests at serving time always considfer the min latency

The type and size of the data impact latency and throughput includes format, volume, and complexity

Infrastructure refers to the hardware, software, networking, and system architecture. You might have to sacrifice high throughput in favor of lower latency
low throughput: optimized to reduce processing time
high throughput: large data volumns adn request rates.

What are the throughput requirements? Make decision based on throughput's required min, avg, max statistics
How many requests the system must handle simultaneously?
What are the latency requirements?
How should the system scale?
What are the cost requirements? What data do we work with? images, text, video?
What is the size of the data we work with?

Inference deplopyment types
- online real-time inference
- asynchronous inference
- offline batch tansform

We can use REST API or gRPC server, the REST API is more accessible but slower using JSON to pass data between client and server.
This approach is used when serving models outside your internal network to the public

gRPC makes your ML server faster, although you have to implement `protobuf` schemas in your client application which can be compiled into bites.

ChatGPT or Claude often use websockets to stream each token individually to the end user. Other real-time inference RAG, online recommendation engines in platforms like TikTok, although may lead to underutilized resources during low traffic periods.

Asyncronous inference places requests in a queue for processing later on. Depending on the size of the result you can put it either in a different queue or an object storage dedicated to storing the result.
Client can use polling that checks on a schedule or a push w/ notifications
Uses resources more efficiently
Although it does introduce higher latency, and complexity
A popular choice for problems such as extracting keywords from documents, summarizing them using LLMs, or running deep fake models on top of videos.

Batch transformation processes large data simultaneously, either on a schedule or tiggered manually, it is then stored AWS S3, data warehouse, GCP BigQuery. Simplest way to serve a model
Batch delays real-time compute for example if we want a recommender system for a video streaming platform having a day to predict movies and tv shows works, for a social media platform this doesn't because you always want to provide fresh content.

Monolithic architecture: everything is in one codebase easy to perform maintenance on small to medium projects. limits flexibility as all components share the same tech stack and runtime. Teams can't optimize independendently
Microservice architecture: splits LLM service and business logic, these components communicate over network via REST, gRPC (scaled horizontal). Teams can optimize independendently, you can have a service for preprocesing, model, and post-processing.

Use monolithic in the beginning with micro in mind keeping ML and business logic in two different python modules/packages.

1) A user sends a query through an HTTP request
2) users input retrieves proper context by leveraging the advance RAG
3) users input and retrieved context are packed into the final prompt using dedicated prompt template
4) The prompt is sent to the LLM microservice through an HTTP request
5) The business microservices wait for the generated answer
6) after generated answer, it is sent to prompt monitoring pipeline along with users input and other vital info to monitor
7) utlimately, the generated answer is sent back to the user

SageMaker:
- endpoint
	secure API to enable real-time predictions to interact with model
- model
	artifact that results from training an algorithm
- config
	hardware/software setup to host the model
- inference component
	connects model and configuration to an endpoint
There are many ML deployment tools and are changing a lot find the tool that best suites your needs.

Applying the same preprocessing and post-processing functions and hyperparameters during training and inference is crucial

We can use Hugging Face inference container to deploy our LLM
Hugging Face's DLCs are specialized Docker images that come pre-loaded with deep-learning frameworks and libs, popular tools like transformers, datasets, and tokenizers. Elimates complex enviornment setup, and optimization. Fully integrated serving stack.
DLC is powered by Text Generation Inference (TGI) engine

TGI is open source for deploying and serving LLMs, it offers high-performance text generation using tensor parallelism, and dynamic batching.
- Tensor parallelism
- Optimized transformers code for inference (flash attention)
- Quantization with `bitsandbytes`
- Continuous batching of incoming requests
- Accelerated weight loading `safetensors
- Token streaming supports real-time interactions through Server-Sent Events (SSE)

ResourceRequirements:
====================
from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements
    model_resource_config = ResourceRequirements(
    requests={
        "copies": settings.COPIES,
        "num_accelerators": settings.GPUS
        "num_cpus": settings.CPUS,
        "memory": 5 * 1024
    },
)
====================
- copies
	how many instances or replicas of the model should be deployed (can help in reducing latency and increasing throughput)
- num_accelerators
	# GPUs to allocate, required to accelerate inference processes
- num_cpus
	# of CPU cores, ability to handle data pre/post processing
- memory
	min about of RAM required, otherwise might get memory shortages

====================
hugging_face_deploy_config = {
    "HF_MODEL_ID": settings.HF_MODEL_ID,
    "SM_NUM_GPUS": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used per replica
    "MAX_INPUT_LENGTH": json.dumps(settings.MAX_INPUT_LENGTH),  # Max length of input text
    "MAX_TOTAL_TOKENS": json.dumps(settings.MAX_TOTAL_TOKENS),  # Max length of the generation (including input text)
    "MAX_BATCH_TOTAL_TOKENS": json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),
    "HUGGING_FACE_HUB_TOKEN": settings.HUGGINGFACE_ACCESS_TOKEN,
    "MAX_BATCH_PREFILL_TOKENS": "10000",
    "HF_MODEL_QUANTIZE": "bitsandbytes",
}
start SageMaker deployment
====================
create_endpoint(endpoint_type=EndpointType.MODEL_BASED)
or using poe:
poetry poe deploy-inference-endpoint
user role:
poetry poe create-sagemaker-role
poetry poe create-sagemaker-execution-role
====================
we can run a test example!
poetry run python -m llm_engineering.model.inference.test
poetry poe test-sagemaker-endpoint
====================
Start the FastAPI web server:
uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload

or using poe:
poetry poe run-inference-ml-service

call the /rag endpoint: noted in the code file
curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\"query\": \"your_query \"}'

poe:
poetry poe call-inference-ml-service
====================

This only runs locally we need to deploy to AWS Elastic Kubernetes Service (EKS) or AWS Elastic Container Service (ECS)

Create an AWS EKS/ECS cluster from the dashboard or infrastructure-as-code (IaC) tool such as Terraform. Then Dockerize the FastAPI code and push the Docker image to AWS ECR create an ECS/EKR deployment using the Docker image hosted on ECR.

Make sure to delete everything so AWS won't break your wallet (if it hasn't already)
poetry poe delete-inference-endpoint
make sure to double check the dashboard

Its expensive to run GPU that become idle and we need more compute as traffic increases so we need an Application Load Balancer (ALB) simpliest approach is to use round robin for each LLM replica.

SageMaker provides a feature called Application Auto Scaling which allows for scaling resources, we would need to register a scalable target and create scalable policy.

Registering a scalable target
specific resource to scale with boundaries
- Resource ID
	the name of the SageMaker Inference component
- Service namespace
	AWS service the resource belongs to (SageMaker)
- Scalable dimension
	number of copies
- Min/Max Capacity
	boundaries of auto scaling

Scalable policy defines specific rules that trigger scaling events
- Policy type
	could select TargetTrackingScaling, adjusts specific target value
- Target tracking configuration
	selecting metric to monitor

TargetTrackingScaling will act has a monitor much like room tempature to stay consistent.

Application Auto Scaling simplify the scaling process no need to configure CloudWatch alarms and define scaling adjustments manually

Set min/max scaling limits before creating a policy

You can specify a cooldown period which prevents rapid fluctuations in the number of instances, dealys the removal of instances during scale-in (reduce) and restricts creation of new replicas during scale-out (increase)

---------------------------------------------------------

MLOps concept is similar to DevOps with MLOps theres more moving parts
data, model, and code
A build can be triggered if any of these change
when one component changes it affects one or more of the others.

changes in data that indirectly effects the model
- After deploying the ML model
	performance might decay as time passes, need new data to re-train
- After understanding how to collect data in the real world
	re-formulate getting data
- experimentation stage and training model
	collecting more data or re-labeling it
- serving model in prod and collecting feedback from users
	we might have to change our model

MLOps is the extension of the DevOps field that makes data and models their first-class citizen perserving the devOps methodology

- Model registry
	A centralized repo for trained ML models
- Feature store
	preprocessing and storing input data as features for model training and inference pipelines
- ML metadata store
	tracks info model config, training/testing data, performance metrics. Compare multiple models
- ML pipeline orchestrator
	automating the sequence of steps in ML projects

six core principles guide MLOps field
- Automation or operationalization
	automate pipelines through CT and CI/CD
- Versioning
	track changes in code, models, and data. versioned thro model registries, data versioning (DVC) or artifact management system
- Experment tracking
	comparing multiple experiements based on predefined metrics
- Testing
	along with testing code, you should also test data and models.
- Monitoring
	detecting performance degradation, logs, system/model metrics
- Reproducibility
	setting well-known seeds to generate pseudo-random numbers is essential to achieving consistent outcomes, theres a lot of config/versions/hyperparamters etc.

ML and MLOps can be combined into the same role at small/medium sized companies, just like engineers that act as devOps by building a CI/CD

Data Science/ML Research (no MLOps):
PyTorch, Tensorflow, HuggingFace etc
ML Engineering (MLOps might be involved here):
API, DBs, modularity
Infrastructure (MLOps)
Docker, K8s, cloud, etc

LLMOps builds ontop of MLOps (same fundamentals) focueses on their large size, complex training requirements, prompt management, non-deterministic nature generating answers

Keep in mind these topics when training a LLM from scratch
- Data collection and preparation
	big data techniques for processing, storing, training datasets
	GPT-4 was trained on 13 trillion tokens equal to ~10 trillion words
- Managing large number of parameters
	vast computation resources, Nvidia GPUs, CUDA
- Model Training
	multi-GPU training, optimizing your processes
- Costs
	estimated training cost for GPT-4 is ~100 million, only the large players in the industry can afford to train LLMs from scratch :(

LLMOps is MLOps at scale. The trend is shifting away from training Neural Networks from scratch, with the rise of fine tuning and foundational models such as GPT only a few organizations develop these foundational models (OpenAi, Google). Most applications rely on the lightwright fine-tuning of these models, prompt engineering, or optionally distilling data or models into smaller specialized inference networks.

Human Feedback
A valuable refinement step of your LLM, you must introduce a feedback loop within your application and gather a human feedback dataset. RLHF Reinforcement Learning with Human Feedback or Direct Preference Optimization (DPO), another one is thumbs up/down

You must add guardrails to prevent accidently leaking personal information, hullucinations, secrets, impersonating a dictator. Some protection must be in place.
- Input guardrails
	this can be protection against asking LLM how to do malicious things, exposing private information to external APIs, executing harmful prompts that could compromise your system
- Output guardrails
	the output of an LLM response, you want to catch failed outputs, empty responses, toxic responses, hallucinations, wrong responses.

Galileo Protect detects prompt injections, toxic language, data privacy, protection leaks and hallucinations. Or use OpenAI's Moderation API for harmful inputs/outputs.

Guardrails will add latency

Prompt monitoring
- Time to First Token (TTFT)
	time for first token to be generated
- Time between Tokens (TBT)
	interval between each token
- Tokens per Second (TPS)
	rate tokens are generated
- Time per Output Token (TPOT)
	Time it takes to generate each output token
- Total Latency
	Total time required to complete a response

When using RAG systems you can also compute metrics relative to relevance and precision of the retrieved context. Monitoring prompts is to log their full traces, monitoring latency, tokens, and costs at each step, providing a fine-grained view of all the steps.

ZenML cloud we can
- ECR service for storing Docker images
- S3 object storage storing models and artifacts
- SageMaker Orchestrator running and scaling ML pipelines

1) build a docker image for dependencies
2) push image to ECR so SageMaker can access
3) trigger any pipeline via CLI or ZenML dashboard
4) Each step in ZenMLs pipleline will be maped to Sagemaker that runs EC2 virtual machine based on dependencies
5) SageMaker pulls teh Docker image from ECR based on the pulled image it creates a Docker container that executes the pipeline step
6) now it can access the S3 artifact storage, MongoDB and Qdrant vector DB to query or push the data

.env (mongoDB and qdrant region may have to match wasn't given a region selection option for qdrant)
====================
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=<get values from qdrant site>
QDRANT_APIKEY=<get values from qdrant site>
DATABASE_HOST= mongodb+srv://<username>:<password>@twin.vhxy1.mongodb.net

peotry poe run-end-to-end-data-pipeline

https://cloud.zenml.io will provide a 7day free account

run this command to make sure everything works fine
poetry poe run-digital-data-etl
====================

*refer to code file to setup Dockerfile*

poetry poe build-docker-image

Will be availble once you run your Dockerfile to push to ECR
AWS_REGION=<your_region> # e.g. AWS_REGION=eu-central-1
AWS_ECR_URL=<your_acount_id>
aws ecr get-login-password --region ${AWS_REGION}| docker login --username AWS --password-stdin ${AWS_ECR_URL}

docker tag llmtwin ${AWS_ECR_URL}:latest
docker push ${AWS_ECR_URL}:latest
====================
Run pipelines on AWS:
ZenML needs to know how to use the AWS stack

zenml stack set aws-stack

open `configs/end_to_end_data.yaml`

settings:
  docker:
    parent_image: <YOUR ECR URL> #e.g., 125839364772.dkr.ecr.eu-central-1.amazonaws.com/zenml-rlwlcs:latest
    skip_build: True

Will automatically pick up the latest changes made to the code whenever we push a new image

poetry poe export-settings-to-zenml
zenml orchestrator update aws-stack --synchronous=False

poetry poe run-end-to-end-data-pipeline

to run other pipelines update `settings.docker.parent_image`

run pipelines locally:
`poetry poe set-local-stack`
disconnect cloud dashboard and use local version:
`zenml disconnect`

if you run into a SageMaker resource error:
https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error

delete old secrets:
`poetry poe delete-settings-zenml`

exports .env
`poetry poe export-settings-to-zenml`
====================

github actions will allow us to automatically build Docker image and push to ECR

this workflow clones current Github repo, and installs python
====================
name: Example
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
         - name: Checkout
           uses: actions/checkout@v3
         - name: Setup Python
           uses: actions/setup-python@v3
           with:
               python-version: "3.11"
====================

*refer to code file Github Actions*
Now that we setup our CI/CD pipeline we will need one for our CT pipeline

We can trigger the data collection pipeline via
- Manual triggers
	through CLI or orchestrator dashboard (ZenML)
- REST API triggers
	HTTP request trigger
- Scheduled triggers
	constant fixed interval (cronjobs)

*refer to code file Pipelines*

We can add the `@track` decorator to `ContextRetriever:search` and `SelfQuery:generate` we capture the prompt sent to LLM and its response

Monitoring everything can be dangerous as it adds to much noise, find good balance
- Model config
	model IDs, tempature, RAG layer
- total number of tokens
	impact serving costs, if tokens suddenly increases might have a bug in the system
- duration of each step
	finding bottlenecks

You can setup alerting if needed via slack/discord etc:
Alerts are a critical component 
====================
from zenml import get_pipeline_context, pipeline
@pipeline(on_failure=notify_on_failure)
def training_pipeline(…):
…
notify_on_success()

from zenml.client import Client
alerter = Client().active_stack.alerter
def notify_on_failure() -> None:
        alerter.post(message=build_message(status="failed"))
@step(enable_cache=False)
def notify_on_success() -> None:
        alerter.post(message=build_message(status="succeeded"))
====================
