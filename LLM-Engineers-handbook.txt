FTI architecture
Feature Training and Inference

Feature Pipeline
Takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference, saved to a feature store

Training Pipeline
Takes the features and labels from the features stored as input and outputs a train model or models and puts them into a model registry

Inference Pipeline
Takes as input the features and labels from the feature store and trained model from the model registry to make predictions

Benefits of FTI:
- Just three components easy to understand
- Each component can be written into its own tech stack, so we can adapt them to specific needs, allows us to pick the best tools for the job.
- There is a transparent interface between the three components, each one can be developed by a different team
- every component can be monitored and deployed independently.

RAG (retrieval-augmented generation)
With any RAG-based system, one of the central peices of the infrastructure is a vector DB

LLM encapsulates and automates all the following steps:
- Data collection
- Data preprocessing
- Data storage, versioning, and retrieval
- LLM fine-tuning
- RAG
- Content generation evaluation
The key is to be data-centric and architecture model-agnostic so we can use different models on specific data.

run `pyenv local 3.11.8` then pyenv will always know to use that python version it creates a `.python-version` file

Poetry is a dependency and vitrual environment manager.
it saves all its requirements in `pyproject.toml`
creates a .lock file just like node package manager.
had used venv before but lacks the dependency management option

Poe the Poet is a plugin on top of Poetry
helps you define and run tasks within your Python project, simplifying automation and script execution.
Just like how we have commands in package.json example:

we've defined:
[tool.poe.tasks]
test = "pytest"

then run:
`poetry poe test`

ZenML acts as the bridge between ML and MLOps. It handles transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment.
ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.

An orchestrator is a system that automates, schedules, and coordinates all your ML pipelines.

ZenML works as an orchestrator via piplelines and steps, example:

from zenml import pipeline
from steps.etl import crawl_links, get_or_create_user
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> None:
	user = get_or_create_user(user_full_name)
	crawl_links(user=user, links=links)

To integrate ZenML with your code you have to write modular code, where each function does one thing (makes it easy to use `@step` decorators):

from loguru import logger
from typing_extensions import Annotated
from zenml import step
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument

@step
def get_or_create_user(user_full_name: str) -> Annotated[UserDocument, "user"]:
    logger.info(f"Getting or creating user: {user_full_name}")
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    return user

Have a directory of steps alongside pipelines on the root so we can swap orchestrators if needed to use a REST API

In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. We can transform anything into an artifact.

For web scraping its good to keep a config folder and a specific .yaml file that have all the URLs associated to (in this case) all posts/code from the same person. 

Keeping everything in pipeline config .yaml files is ideal

We can use Comet to track metrics and visualize them, track different config between experiements

Opik: prompt monitoring
Qdrant: vector database

SageMaker provides a comprehensive platform for building, training, and deploying machine learning models vs Amazon Bedrock which is just pretrained models accessed directly through an API an "out-of-box" solution.
Even SageMaker isn’t fully customizable. If you want complete control over your deployment use EKS AWS Kubernetes self-managed service.

We use an ETL pipline to extract data, transforming and cleaning data into a suitable format for storage and analysis, and load into our warehouse or db.

ETL and the feature pipeline strictly communicate trough the Mongo data warehouse. Feature will read and the ETL process will write acting independently.

using MongoDB as a data warehouse is uncommon, but we're using it for a small amount of data and its fine for our unstrucutred data (internet web articles), for production or dealing with tons of data we would use Snowflake or BigQuery

Our crawling logic/step:
===========================================================
from urllib.parse import urlparse
from loguru import logger
from tqdm import tqdm
from typing_extensions import Annotated
from zenml import get_step_context, step
from llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher
from llm_engineering.domain.documents import UserDocument

@step
def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], "crawled_links"]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(f"Starting to crawl {len(links)} link(s).")
    metadata = {}
    successfull_crawls = 0
    for link in tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
        step_context = get_step_context()
    step_context.add_output_metadata(output_name="crawled_links", metadata=metadata)
    logger.info(f"Successfully crawled {successfull_crawls} / {len(links)} links.")
    return links

def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    try:
        crawler.extract(link=link, user=user)
        return (True, crawler_domain)
    except Exception as e:
        logger.error(f"An error occurred while crawling: {e!s}")
        return (False, crawler_domain)

def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:
	if domain not in metadata:
	    metadata[domain] = {}
	metadata[domain]["successful"] = metadata.get(domain, {}).get("successful", 0) + successfull_crawl
	metadata[domain]["total"] = metadata.get(domain, {}).get("total", 0) + 1
	return metadata
===========================================================
CrawlerDispatcher:
===========================================================
import re
from urllib.parse import urlparse
from loguru import logger
from .base import BaseCrawler
from .custom_article import CustomArticleCrawler
from .github import GithubCrawler
from .linkedin import LinkedInCrawler
from .medium import MediumCrawler

class CrawlerDispatcher:
    def __init__(self) -> None:
        self._crawlers = {}

    @classmethod
    def build(cls) -> "CrawlerDispatcher":
        dispatcher = cls()
        return dispatcher

    # CrawlerDispatcher.build().register_linkedin().register_medium()
    def register_medium(self) -> "CrawlerDispatcher":
        self.register("https://medium.com", MediumCrawler)
        return self
    def register_linkedin(self) -> "CrawlerDispatcher":
        self.register("https://linkedin.com", LinkedInCrawler)
        return self
    def register_github(self) -> "CrawlerDispatcher":
        self.register("https://github.com", GithubCrawler)
        return self


    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[r"https://(www\.)?{}/*".format(re.escape(domain))] = crawler


    def get_crawler(self, url: str) -> BaseCrawler:
        for pattern, crawler in self._crawlers.items():
            if re.match(pattern, url):
                return crawler()
        else:
            logger.warning(f"No crawler found for {url}. Defaulting to CustomArticleCrawler.")
            return CustomArticleCrawler()
===========================================================
BaseCrawler
===========================================================
from abc import ABC, abstractmethod
class BaseCrawler(ABC):
    model: type[NoSQLBaseDocument]
    @abstractmethod
    def extract(self, link: str, **kwargs) -> None: ...

Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave.
we can use this tool to login, scroll websites, click on different elements etc
===========================================================
BaseSeleniumCrawler
===========================================================
import time
from tempfile import mkdtemp
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from llm_engineering.domain.documents import NoSQLBaseDocument
# Check if the current version of chromedriver exists
# and if it doesn't exist, download it automatically,
# then add chromedriver to path
chromedriver_autoinstaller.install()

class BaseSeleniumCrawler(BaseCrawler, ABC):
    def __init__(self, scroll_limit: int = 5) -> None:
        options = webdriver.ChromeOptions()
       
        options.add_argument("--no-sandbox")
        options.add_argument("--headless=new")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-background-networking")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument(f"--user-data-dir={mkdtemp()}")
        options.add_argument(f"--data-path={mkdtemp()}")
        options.add_argument(f"--disk-cache-dir={mkdtemp()}")
        options.add_argument("--remote-debugging-port=9226")

        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )

    # placeholders
    def set_extra_driver_options(self, options: Options) -> None:
        pass
    def login(self) -> None:
        pass

    def scroll_page(self) -> None:
        """Scroll through the LinkedIn page based on the scroll limit."""
        current_scroll = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):
                break
            last_height = new_height
            current_scroll += 1

===========================================================
next we can look at the concrete crawlers, in our case theres three
GitHubCrawler(BaseCrawler)
CustomArticleCrawler(BaseCrawler)
MediumCrawler(BaseSeleniumCrawler)

We don't have to log in to GitHub through the browser, as we can leverage Git clone functionality. this means no Selenium functionality. 


GithubCrawler

===========================================================
class GithubCrawler(BaseCrawler):
    model = RepositoryDocument
    def __init__(self, ignore=(".git", ".toml", ".lock", ".png")) -> None:
        super().__init__()
        self._ignore = ignore

    def extract(self, link: str, **kwargs) -> None:
    	old_model = self.model.find(link=link)
    	if old_model is not None:
        	logger.info(f"Repository already exists in the database: {link}")
        	return
        logger.info(f"Starting scrapping GitHub repository: {link}")
    	repo_name = link.rstrip("/").split("/")[-1]
    	local_temp = tempfile.mkdtemp()

	    try:
        	os.chdir(local_temp)
        	subprocess.run(["git", "clone", link])

        	# walks dir tree, skipping any ignore patterns
        	# removes spaces

        	repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])
	        tree = {}
	        for root, _, files in os.walk(repo_path):
	            dir = root.replace(repo_path, "").lstrip("/")
	            if dir.startswith(self._ignore):
	                continue
	            for file in files:
	                if file.endswith(self._ignore):
	                    continue
	                file_path = os.path.join(dir, file)
	                with open(os.path.join(root, file), "r", errors="ignore") as f:
	                    tree[file_path] = f.read().replace(" ", "")
	        # creates a new instance of RepositoryDocument model
	        # then saved to MongoDB
	        user = kwargs["user"]
	        instance = self.model(
	            content=tree,
	            name=repo_name,
	            link=link,
	            platform="github",
	            author_id=user.id,
	            author_full_name=user.full_name,
	        )
	        instance.save()
	    # clean up the temp directory
	    except Exception:
	        raise
	    finally:
	        shutil.rmtree(local_temp)
	    logger.info(f"Finished scrapping GitHub repository: {link}")
===========================================================
CustomArticleCrawler
This will use AsyncHtmlLoader and Html2TextTransformer
both from the langchain community

some developers avoid using LangChain in production

===========================================================
from urllib.parse import urlparse
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers.html2text import Html2TextTransformer
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseCrawler

class CustomArticleCrawler(BaseCrawler):
    model = ArticleDocument
    def extract(self, link: str, **kwargs) -> None:
        old_model = self.model.find(link=link)
        if old_model is not None:
            logger.info(f"Article already exists in the database: {link}")
            return
        # here we use both
        # AsyncHtmlLoader, and Html2TextTransformer
        # we are not in control is extracted and parsed
        # fallback where we don’t have anything custom implemented

	    logger.info(f"Starting scrapping article: {link}")
	    loader = AsyncHtmlLoader([link])
	    docs = loader.load()
	    html2text = Html2TextTransformer()
	    docs_transformed = html2text.transform_documents(docs)
	    doc_transformed = docs_transformed[0]
	    # page content and meta data
        content = {
            "Title": doc_transformed.metadata.get("title"),
            "Subtitle": doc_transformed.metadata.get("description"),
            "Content": doc_transformed.page_content,
            "language": doc_transformed.metadata.get("language"),
        }

        # This parses URL to determine the platform (domain) 
        # from which article was scrapped

        parsed_url = urlparse(link)
        platform = parsed_url.netloc

        # Next save everything to MongoDB

        user = kwargs["user"]
        instance = self.model(
            content=content,
            link=link,
            platform=platform,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Finished scrapping custom article: {link}")       
===========================================================
MediumCrawler

NOTE: since we use extract and a conditional at the beginnning
for each of these crawlers this should be abstracted out as well.
===========================================================
from bs4 import BeautifulSoup
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseSeleniumCrawler
class MediumCrawler(BaseSeleniumCrawler):
    model = ArticleDocument

    def set_extra_driver_options(self, options) -> None:
    	options.add_argument(r"--profile-directory=Profile 2")

    def extract(self, link: str, **kwargs) -> None:
	    old_model = self.model.find(link=link)
	    if old_model is not None:
	        logger.info(f"Article already exists in the database: {link}")
	        return
	    logger.info(f"Starting scrapping Medium article: {link}")
	    self.driver.get(link)
	    self.scroll_page()
	    # The only crawler using BeautifulSoup
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        title = soup.find_all("h1", class_="pw-post-title")
        subtitle = soup.find_all("h2", class_="pw-subtitle-paragraph")
        data = {
            "Title": title[0].string if title else None,
            "Subtitle": subtitle[0].string if subtitle else None,
            "Content": soup.get_text(),
        }

        # save to DB

        self.driver.close()
        user = kwargs["user"]
        instance = self.model(
            platform="medium",
            content=data,
            link=link,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Successfully scraped and saved article: {link}")

Linked in is a bit longer then the other two scrapers as it uses Beautiful soup
does a bulk_insert for all posts, logins() the user, find elements based on classes which in fact may change over time but the same idea applies for scrapping setting up model and saving

Since LLM core features is scrapping there are two other popular scrapping data:
Scrapy: https://github.com/scrapy/scrapy
Crawl4Ai: https://github.com/unclecode/crawl4ai
ORMs:
FastAPI SQLModel: https://github.com/fastapi/sqlmodel
SQLAlchemy: https://www.sqlalchemy.org/

It is best practice to structure data in classes instead of dictionaries, each item is more verbose, reducing run errors. 

ODM pattern is similar to ORM but instead it simply works with NoSQL

===========================================================
NoSQLBaseDocument

Nothing special in these methods just a wrapper for
DB management, the class methods here are self explaintory
similar to mongoengine
===========================================================
import uuid
from abc import ABC
from typing import Generic, Type, TypeVar
from loguru import logger
from pydantic import UUID4, BaseModel, Field
from pymongo import errors
from llm_engineering.domain.exceptions import ImproperlyConfigured
from llm_engineering.infrastructure.db.mongo import connection
from llm_engineering.settings import settings

_database = connection.get_database(settings.DATABASE_NAME)
T = TypeVar("T", bound="NoSQLBaseDocument")

# id field is defined as a UUID4, with a default factory generating a unique UUID
class NoSQLBaseDocument(BaseModel, Generic[T], ABC):
	id: UUID4 = Field(default_factory=uuid.uuid4)
	def __eq__(self, value: object) -> bool:
	    if not isinstance(value, self.__class__):
	        return False
	    return self.id == value.id
	def __hash__(self) -> int:
	    return hash(self.id)

	# The from_mongo() and to_mongo()
	# are what it says mongoDB->class, dictionary->mongoDB
	@classmethod
	def from_mongo(cls: Type[T], data: dict) -> T:
	    if not data:
	        raise ValueError("Data is empty.")
	    id = data.pop("_id")
	    return cls(**dict(data, id=id))
	def to_mongo(self: T, **kwargs) -> dict:
	    exclude_unset = kwargs.pop("exclude_unset", False)
	    by_alias = kwargs.pop("by_alias", True)
	    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
	    if "_id" not in parsed and "id" in parsed:
	        parsed["_id"] = str(parsed.pop("id"))
	    for key, value in parsed.items():
	        if isinstance(value, uuid.UUID):
	            parsed[key] = str(value)
	    return parsed

	def save(self: T, **kwargs) -> T | None:
	    collection = _database[self.get_collection_name()]
	    try:
	        collection.insert_one(self.to_mongo(**kwargs))
	        return self
	    except errors.WriteError:
	        logger.exception("Failed to insert document.")
	        return None

	@classmethod
	def get_or_create(cls: Type[T], **filter_options) -> T:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        new_instance = cls(**filter_options)
	        new_instance = new_instance.save()
	        return new_instance
	    except errors.OperationFailure:
	        logger.exception(f"Failed to retrieve document with filter options: {filter_options}")
	        raise

	@classmethod
	def bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:
	    collection = _database[cls.get_collection_name()]
	    try:
	        collection.insert_many([doc.to_mongo(**kwargs) for doc in documents])
	        return True
	    except (errors.WriteError, errors.BulkWriteError):
	logger.error(f"Failed to insert documents of type {cls.__name__}")
	        return False

	@classmethod
	def find(cls: Type[T], **filter_options) -> T | None:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        return None
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return None

	@classmethod
	def bulk_find(cls: Type[T], **filter_options) -> list[T]:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instances = collection.find(filter_options)
	        return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return []

	@classmethod
	def get_collection_name(cls: Type[T]) -> str:
	    if not hasattr(cls, "Settings") or not hasattr(cls.Settings, "name"):
	        raise ImproperlyConfigured(
	            "Document should define an Settings configuration class with the name of the collection."
	        )
	    return cls.Settings.name

=========================================================
Lastly concrete classes that define our data categories
=========================================================
from abc import ABC
from typing import Optional
from pydantic import UUID4, Field
from .base import NoSQLBaseDocument
from .types import DataCategory
from enum import StrEnum

class DataCategory(StrEnum):
    PROMPT = "prompt"
    QUERIES = "queries"
    INSTRUCT_DATASET_SAMPLES = "instruct_dataset_samples"
    INSTRUCT_DATASET = "instruct_dataset"
    PREFERENCE_DATASET_SAMPLES = "preference_dataset_samples"
    PREFERENCE_DATASET = "preference_dataset"
    POSTS = "posts"
    ARTICLES = "articles"
	REPOSITORIES = "repositories"

class Document(NoSQLBaseDocument, ABC):
    content: dict
    platform: str
    author_id: UUID4 = Field(alias="author_id")
    author_full_name: str = Field(alias="author_full_name")

class RepositoryDocument(Document):
    name: str
    link: str
    class Settings:
        name = DataCategory.REPOSITORIES
class PostDocument(Document):
    image: Optional[str] = None
    link: str | None = None
    class Settings:
        name = DataCategory.POSTS
class ArticleDocument(Document):
    link: str
    class Settings:
        name = DataCategory.ARTICLES

class UserDocument(NoSQLBaseDocument):
    first_name: str
    last_name: str
    class Settings:
        name = "users"
    @property
    def full_name(self):
        return f"{self.first_name} {self.last_name}"

-------------------------------------------------------------------------------
You often want to use the LLM on data it wasn’t trained on
Retrieval augmented generation (RAG)
- used to inject custom data into the LLM to perform a given action
- fine tuning LLM is highly costly, RAG bypasses need for constant fine-tuning
- summarize, reformulate, and extract the injected data

Retrieval: Search for relevant data
Augmented: Add data as context to the prompt
Generation: Augmented prompt with an LLM for generation

LLM is bound to the data it was trained on (parameterized knowledge)
GPT-4o trained data up to Oct 2023, RAG overcomes these limits and provides access to external or latest data and prevents hallucinations.

RAG Solves:
Hallucinations (could tell us something that isn't true)
Old or private information (new data is generated every second!)

RAG system (3 modules):
Ingestion pipeline: batch or streaming pipeline to populate vectorDB
Retrieval pipeline: queries vector DB and retrieves relevant users input
Generation pipeline: uses retrieved data to augment the prompt and LLM to generate answers

ingestion pipeline (constantly updating)
user input->retrieval->generation->LLM->answer

Ingestion pipeline
---------------------------------------
gets data from many sources then cleans, chunks, and embeds
loads the embedded chunks into a vector DB

first data extraction, from DB APIs or webpages
second cleaning layer, depending on your data cleaning varies such as invalid/unwanted characters 
third chunking, splits cleaned documents into smaller ones. ensure it doesn't exceed the models input max size.
fourth embedding, takes the chunks content and project it into a dense vector packed with semantic value.
fifth loading, takes embedded chunks along with metadata the embedding is used as an index to query similar chnks. metadata is used to access the information added to augment the prompt

Retrieval pipeline
---------------------------------------
take user input (text, image, audio) embed it and query vector DB for similar vectors
project users input into the same vector space as the embeddings used as an index in the vector DB. We can then find the top K's most similar entried by comparing embeddings from the vector storage with the user's input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer

distance formula cosine distance is popular (others Euclidean, Manhatten)
1 - cos(θ) = 1 - (A ⋅ B)/(||A||⋅||B||)
ranges from -1 (opposite directions) to 1 (same direction), 0 (orthogonal)
distance between two vectors depends on data and embedding model used.
To avoid training-serving skew you must clean, chunk and embed the users input using the same functions, models, and hyperparameters, input and embeddings must be in the same vector space.

Generation pipeline
---------------------------------------
Last step take users input retrieve data, pass to LLM, generate answer. Usually all the prompt engineering is done at the prompt template level.
The final prompt populated with user's query and retrieved context

As the prompt templates evolve, each change should be tracked and versioned using machine learning operations (MLOps) best practices.
could use git or prompt management tools like LangFuse etc.

Emedddings is like a map where words with similar meanings are clustered together with numerical represenation encoded as vectors in continuous vector space.
Could be used in words, images or recommendation systems.

NLP (natrual language processing) embeddings translate words into vectors where semantically similar words are positioned close in vector space.

embeddings have more than 2 or 3 dimensions usually 64 to 2048 you must project them again to 2D or 3D.
t-SNE or UMAP uses transform high-dimensional reduction keeping geometrical properties between points, making it easier to visualize, interpret, and process while minimizing the loss of important information.
embeddings encode any categorical variable and you can feed it to an ML model.

One-hot can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.

Feature hashing (hash trick) maps categories into a fixed number of bins or buckets. risk of collisions, where different categories might map to the same bin, leading to a loss of information. It's also difficult to understand the relationship between the original categories and the hashed features.

Embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. Similar to techniques of a CNN

====================================================
example using Sentence Transformers Python package
(also in hugging face)
====================================================
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
sentences = [
"The cat sits outside waiting for a mouse.",
"I am going swimming.",
"The cat is swimming."
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# Output: [3, 384]
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# Output:
# tensor([[ 1.0000, -0.0389, 0.2692],
# [-0.0389, 1.0000, 0.3837],
# [ 0.2692, 0.3837, 1.0000]])
#
# similarities[0, 0] = The similarity between the first sentence and itself.
# similarities[0, 1] = The similarity between the first and second sentence.
# similarities[2, 1] = The similarity between the third and second sentence.
====================================================
you can pick between best-performing model or one with smallest memory footprint, descisions should be based on requirements, best to experiment

Models like CLIP, lets you embed a piece of text and an image in the same vector. This allows you to find similar images using sentence as input and vice versa
example:
====================================================
from io import BytesIO
import requests
from PIL import Image
from sentence_transformers import SentenceTransformer

response = requests.get(
"https://i.etsystatic.com/9632921/r/il/a3e46a/3070971627/il_1588xN.3070971627_5jbv.jpg"
)
image = Image.open(BytesIO(response.content))
model = SentenceTransformer("clip-ViT-B-32")
img_emb = model.encode(image)
text_emb = model.encode(
	[
		"A crazy cat smiling.",
		"A white and brown cat with a yellow bandana.",
		"A man eating in the garden."
	]
)

print(text_emb.shape)
# Output: (3, 512)
similarity_scores = model.similarity(img_emb, text_emb)
print(similarity_scores)
# Output: tensor([[0.3068, 0.3300, 0.1719]])

====================================================

A Vector db look for the closest neighbors of the query vector, it uses approximate nearest neighbor (ANN) algorithms using only approximations of the top matches for a given input query works well enough trade-off between accuracy and latency

workflow of a vector DB
1. indexing vectors, vectors are indexed using data structures optimized for high-dimensional data.
	- Hierarchiacal navigable small world (HNSW)
		- multi-layer graph where each node represents a set of vectors.
	- random projection
		reduces high-dimensonility of vectors by projecting into a lower dimensional space using a random matrix
	- product quantization (PQ)
		- dividing vectors into smaller sub-vectors and then quantizing these sub-vectors into representative codes.
	- locality-sensitive hashing (LSH)
		- maps similar vectors into buckets, fast approximate nearest neighbor searches by focusing on a subset of the data
2. querying for similarity (distance formulas)
3. post-processing results: refine accuracy, most relevant vectors are returned

Vector DB share common characteristics of a standard DB
- Sharding and replication
- Monitoring
- Access Control
- Backups

There are some limitiations in the vanilla RAG framework, we can improve this with an advanced RAG in these stages:
- Pre-retrieval index optimizations as well as query optimization
- Retrieval improving the embedding models and metadata filtering
- Post-retrieval filter out noise from retrieved documents and compress the prompt before feeding it to an LLM

Pre-retrieval
-----------------------------
- Data indexing cleaning or chunking modules to preprocess data for better indexing
- Query Optimization, algorithm performed directly on users's query before embedding it and retrieving chunks from vector DB

improve better preprocessing and structuring data:
- Sliding window
	- overlap between text chunks, ensuring important context near chunk bondaries is reatined, legal documents, scientific papers, customer support logs, medical records, critical information spans multiple sections. (maintains context)
- Enhance data granularity 
	- removing irrelevant details, factual accuracy, updating outdated information
- Metadata
	- dates, URLs, chapter markers (filter)
- Optimizing index structures
	- chunk sizes and multi-indexing strategies
- Small-to-big
	- decouples chunks for retrieval and context used in prompt for final answer generation, uses small sequence of text to compute the embedding while preserving the sequence and a wider window around it in the metadata, smaller chunks enhances the retrievals accuracy, the larger context adds more context info to the LLM
We do this to reduce the noise, or the text could contain multiple topics
- Query routing
	- Natural language is used to route queries if user input has multiple categories
- Query rewriting (reformulating question to match indexed information better)
	- Paraphrasing (e.g. "are there a lot of people that die from snakes?" could be written as "How many people die each year from snakes")
	- Synonym substitution less common words with synonyms to broaden the search (e.g. "gleeful" to "hppy")
	- Sub-queries break down long queries to mroe focused sub-queries
- Hypothetical document embeddings (HyDE) LLM creates hypotheticals, both original query and LLMs response are fed into retrieval stage
- Query expansion adding terms or concepts resulting in different perspectives for example you could search for "Zelda" but also include "Nintendo" or "Master Sword"
- Self-Query unstructured queries to structured ones

You'll have to experiment with each of these because it depends on your data

Retrieval
-----------------------------
- imporving the embedding models
- Leveraging the DB's filter and search features
both enhance the vector search step by finding similarity between query and indexed data

Instructor Models: (instead of fine-tuning the embedding model)
Tailoring your embedding network to your data using such a model can be a good option fine-tuning a model consumes more computing and human resources.

====================================================
from InstructorEmbedding import INSTRUCTOR

model = INSTRUCTOR("hkunlp/instructor-base")
sentence = "RAG Fundamentals First"
instruction = "Represent the title of an article about AI:"
embeddings = model.encode([[instruction, sentence]])

print(embeddings.shape)
# Output: (1, 768)
====================================================
python3 -m venv instructor_venv && source instructor_venv/bin/activate
pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1

classic filter and search DB filters

- Hybrid search A vector and keyword-based search blend, you have an alpha that controls weight between the two methods, two independent searches, later normalized and unified.
- Filtered vector search, metadata index to filter for specific keywords within metadata.


Post-Retrieval
-----------------------------
This step is to limit irrelevant information, or the context can be too large which can distract the LLM
- Prompt compression eliminate unnecessary details
- re-ranking use a cross-encoder ML model to give a matching score between the user's input and every retrieved chunk. Retrieved items are sorted based on this score. its costly to do this initial retrieval step.

Any RAG system is split into two independent components
- The ingestion pipeline
	- raw data, cleans, chunks, embeds, loads into vector DB
- The inference pipeline
	- queries vector db for relevant context generates by leveraging an LLM

Batch Pipelines (refer to data engineer notes)
-----------------------------
data is collected processed, and stored in intervals
- Data collection
	- data from DB, logs, files, other sources
- Scheduled processing
	- intervals hourly, daily, processed in bulk
	- cleansing, transformation, aggregation, and other operations
- Data loading
	- loaded into a target system, db, warehouse, data lake, feature store
	- available for analysis, querying, or further processing

Batching does not need immediate processing
- Efficiency
	- optimize resources, and parallel processing
- Complex processing
	- data transformation, and aggregations (which might not be suitable for real-time processing)
- Simplicity
	- simpler than real time, making them easier to implement and maintain

Streaming is well suited for social media, where behavior changes frequently. For example your looking at cats for 10 minutes on social media but you get bored and want to watch something more engaging so you switch to look at videos about educative content, the recommender system had to capture behavior system in real-time without delay. You couldn't do this with a batch system
- Fraud detection
- trading platforms

An ETL pipeline is a good approach for a batch system

Start with a batch architecture, after gradually move to a streaming design

Change data capture (CDC) allows to sync two or more data types in sync without computing and I/O overhead, it captures any CRUD operation done on the source DB and replicated on a target DB. Example: how to sync data warehouse with feature store
- Push
	- near instantaneous updates to the target, a messaging system is typically used as a buffer because data loss can occur
- Pull
	- passive role to the source db, only records changes, also needs a messaging system is essential to prevent data loss
Push is ideal for immediate data access, pull for large-scale data transffers where real-time updates aren't critical.

CDC patterns:
- Timestamp-based
	- add last_modified or last_updated time column, limited to tracking changes not deletions, imposes performance overhead (table scan)
- Trigger-based
	- tigger based approach, records data modification to a separate table upon INSERT, UPDATE, DELETE operations (event table), can impact db performance
- Log-based
	- transaction logs for all modifications w/ timestamps used for recovery, requires no schema modification, propagate changes to target system in real-time, minimizes performance impact on the source DB, lacks standardized log formats.

* Please note the following code is very step by step process *
its only to analyse the bigger picture, I put them here just to understand
a base example.

Ingestion code:
====================================================
from zenml import pipeline
from llm_engineering.interfaces.orchestrator.steps import feature_engineering as fe_steps

@pipeline
def feature_engineering(author_full_names: list[str]) -> None:
    raw_documents = fe_steps.query_data_warehouse(author_full_names)
    cleaned_documents = fe_steps.clean_documents(raw_documents)
     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)
    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)
    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)
    return [last_step_1.invocation_id, last_step_2.invocation_id]
====================================================
We can run feature_engineering from the command line, its easier to do this with a yaml file:

parameters:
  author_full_names:
    - Mathias
    - Nate
    - Hope

feature_engineering.with_options(config_path="…/feature_engineering.yaml")()

query_data_warehouse()
====================================================
… # other imports
from zenml import get_step_context, step

@step
def query_data_warehouse(
    author_full_names: list[str],
) -> Annotated[list, "raw_documents"]:
    documents = []
    authors = []
    for author_full_name in author_full_names:
        logger.info(f"Querying data warehouse for user: {author_full_name}")
        first_name, last_name = utils.split_user_full_name(author_full_name)
        logger.info(f"First name: {first_name}, Last name: {last_name}")
        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
        authors.append(user)
        results = fetch_all_data(user)
        user_documents = [doc for query_result in results.values() for doc in query_result]
        documents.extend(user_documents)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="raw_documents", metadata=_get_metadata(documents))
    return documents
====================================================

Multi-threaded fetch_all_data() each data source will have its own GIL and run in parallel

====================================================
def fetch_all_data(user: UserDocument) -> dict[str, list[NoSQLBaseDocument]]:
    user_id = str(user.id)
    with ThreadPoolExecutor() as executor:
        future_to_query = {
            executor.submit(__fetch_articles, user_id): "articles",
            executor.submit(__fetch_posts, user_id): "posts",
            executor.submit(__fetch_repositories, user_id): "repositories",
        }
        results = {}
        for future in as_completed(future_to_query):
            query_name = future_to_query[future]
            try:
                results[query_name] = future.result()
            except Exception:
                logger.exception(f"'{query_name}' request failed.")
                results[query_name] = []
    return results
====================================================

_get_metadata():
Counts the number of documents related to each author relative to category
useful for monitoring/debugging
====================================================
def _get_metadata(documents: list[Document]) -> dict:
    metadata = {
        "num_documents": len(documents),
    }
    for document in documents:
        collection = document.get_collection_name()
        if collection not in metadata:
            metadata[collection] = {}
        if "authors" not in metadata[collection]:
            metadata[collection]["authors"] = list()
        metadata[collection]["num_documents"] = metadata[collection].get("num_documents", 0) + 1
        metadata[collection]["authors"].append(document.author_full_name)
    for value in metadata.values():
        if isinstance(value, dict) and "authors" in value:
            value["authors"] = list(set(value["authors"]))
    return metadata
====================================================

clean_documents()
CleaningDispatcher will know how to clean each data source
====================================================
@step
def clean_documents(
    documents: Annotated[list, "raw_documents"],
) -> Annotated[list, "cleaned_documents"]:
    cleaned_documents = []
    for document in documents:
        cleaned_document = CleaningDispatcher.dispatch(document)
        cleaned_documents.append(cleaned_document)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="cleaned_documents", metadata=_get_metadata(cleaned_documents))
    return cleaned_documents
====================================================

chunk_and_embed()
ChunkingDispatcher/EmbeddingDispatcher knows how to handle each source
====================================================
@step
def chunk_and_embed(
    cleaned_documents: Annotated[list, "cleaned_documents"],
) -> Annotated[list, "embedded_documents"]:
    metadata = {"chunking": {}, "embedding": {}, "num_documents": len(cleaned_documents)}
    embedded_chunks = []
    for document in cleaned_documents:
        chunks = ChunkingDispatcher.dispatch(document)
        metadata["chunking"] = _add_chunks_metadata(chunks, metadata["chunking"])
        for batched_chunks in utils.misc.batch(chunks, 10):
            batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks)
            embedded_chunks.extend(batched_embedded_chunks)
    metadata["embedding"] = _add_embeddings_metadata(embedded_chunks, metadata["embedding"])
    metadata["num_chunks"] = len(embedded_chunks)
    metadata["num_embedded_chunks"] = len(embedded_chunks)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="embedded_documents", metadata=metadata)
    return embedded_chunks
====================================================

Metadata can save so much time when debugging.
load_to_vector_db()
====================================================
@step
def load_to_vector_db(
    documents: Annotated[list, "documents"],
) -> None:
    logger.info(f"Loading {len(documents)} documents into the vector database.")
    grouped_documents = VectorBaseDocument.group_by_class(documents)
    for document_class, documents in grouped_documents.items():
        logger.info(f"Loading documents into {document_class.get_collection_name()}")
        for documents_batch in utils.misc.batch(documents, size=4):
            try:
                document_class.bulk_insert(documents_batch)
            except Exception:
                return False
    return True
====================================================

domain-driven design (DDD) state that domain entities are the core of your application.

Pydantic for type validation at runtime makes your system more robust

We use abstract classes in case we need to easily extend to other data sources
====================================================
class CleanedDocument(VectorBaseDocument, ABC):
    content: str
    platform: str
    author_id: UUID4
    author_full_name: str
class CleanedPostDocument(CleanedDocument):
    image: Optional[str] = None
    class Config:
        name = "cleaned_posts"
        category = DataCategory.POSTS
        use_vector_index = False
class CleanedArticleDocument(CleanedDocument):
    link: str
    class Config:
        name = "cleaned_articles"
        category = DataCategory.ARTICLES
        use_vector_index = False
class CleanedRepositoryDocument(CleanedDocument):
    name: str
    link: str
    class Config:
        name = "cleaned_repositories"
        category = DataCategory.REPOSITORIES
        use_vector_index = False
====================================================
class Chunk(VectorBaseDocument, ABC):
    content: str
    platform: str
    document_id: UUID4
    author_id: UUID4
    author_full_name: str
    metadata: dict = Field(default_factory=dict)
… # PostChunk, ArticleChunk, RepositoryChunk
class EmbeddedChunk(VectorBaseDocument, ABC):
    content: str
    embedding: list[float] | None
    platform: str
    document_id: UUID4
    author_id: UUID4
    author_full_name: str
    metadata: dict = Field(default_factory=dict)
… # EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk
====================================================
class DataCategory(StrEnum):
    POSTS = "posts"
    ARTICLES = "articles"
    REPOSITORIES = "repositories"
====================================================

OVM is inspired by ORM but instead of SQL and structured data we workd with embedding and vector DBs

VectorBaseDocument supports CRUD operations ontop of Qdrant
====================================================
from pydantic import UUID4, BaseModel
from typing import Generic
from llm_engineering.infrastructure.db.qdrant import connection

T = TypeVar("T", bound="VectorBaseDocument")
class VectorBaseDocument(BaseModel, Generic[T], ABC):
    id: UUID4 = Field(default_factory=uuid.uuid4)
    @classmethod
    def from_record(cls: Type[T], point: Record) -> T:
        _id = UUID(point.id, version=4)
        payload = point.payload or {}
        attributes = {
            "id": _id,
            **payload,
        }
        if cls._has_class_attribute("embedding"):
            payload["embedding"] = point.vector or None
        return cls(**attributes)
    def to_point(self: T, **kwargs) -> PointStruct:
        exclude_unset = kwargs.pop("exclude_unset", False)
        by_alias = kwargs.pop("by_alias", True)
        payload = self.dict(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
        _id = str(payload.pop("id"))
        vector = payload.pop("embedding", {})
        if vector and isinstance(vector, np.ndarray):
            vector = vector.tolist()
        return PointStruct(id=_id, vector=vector, payload=payload)

    @classmethod
    def bulk_insert(cls: Type[T], documents: list["VectorBaseDocument"]) -> bool:
        try:
            cls._bulk_insert(documents)
        except exceptions.UnexpectedResponse:
            logger.info(
                f"Collection '{cls.get_collection_name()}' does not exist. Trying to create the collection and reinsert the documents."
            )
            cls.create_collection()
            try:
                cls._bulk_insert(documents)
            except exceptions.UnexpectedResponse:
                logger.error(f"Failed to insert documents in '{cls.get_collection_name()}'.")
                return False
        return True

    @classmethod
    def _bulk_insert(cls: Type[T], documents: list["VectorBaseDocument"]) -> None:
        points = [doc.to_point() for doc in documents]
        connection.upsert(collection_name=cls.get_collection_name(), points=points)
    
    @classmethod
    def get_collection_name(cls: Type[T]) -> str:
        if not hasattr(cls, "Config") or not hasattr(cls.Config, "name"):
            raise ImproperlyConfigured(
                "The class should define a Config class with" "the 'name' property that reflects the collection's name."
            )
        return cls.Config.name

    @classmethod
    def bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:
        try:
            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)
        except exceptions.UnexpectedResponse:
            logger.error(f"Failed to search documents in '{cls.get_collection_name()}'.")
            documents, next_offset = [], None
        return documents, next_offset

    @classmethod
    def _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:
        collection_name = cls.get_collection_name()
        offset = kwargs.pop("offset", None)
        offset = str(offset) if offset else None
        records, next_offset = connection.scroll(
            collection_name=collection_name,
            limit=limit,
            with_payload=kwargs.pop("with_payload", True),
            with_vectors=kwargs.pop("with_vectors", False),
            offset=offset,
            **kwargs,
        )
        documents = [cls.from_record(record) for record in records]
        if next_offset is not None:
            next_offset = UUID(next_offset, version=4)
        return documents, next_offset

    @classmethod
    def search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:
        try:
            documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)
        except exceptions.UnexpectedResponse:
            logger.error(f"Failed to search documents in '{cls.get_collection_name()}'.")
            documents = []
        return documents

    @classmethod
    def _search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:
        collection_name = cls.get_collection_name()
        records = connection.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            with_payload=kwargs.pop("with_payload", True),
            with_vectors=kwargs.pop("with_vectors", False),
            **kwargs,
        )
        documents = [cls.from_record(record) for record in records]
        return documents
====================================================

The dispatcher layer
CleaningDispatcher
(ChunkingDispatcher/EmbeddingDispatcher follow the same pattern)

====================================================
class CleaningDispatcher:
    cleaning_factory = CleaningHandlerFactory()
    @classmethod
    def dispatch(cls, data_model: NoSQLBaseDocument) -> VectorBaseDocument:
        data_category = DataCategory(data_model.get_collection_name())
        handler = cls.cleaning_factory.create_handler(data_category)
        clean_model = handler.clean(data_model)
        logger.info(
            "Data cleaned successfully.",
            data_category=data_category,
            cleaned_content_len=len(clean_model.content),
        )
        return clean_model
====================================================
# a match case would work here just fine

class CleaningHandlerFactory:
    @staticmethod
    def create_handler(data_category: DataCategory) -> CleaningDataHandler:
        if data_category == DataCategory.POSTS:
            return PostCleaningHandler()
        elif data_category == DataCategory.ARTICLES:
            return ArticleCleaningHandler()
        elif data_category == DataCategory.REPOSITORIES:
            return RepositoryCleaningHandler()
        else:
            raise ValueError("Unsupported data type")

====================================================

CleaningDataHandler

====================================================
# Other imports.
from typing import Generic, TypeVar

DocumentT = TypeVar("DocumentT", bound=Document)
CleanedDocumentT = TypeVar("CleanedDocumentT", bound=CleanedDocument)
class CleaningDataHandler(ABC, Generic[DocumentT, CleanedDocumentT]):
    @abstractmethod
    def clean(self, data_model: DocumentT) -> CleanedDocumentT:
        pass

====================================================
class PostCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: PostDocument) -> CleanedPostDocument:
        return CleanedPostDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(data_model.content.values())),
            # Copy the rest of the parameters from the data_model object.
        )
class ArticleCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: ArticleDocument) -> CleanedArticleDocument:
        valid_content = [content for content in data_model.content.values() if content]
        return CleanedArticleDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(valid_content)),
            platform=data_model.platform,
            link=data_model.link,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
        )
class RepositoryCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: RepositoryDocument) -> CleanedRepositoryDocument:
        return CleanedRepositoryDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(data_model.content.values())),
            # Copy the rest of the parameters from the data_model object.
        )
====================================================

ChunkingDataHandler

====================================================
# Other imports.
from typing import Generic, TypeVar

CleanedDocumentT = TypeVar("CleanedDocumentT", bound=CleanedDocument)
ChunkT = TypeVar("ChunkT", bound=Chunk)
 class ChunkingDataHandler(ABC, Generic[CleanedDocumentT, ChunkT]):
    @property
    def metadata(self) -> dict:
        return {
            "chunk_size": 500,
            "chunk_overlap": 50,
        }
    @abstractmethod
    def chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:
        pass
====================================================
class ArticleChunkingHandler(ChunkingDataHandler):
    @property
    def metadata(self) -> dict:
        return {
            "min_length": 1000,
            "max_length": 1000,
        }
    def chunk(self, data_model: CleanedArticleDocument) -> list[ArticleChunk]:
        data_models_list = []
        cleaned_content = data_model.content
        chunks = chunk_article(
            cleaned_content, min_length=self.metadata["min_length"], max_length=self.metadata["max_length"]
        )
        for chunk in chunks:
            chunk_id = hashlib.md5(chunk.encode()).hexdigest()
            model = ArticleChunk(
                id=UUID(chunk_id, version=4),
                content=chunk,
                platform=data_model.platform,
                link=data_model.link,
                document_id=data_model.id,
                author_id=data_model.author_id,
                author_full_name=data_model.author_full_name,
                metadata=self.metadata,
            )
            data_models_list.append(model)
        return data_models_list
====================================================

regex removes punctuation, abbrevations, until max_length is reached

====================================================
def chunk_article(text: str, min_length: int, max_length: int) -> list[str]:
    sentences = re.split(r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s", text)
    extracts = []
    current_chunk = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += sentence + " "
        else:
            if len(current_chunk) >= min_length:
                extracts.append(current_chunk.strip())
            current_chunk = sentence + " "
    if len(current_chunk) >= min_length:
        extracts.append(current_chunk.strip())
    return extracts
====================================================

The other chunking handlers use chunk_text()

RecursiveCharacterTextSplitter() from LangChain split the text
based on a given seperator or chunk size, if too long it cuts

SenteceTransformersTokenTextSplitter() considers the max input length
====================================================

# Other imports.
from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter
from llm_engineering.application.networks import EmbeddingModelSingleton

def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list[str]:
    character_splitter = RecursiveCharacterTextSplitter(separators=["\n\n"], chunk_size=chunk_size, chunk_overlap=0)
    text_split_by_characters = character_splitter.split_text(text)
    token_splitter = SentenceTransformersTokenTextSplitter(
        chunk_overlap=chunk_overlap,
        tokens_per_chunk=embedding_model.max_input_length,
        model_name=embedding_model.model_id,
    )
    chunks_by_tokens = []
    for section in text_split_by_characters:
        chunks_by_tokens.extend(token_splitter.split_text(section))
    return chunks_by_tokens
====================================================

when calling the embedding model, we want to batch as many samples as possible to optimize the inference process. GPU can parallize this process

EmbeddingDataHandler()
====================================================

# Other imports.
from typing import Generic, TypeVar, cast
from llm_engineering.application.networks import EmbeddingModelSingleton

ChunkT = TypeVar("ChunkT", bound=Chunk)
EmbeddedChunkT = TypeVar("EmbeddedChunkT", bound=EmbeddedChunk)
embedding_model = EmbeddingModelSingleton()
class EmbeddingDataHandler(ABC, Generic[ChunkT, EmbeddedChunkT]):
    """
    Abstract class for all embedding data handlers.
    All data transformations logic for the embedding step is done here
    """
    def embed(self, data_model: ChunkT) -> EmbeddedChunkT:
        return self.embed_batch([data_model])[0]
    def embed_batch(self, data_model: list[ChunkT]) -> list[EmbeddedChunkT]:
        embedding_model_input = [data_model.content for data_model in data_model]
        embeddings = embedding_model(embedding_model_input, to_list=True)
        embedded_chunk = [
            self.map_model(data_model, cast(list[float], embedding))
            for data_model, embedding in zip(data_model, embeddings, strict=False)
        ]
        return embedded_chunk
    @abstractmethod
    def map_model(self, data_model: ChunkT, embedding: list[float]) -> EmbeddedChunkT:
        pass
====================================================
class ArticleEmbeddingHandler(EmbeddingDataHandler):
    def map_model(self, data_model: ArticleChunk, embedding: list[float]) -> EmbeddedArticleChunk:
        return EmbeddedArticleChunk(
            id=data_model.id,
            content=data_model.content,
            embedding=embedding,
            platform=data_model.platform,
            link=data_model.link,
            document_id=data_model.document_id,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
            metadata={
                "embedding_model_id": embedding_model.model_id,
                "embedding_size": embedding_model.embedding_size,
                "max_input_length": embedding_model.max_input_length,
            },
        )
====================================================

Writing a wrapper over external packages is often good practice.

from sentence_transformers.SentenceTransformer import SentenceTransformer
from llm_engineering.settings import settings
from .base import SingletonMeta

class EmbeddingModelSingleton(metaclass=SingletonMeta):
    def __init__(
        self,
        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,
        device: str = settings.RAG_MODEL_DEVICE,
        cache_dir: Optional[Path] = None,
    ) -> None:
        self._model_id = model_id
        self._device = device
        self._model = SentenceTransformer(
            self._model_id,
            device=self._device,
            cache_folder=str(cache_dir) if cache_dir else None,
        )
        self._model.eval()
    @property
    def model_id(self) -> str:
        return self._model_id
    @cached_property
    def embedding_size(self) -> int:
        dummy_embedding = self._model.encode("")
        return dummy_embedding.shape[0]
    @property
    def max_input_length(self) -> int:
        return self._model.max_seq_length
    @property
    def tokenizer(self) -> AutoTokenizer:
        return self._model.tokenizer
    def __call__(
        self, input_text: str | list[str], to_list: bool = True
    ) -> NDArray[np.float32] | list[float] | list[list[float]]:
        try:
            embeddings = self._model.encode(input_text)
        except Exception:
            logger.error(f"Error generating embeddings for {self._model_id=} and {input_text=}")
            return [] if to_list else np.array([])
        if to_list:
            embeddings = embeddings.tolist()
        return embeddings
----------------------------------------------------------
Supervised Fine-Tuning (SFT)
refines the model's capabilities using carefully curated pairs of instructions and corresponding answers. This teaches the model to understand and follow a specific chat format transforming it into a conversation, the model adapts its broad knowledge base to excel in targeted tasks or specialized domains.

Shapes LLMs behavior to align with specific goals.

Instruction datasets are defined as pairs of instructions and answers.

Here is an example from the SlimOrca dataset, with "system" and "instruction":
System
You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.

Instruction
Concepts: building, shop, town
Write a sentence that includes all these words.

Output
In our little town, there is a shop inside a big building where people go to buy their favorite toys and candies.

Once we have enough samples we only keep the high-quality:
- Accuracy
	- making sure responses are factually accurate
	- relevant to instructions
	- unlike FOX, CNBC, any media, we on the other hand want to be trustworthy
- Diversity
	- span different topics
	- contexts
	- writing styles
- Complexity
	- multi-step reasoning
	- complex real world problems

Hugging Face Hub contains numerous instruction datasets
The quality of the data is a crucial factor, and a high number of samples is always desirable.

Two types of finetunes: general-purpose, domain specific
Some fields, like medicine or law, may require as much data as general-purpose fine-tuning due to their vast technical corpora.

task specific models: collecting examples of the desired task from existing datasets or creating new ones.

Domain specific models: requires collaboration with subject matter experts to gather and validate relevant texts, research papers, technical documents, and other domain specific content.

few shot prompting: providing a few examples of the desired task within the input prompt.

Sometimes a blury line between task/domain models

rule based filtering relies on explicit predefined rules to filter data
- length filtering setting thresholds for length of responses in the dataset
	this can vary significantly depending on task/domain
- keyword exclusion focuses on content rather than their structure
	filter inappropriate content, low quality terms (profanities, spam, off topic).
- Format checking structured data follow specific formatting to maintain consistency
	code samples that are syntactiaclly correct and follow a specific style

Predefined rules may lack the nuance required to capture full complexity of language and context could remove valid but unusual samples, rules need to always be reviewed because of the nature of data and quality standards evolve

Data deduplication
----------------------------------------------------------
issues with duplicates or near-duplicates:
	- Overfitting
	- Biased performance
	- Inefficient training
	- Inflated evaluation metrics

Exact deduplication removes identifical sample through
	- data normalization
	- hash generation
	- duplicate remove

Fuzzy deduplication is MinHash deduplication (most popular)
generates compact representations or signatures for each data item, transforms data items into sets of shingles, applies multiple hash functions then selects the min has values to form signature vectors. These signatures are compared using similarity measures like Jaccard similarity to find near-duplicates

Semantic similarity
focuses on meaning of text for dedup, words -> vector representations using natural language processing, Word2Vec, GloVe, and FastText, words -> dense vectors

Context-aware representations like BERT, sentence transformers, or cross-encoders can generate embedding for entire sentences or documents. dedup can be performed by comparing similarity between vectors (cosine distance). Clustering techniques may be applied to group similar vectors (K-means and others).

Data decontamination
----------------------------------------------------------
Preventing overfitting or memorization of test data by ensuring that the training dataset does not contain identicals or highly similar to those in the evaluation or test sets.

Remove similar training samples that are similar to evaluation samples could involve using MinHash or computing scores based on n-grames or embeddings

Add your evaluation set to the instruction dataset during the data deduplication stage. Automatically add your evaluation sets in the data deduplication stage to fully automate this process.

----------------------------------------------------------
LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. Example:

====================================================
Instruction
You are a data quality evaluator. Your goal is to assess an instruction and its corresponding answer, determining how effectively the answer addresses the given task.
In your evaluation, you will provide feedback detailing the strengths and weaknesses of the answer, followed by a score on a scale of 1 to 4.
A score of 1 means that the answer is terrible and irrelevant to the instruction.
A score of 2 means that the answer is not helpful and misses important aspects of the instruction.
A score of 3 means that the answer is helpful but could be improved in terms of relevance, accuracy, and depth.
A score of 4 means that the answer is excellent and fully addresses the task.
Provide your evaluation as follows:
Feedback: (strengths and weaknesses you find relevant)
Score: (number between 1 and 4)
====================================================
LLM-as-a-judge is known to have several biases
- Position bias in comparative scoring
	Favors first answer, randomize the order of answers A and B
- Favors long answers
	length normalizations can be applied
- intra-model favoritism
	- prefer models from the same family
Using multiple LLMs as a jury reduced bias and improve consistency.
Reward models are another way to re-purpose LLMs for data quality take an instruction and answer pair and return a score as output. Score could be multiple scores helpfulness, correctness, coherence, complexity etc.

Hugging Face is a good resource for comparing different reward models.

Classifiers or encoder-only models can be trained to perform data quality evaluation.
At smaller scale, encoder-only models are still valuable to filter out outliers or as part of an automated data pipeline, which requires faster processing.

Manual dataset exploration, is time consuming (important step). It reveals errors and inconsistencies that automated processes might miss, including formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies.

Argilla A collaborative platform for manual data quality evaluation and exploration.

Statisitcal analysis reveals vocab diversity, potential biases, concept representation. Utilizes natrual language processing libraries for tokenization and analysis of large text volumes. Visualization tools such as matplotlib or Seaborn uses histograms and word clouds.

Topic clustering reveals patterns

Synthetic data generation using LLMs offers a more efficient and scalable alternative than crowdsourcing. Process begins with preparation of a set of carefully designed promprts (taxonomy).
Useful for addressing biases and gaps in existing datasets. its possible to create more balanced and inclusive datasets that represent a wider range of perspectives, topics, and language styles.

Data augmentation
---------------------------------------------------------
In-depth evolving enchancing the complexity of existing instructions
	- Constraints
		introducing additional requirements or limitations to the original instruction
	- Deepening
		finds more deep questions, requiring more comprehensive repsonses
	- Concretizing
		replaces general concepts with more specific ones, adding detail to the instructtion
	- Increasing reasoning steps
		explicity requires multiple-step reasoning promoting more complex problem solving
	- Complicated input
		adding more complex data formats or structures to the instruction code snippets, JSON, LaTeX

In-breadth evolving expands the diversity of the instruction dataset

UltraFeedback method focused on answer quality instead of instruction quality. 

LLMs are not reliable when it comes to producing structured output.
Structured generation is an effective method to force an LLM to follow a predefined template, such as JSON, pydantic classes, or regular expressions.

synthetic data pipeline
====================================================
openai==1.37.1 (interact with model)
datasets==2.20.0 (format it into a Hugging Face compatible)
tqdm==4.66.4 (visualize)
====================================================
import concurrent.futures
import json
import random
import re
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple
from datasets import Dataset
from openai import OpenAI
from pydantic import BaseModel, Field
from tqdm.auto import tqdm

def load_articles_from_json(file_path: str) -> Dataset:
    with open(file_path, "r") as file:
        data = json.load(file)
    return Dataset.from_dict(
        {
            "id": [item["id"] for item in data["artifact_data"]],
            "content": [item["content"] for item in data["artifact_data"]],
            "platform": [item["platform"] for item in data["artifact_data"]],
            "author_id": [item["author_id"] for item in data["artifact_data"]],
            "author_full_name": [item["author_full_name"] for item in data["artifact_data"]],
            "link": [item["link"] for item in data["artifact_data"]],
        }
    )

def clean_text(text):
    text = re.sub(r"[^\w\s.,!?']", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:
    extracts = []
    sentence_pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
    for article in dataset["content"]:
        cleaned_article = clean_text(article)
        sentences = re.split(sentence_pattern, cleaned_article)
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence + " "
            else:
                if len(current_chunk) >= min_length:
                    extracts.append(current_chunk.strip())
                current_chunk = sentence + " "
        if len(current_chunk) >= min_length:
            extracts.append(current_chunk.strip())
    return extracts
====================================================
instruction-answer pairs

InstructionAnswerSet
====================================================
class InstructionAnswerSet:
    def __init__(self, pairs: List[Tuple[str, str]]):
        self.pairs = pairs

    @classmethod
    def from_json(cls, json_str: str) -> 'InstructionAnswerSet':
        data = json.loads(json_str)
        pairs = [(pair['instruction'], pair['answer'])
                 for pair in data['instruction_answer_pairs']]
        return cls(pairs)

    def __iter__(self):
        return iter(self.pairs)
====================================================

generate five instruction and answer pairs for each extract

====================================================
def generate_instruction_answer_pairs(
    extract: str, client: OpenAI
) -> List[Tuple[str, str]]:
    prompt = f"""Based on the following extract, generate five instruction-answer pairs. Each instruction \
must ask to write about a specific topic contained in the context. each answer \
must provide a relevant paragraph based on the information found in the \
context. Only use concepts from the context to generate the instructions. \
Instructions must never explicitly mention a context, a system, a course, or an extract. \
Instructions must be self-contained and general. \
Answers must imitate the writing style of the context. \
Example instruction: Explain the concept of an LLM Twin. \
Example answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \
It's designed to write just like you by incorporating these elements into a language model. \
The idea is to create a digital replica of your writing habits using advanced AI techniques. \
Provide your response in JSON format with the following structure:
{{
    "instruction_answer_pairs": [
        {{"instruction": "...", "answer": "..."}},
        ...
    ]
}}
Extract:
{extract}
"""
====================================================

System prompt

parsed using the InstructionAnswerSet class
====================================================
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
        	"role": "system",
        	"content": "You are a helpful assistant who \
        	generates instruction-answer pairs based on the given context. \
        	Provide your response in JSON format.",
        },
        {
        	"role": "user",
        	"content": prompt
    	},
    ],
    response_format={"type": "json_object"},
    max_tokens=1200,
    temperature=0.7,
)
# Parse the structured output
result = InstructionAnswerSet.from_json(completion.choices[0].message.content)
# Convert to list of tuples
return result.pairs
====================================================

Creating create_instruction_dataset() to automate the process!
extracts substrings from the input dataset

====================================================
def create_instruction_dataset(
    dataset: Dataset, client: OpenAI, num_workers: int = 4
) -> Dataset:
    extracts = extract_substrings(dataset)
    instruction_answer_pairs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(generate_instruction_answer_pairs, extract, client)
            for extract in extracts
        ]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)
        ):
            instruction_answer_pairs.extend(future.result())
    instructions, answers = zip(*instruction_answer_pairs)
    return Dataset.from_dict(
        {"instruction": list(instructions), "output": list(answers)}
    )
====================================================

orchestrate the pipeline, loads raw data, creates instruction
dataset, splits into training and testing sets pushes to Hugging Face Hub

====================================================
def main(dataset_id: str) -> Dataset:
    client = OpenAI()
    # 1. Load the raw data
    raw_dataset = load_articles_from_json("cleaned_documents.json")
    print("Raw dataset:")
    print(raw_dataset.to_pandas())
    # 2. Create instructiondataset
instruction_dataset = create_instruction_dataset(raw_dataset, client)
    print("Instruction dataset:")
    print(instruction_dataset.to_pandas())
    # 3. Train/test split and export
    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)
    filtered_dataset.push_to_hub("MemoryOverflow/llmtwin")
    return filtered_dataset
Dataset({
    features: ['instruction', 'output'],
    num_rows: 3335
})
====================================================
Hugging Face Hub provides a convenient dataset viewer
Instruction  |  Output

SFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instructions and answers. 
	- Low-Rank Adaptation (LoRA)
	- Quantization-aware Low-Rand Adaptation (QLoRA)

Instead of building applications around a chatbot, fine-tuning allows developers to create more diverse interactions with LLMs, like tool analytics, moderation, and additional context. 

Everything has limitations
fine-tuning also has limitations
 - knowledge that is too distant from what has been learned in the pre-training set (such as an unknown or rare language) can be difficult to learn effectively.

fine-tuning a model on new knowledge could result in more frequent hallucinations

Chat templates offer a unified way to present the instructions and answers to the model. Special tokens to identify the beginning and the end of a message, and author of the message. Base models are not designed to follow instructions so you can choose any template. Not recommended to fine-tune an instruct model

example:
====================================================
<|im_start|>system
You are a helpful assistant, who always provide explanation. Think like you are answering to a nine year old.<|im_end|>
<|im_start|>user
Concepts: cat, bed, play
Write a sentence that includes all these words.<|im_end|>
<|im_start|>assistant
Before bed I like to play with my cat.<|im_end|>
====================================================
Now understands that the next tokens should be an answer relevant to the user instruction and guided by the system prompt.
This is how fine-tuned models acquire instruction following capabilities.

issue with chat templates:
Whitespace and line break is very important. Adding or removing any character would result in a wrong tokenization, which negatively impacts the performance of the model. Use something like Jinja. different types of templates
	- Alpaca
	- ChatML
	- Llama3
	- Phi-3
	- Gemma

Full fine-tuning
---------------------------------------------------------
Re-training every parameter in the base model.
SFT uses next-token prediction as its training objective.
This method often provides the best results but requires significant computational resources.

Memory = Parameters + Gradients + Optimizer States + Activations

- Paramters
	Learnable weights and biases within a neural network
- Gradients
	Partial derivatives of the loss function with respect to each model parameter
	minimize loss
	computed for each parameter backwards propagation
- Optimizer States
	Additional values maintained by optimization algorithms (Adam or AdamW)
- Activations
	outputs of each layer during foward pass
	kept in memory to compute gradients in backward pass

How to reduce memory usuage during LLM fine-tuning?
- Model parallelism (adds some overhead)
- Gradient accumlation (enables larger batch sizes)
- Optimizers like 8-bit Adam
- Activation checkpointing (trades computation for memory)

Full fine-tuning directly modifies the pre-training weights, which makes it destructive by nature. If training doesn’t behave as expected, it might erase previous knowledge and skills "catastrophic forgetting."

LoRA (fine-tuning)
---------------------------------------------------------
Enabling fine-tuning of LLMs with significanlty reduced computational resources.
trainable low-rank matrices that modify behavior of the model without changing original parameters
- Dramatically reduced memory during training
- faster fine-tuning
- preservation of pre-trained model weights (non-descructive)
- ability to switch between tasks efficiently by swapping LoRA weights

W' = W + BA
W is original weight matrix
B and A are LoRA matrices
W' effective weight matrix used during inference

A and B are chosen to have the same shape as W but with lower rank
this rank is denoted as r
original W remains frozen, and only A and B are updated

Hyperparameters:
- Rank (r)
	size of LoRA matrices common to start r=8
- Alpha (a)
	requires experimentation, this is a scaling factor a/r. Its common to set this valuie twice the value of r

Can add a drop out layer between 0 - 0.1 as an optional regularization factor

LoRA is primarily focused on query (Q) and value (V) in the transformer layers
additional targets:
- Key (K) matrices in attention layers
- Output projection layers (O)
- Feed-forward or multi-layer perceptron (MLP) between attention layers
- linear output layers

LoRA could produce better results than full-tuning
LoRAX multiple-LoRA serving
A feature supported by Hugging Face's Text Generation Inference (TGI) and Nvidia Inference Microservices (NM)

QLoRA (fine-tuning)
---------------------------------------------------------
fine-tune models on relatively small, widely available GPUs

quantizing the base model parameters to a custom 4-bit NormalFloat (NF4) data type
QLoRA indroduces small, trainable low-rank matrices (adapters) to specific layers
only these adapters are updating during training.
double quantization, quantizes the quantization constants
uses page optimizer to manage memory spikes during training Nvidia's unified memory feature

significant memory saves compared to LoRA
usually benefical when memory constraints are a primary concern

Difference between the two should be on the specific requirements of the project, available hardware, memory usage, speed, performance.

Learning rate and scheduler
---------------------------------------------------------
A hyperparameter, it controls how much the model's parameters are updated during training, ranges from small values: 1e-6 to 1e-3 a common starting point is 1e-5, if learning rate to low it may get stuck, to high it might diverge

Using a learning rate schedule often leads to faster convergence and better performance than using a fixed learning rate.

Learning rate schedule adjusts learning rate throughout the training process, start with higher learning rate then gradually decreases in later stages, two common types of schedulers:
- Linear
	decreases the learning rate steadily over time
- Cosine
	decresing slowly at first then more rapid toward end of training
both have same level of performance

Warmup and Decay:
During the initial stages, the model is far from optimal. A small or gradually increasing learning rate helps the model explore the solution space without instability.

Later in training, the model is closer to an optimal solution. A smaller learning rate allows for more precise adjustments to the parameters.

(LR: Learning Rate)
Warmup:
for 5% of training steps learning rate increases linearly:
LR = (stepnumber/warmupsteps) x initial LR

Decay:
remaining 95% the learning rate descreases following a chosen schedule (e.g. exponential decay):
LR = initial LR x exp(-(stepnumber - warmupsteps/decay constant))

used in large-scale pretraining (e.g. transformers)
finetuning on specific tasks or datasets

Batch size
---------------------------------------------------------
Number of samples processed before the model's weights are updated
range from 1-32 common values 1,2,4,8 or 16 larger batch sizes lead to more stable gradient estimates and can improve training speed, as they provide a better approximation of the true gradient of the entire dataset, requires more memory.

gradient accumulation can be used to mitigate memory constraints from larger batch sizes. Performs multiple forward and backward passes with smaller mini-batches, accumulating the gradients. More accumulation steps allow for larger effective batch sizes but increase the time required for each update.

Effective Batch Size = Batch Size x #GPUs x Gradient Accumulation Steps
say your using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accumulation steps you get 4 * 2 * 4 = 32 samples

Max length and packing
---------------------------------------------------------
longest input the model can process, this is set between 512 and 4096 tokens but can go up to 128,000+ max length of 2,048 tokens is common, RAG applications can go up to 8,192+

Longer input are truncated, this can happen to the left or right

Packing combines multiple smaller smaples into a single batch, increasing the amount of data processed in each iteration. If max sequence length is 1024 tokens but many of your samples are only 200-300 tokens long packing allows you to fit 3-4 samples into each batch slot. This improves training efficiency.

Packing requires careful implementation to ensure that model attention doesn't cross between packed samples. This can be mitigated by attention masks from attending to toakes from different sampels within the same packed sequence

Number of Epochs
---------------------------------------------------------
Number of complete passes through the entire training dataset.
LLM fine-tuning range 1-10 epochs, too few underfit, to much overfit.
this will depend on model, datasize, complexity

Optimizers
---------------------------------------------------------
Adjusts the model's parameters to minimize the loss function,
for LLM AdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended
combines adaptive learning rates with weight decay

AdaFactor is designed for memory efficency, it may not always match AdamW performance
if max performace is needed the non-quantized adamw_torch optimizer may be the best.

Weight decay
---------------------------------------------------------
You'll have to experiment with values but this works by adding a penalty for large weights to the loss function, encouraging the model to learn simpler, more generalizable features. Can improve performance on unseen data. 0.01 to 0.1

Gradient checkpointing
---------------------------------------------------------
Reduces memory consumption during training by storing a subset of intermediate activations by forward pass. its standard that all intermediate activations are retained in memory to facilitate gradient calculation during the backward pass.

selectively saving activation at specific layers within the network, if not saved they are re-computed during the backward pass for gradient computation. Has a trade-off between computation time and memory usage.

It may increase overall computation but reduces memory

---------------------------------------------------------

To select the most relevant LLM, we need to consider:
- License
- Budget
- Performance

Specialized tools and libraries to fine-tune models:
- TRL (popular)
	Hugging Face to train LLMs using SFT and perference alignment
- Axolotl
	fine-tune LLM with reusable YAML config files
- Unsloth
	uses custom kernels to speed up training (2-5x) and reduce memory by 80%
	only available for single-GPU settings.

====================================================
Using Unsloth
====================================================
import os
import torch
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets
from transformers import TrainingArguments, TextStreamerfrom unsloth import FastLanguageModel, is_bfloat16_supported

max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Meta-Llama-3.1-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=False, # using LoRA instead of QLoRA
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    lora_alpha=32,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)
# we have to load in more samples and only take 10000 samples
dataset1 = load_dataset("mlabonne/llmtwin")
dataset2 = load_dataset("mlabonne/FineTome-Alpaca-100k", split="train[:10000]")
dataset = concatenate_datasets([dataset1, dataset2])

# format data using a chat template, we manually add EOS (end of sentence)
# to ensure that the model learns to output it, otherwise it will keep generating answers without stopping
alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{}
### Response:
{}"""
EOS_TOKEN = tokenizer.eos_token
dataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)

# training 95%, 5% test
dataset = dataset.train_test_split(test_size=0.05)

# SFTTrainer stores all hyperparameters
trainer = SFTTrainer(
    model=model,
   tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=True,
    args=TrainingArguments(
        learning_rate=3e-4,
        lr_scheduler_type="linear",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        warmup_steps=10,
        output_dir="output",
        report_to="comet_ml",
        seed=0,
    ),
)
trainer.train()

# empty answer format() this forces model to answer the instruction instead of completing it
FastLanguageModel.for_inference(model)
message = alpaca_prompt.format("Write a paragraph to introduce supervised fine-tuning.", "")
inputs = tokenizer([message], return_tensors="pt").to("cuda")
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)
====================================================
Answer:
====================================================
Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's 
responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.
====================================================
# we can save our model locally and/or push to Hugging Face hub
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
model.push_to_hub_merged("MemoryOverload/TwinLlama-3.1-8B", tokenizer, save_method="merged_16bit")

You can use comet to visulaize different metrics
- Training Loss
	How well is the model doing on the task its being trained for
- Validation Loss
	measures the loss using the validation set instead of training set
	if taining loss continues to decrease while validation loss increase we overfitted
	vise versa underfitting 
- Gradient norm
	magnitude of the gradient vector during training
	A stable or decreasing gradient norm means the model is converging towards local optimum
	to mitigate large gradient norms use gradient clipping
---------------------------------------------------------

Preference alignment addresses shortcomings of SFT by adding direct human or Ai feedback into the training process

DPO datasets (Direct preference optimization)
Each instruction is paried with one preferred answer and one rejected answer, the objective is to train the model to generate the perferred response.

- Chatbots
	responses depends on subjective factors like naturalness, engagement, and context.
- Content moderation
	is content appropriate? Preference dataset can learn borderline cases
- Summarization
	is it concise, relevant, and coherent. Preference can learn to generate summaries
	SFT might result in summaries taht are correct but less perferable to humans
- Code generation
	multiple correct solutions, some more readable. Preference help model learn code quality
- Creative writting
	this is highly subjective, Preference datasets can capture human judgements about style, creativity, and emotional impact
- Translations
	Preference datasets can help produce translations that native speakers prefer

This is not only technically accurate but also better aligned with huiman judgment and preferences in complex open-ended tasks.

Data quantity
---------------------------------------------------------
Large number of preference pairs is beneficial for data quality
Nvidia and Meta are converging on similar post-training pipelines involving multiple rounds of perference alignment and extensive use of synthetic data.

DPO is less destructive than SFT and has a milder impact on the final model.

When creating preference datasets, data generation and evaluation are closely linked. We create answers and then rate them to make final dataset

Generating preferences
---------------------------------------------------------
its good to look at relevant open-source datasets before making new preference data
a well known one is the Anthropic HH-RLHF dataset, which has human preferences for helpful and harmless Ai responses and the OpenAI Summarize from Human Feedback dataset focuses on article summaries

DPO datasets can be created using various methods
- Human generated, human-evaluate datasets
	extermely resource-intensive and difficult to scale, involves hiteing people, but ideal for complex tasks
- Human generated, LLM-evaluated datasets
	useful if you have a lot of existing human-generated content, requires significant human input for response generation
- LMM generated, human-evaluated datasets
	good balance between quality and efficiency. LLMs generate responses to prompts and humans rank these. This is perferred method, humans are better judging answers than writing from scratch, may not provide creative or unexpected responses
- LLM generated, LLM-evauluated
	fully synthetic datasets, increasingly common due to their scalablility and cost-effectiveness. massive datasets quickly and improves as LLM capabilities advance

`Intel/orca_dpo_pairs` dataset on hugging face hub is an instruction-following Ai. Datasets like this are key to building models similar to OpenAI’s ChatGPT or Anthropic’s Claude, where human preferences play a crucial role in defining the quality of the output.

`Intel/orca_dpo_pairs` provides a foundational building block for preference-based tuning!

Introducing variability in the outputs is another crucial aspect of generating synthetic preference datasets. You can do this by manipulating the temperature settings or trying other sampling methods in the LLM. Higher tempature settings tend to produce more creative and diverse responses, lower settings result in more focused and deterministic outputs which creates a trade-off, depends also on the data you want to generate

Using multiple LLMs to generate samples can be better than just one model introduces variety. used by datasets like `argilla/Capybara-Preferences` combining GPT-4 with open weight models. The evaluation process selected the chosen and rejected answers

Pairwise ranking involves presenting the LLM with two responses and asking it to choose the better one or rank them
example of two approaches:
====================================================
"Rate the following response on a scale of 1-5 based on relevance, coherence, and helpfulness: [INSERT RESPONSE]." For pairwise ranking, the prompt could be: "Compare the following two responses. Which one is better in terms of relevance, coherence, and helpfulness? Response A: [INSERT RESPONSE A] Response B: [INSERT RESPONSE B]."
====================================================
Pairwise ranking is an ideal approach more closely correlated to human judgment than absolute scoring.

chain-of-thought reasoning encourges evaluating LLM to condsider multiple aspect of responses and articulate its decision making process leading to more thorough evaluations.

LLM-as-a-judge prompt to perform pairwise ranking:
====================================================
Instruction

You are an answer judge. Your goal is to compare answer A and answer B. I want to know which answer does a better job of answering the instruction in terms of relevance, accuracy, completeness, clarity, structure, and conciseness.
Instruction: {instruction}
Answer A: {answer_a}
Answer B: {answer_b}
Explain your reasoning step by step and output the letter of the best answer using the following structure:
Reasoning: (compare the two answers)
Best answer: (A or B)
====================================================
introduces bias
- Position bias
	LLM judges prefer first answer
- Length bias
	LLM prefers longer answers (overlooking shorter concise responses)
- Family bias
	LLM favor responses generated by themselves or models from same family

You can randomize the order, provide few-shot examples that demonstrate a balance distrubition of scores also can employ multiple models as a jury rather than relying on a single LLM judge

generation pipeline:
this time we need triples (instruction, answer1, answer2)
load_articles_from_json() and clean_text() are the same as before
====================================================
import concurrent.futures
import json
import re
from typing import List, Tuple
from datasets import Dataset
from openai import OpenAI
from tqdm.auto import tqdm

class PreferenceSet:
    def __init__(self, triples: List[Tuple[str, str, str]]):
        self.triples = triples

    @classmethod
    def from_json(cls, json_str: str) -> 'PreferenceSet':
        data = json.loads(json_str)
        triples = [(triple['instruction'], triple['generated_answer'], triple['extracted_answer'])
                   for triple in data['preference_triples']]
        return cls(triples)

    def __iter__(self):
        return iter(self.triples)

	def load_articles_from_json(file_path: str) -> Dataset:
	    with open(file_path, "r") as file:
	        data = json.load(file)
	    return Dataset.from_dict(
	        {
	            "id": [item["id"] for item in data["artifact_data"]],
	            "content": [item["content"] for item in data["artifact_data"]],
	            "platform": [item["platform"] for item in data["artifact_data"]],
	            "author_id": [item["author_id"] for item in data["artifact_data"]],
	            "author_full_name": [item["author_full_name"] for item in data["artifact_data"]],
	            "link": [item["link"] for item in data["artifact_data"]],
	        }
	    )

	def clean_text(text: str) -> str:
		text = re.sub(r"[^\w\s.,!?']", " ", text)
		text = re.sub(r"\s+", " ", text)
		return text.strip()

	def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:
	    extracts = []
	    sentence_pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
	    for article in dataset["content"]:
	        cleaned_article = clean_text(article)
	        sentences = re.split(sentence_pattern, cleaned_article)
	        current_chunk = ""
	        for sentence in sentences:
	            sentence = sentence.strip()
	            if not sentence:
	                continue
	            if len(current_chunk) + len(sentence) <= max_length:
	                current_chunk += sentence + " "
	            else:
	                if len(current_chunk) >= min_length:
	                    extracts.append(current_chunk.strip())
	                current_chunk = sentence + " "
	        if len(current_chunk) >= min_length:
	            extracts.append(current_chunk.strip())
	    return extracts

	def generate_preference_triples(extract: str, client: OpenAI) -> List[Tuple[str, str, str]]:
	    prompt = f"""Based on the following extract, generate five instruction-answer triples. Each triple should consist of:
		1. An instruction asking about a specific topic in the context.
		2. A generated answer that attempts to answer the instruction based on the context.
		3. An extracted answer that is a relevant excerpt directly from the given context.
		Instructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.
		Important:
		- Ensure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.
		- Do not add any ellipsis (...) or [...]  to indicate skipped text in the extracted answer.
		- If the relevant text is not continuous, use two separate sentences from the context instead of skipping text.
		Provide your response in JSON format with the following structure:
		{{
		    "preference_triples": [
		        {{
		            "instruction": "...",
		            "generated_answer": "...",
		            "extracted_answer": "..."
		        }},
		        ...
		    ]
		}}
		    Extract:
		    {extract}
		"""
		completion = client.chat.completions.create(
	        model="gpt-4o-mini",
	        messages=[
	            {
	                "role": "system",
	                "content": "You are a helpful assistant who generates instruction-answer triples based on the given context. Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format.",
	            },
	            {"role": "user", "content": prompt},
	        ],
	        response_format={"type": "json_object"},
	        max_tokens=2000,
	        temperature=0.7,
	    )
	    result = PreferenceSet.from_json(completion.choices[0].message.content)
	    return result.triples








