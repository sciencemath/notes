Lecture 1
Complex data types and Cumulation
--------------------------------------------

Dimensions are attributes of an entity (e.g. user's bday, fav food)
some may identify an entity (user's ID)
Dimensions can be slow changing (e.g. time dependent)
Dimensions can be fixed, like your bday, a phone manufacture

Knowing your customer for data (how is your data being used)
- Data analysts / data scientists
should be easy to query, this is probabily flat
- Other data engineers
should be compact and might be harder to query, nested types are ok
- ML models
depends on the model and how its trained
- Customers
should be very easy to interpret (good annotations)

OLTP vs OLAP vs Master data
OLTP (online transaction processing)
optimizes for low-latency, low-volume queries. (3rd normal form)
MySQL, Postegres, performing a lot of joins to get the data we want, looks at one user (already filtered to one entity)
OLAP (online analytical processing)
optimizes for large volume, GROUPBY queries, minimizes, JOINS
looks at the entire dataset or a subset 
Master data
optimizes for completeness of entity definitions, deduped
(sites in the middle of OLTP and OLAP)

OLTP & OLAP is a continuum
Production db snapshots -> master data -> OLAP cubes -> metrics
Master data should be one table with all the joins of the transactions so were not using 40 different joins on a query.
OLAP cubes: slice and dice (flatten the data, can have multiple rows per entity, data scientists/analystis like this) can easily do GROUP BY methods on this
metrics: distill the data from all tables

Cumulative Table design
Core Components consists of: 
- 2 dataframes (yesterday and today)
- FULL OUTER JOIN the two data frames together
- COALESCE values to keep everything around
- Hang onto all of history
Usage:
- Growth analytics @ Facebook (dim_all_users)
- state transition tracking
(if a user is active yesterday but not today they are considered "churned", but if not active yesterday but active today "resurrected", theres lots of transitions we can classify here from yesterday to today, another one is "new" modeling patterns)
some filtering can occur because the data is bigger each day (maybe users that haven't logged in 180 days/deleted users)
cumulative output of today will become yesterdays input
strengths:
- historical analysis without shuffle
- easy transition analysis
drawbacks:
- can only be backfilled sequentially
- handing PII data can be a mess since deleted/inactive users get carried forward

Compactness vs usability tradeoffs
Most usable tales:
- have no complex data types
- easily can be manipulated with WHERE and GROUP BY
Most compact tables (not human readable):
- compressed so small they can't be queired directly until decoded
(this can be useful because Network I/O is slow! AirBnB does this for pricing/availability sends down compact data and decodes on the app)
Middle-ground tables:
- use complex data types (array, map, struct), making querying trickier but also compacting more.

When would you use compact vs usability tables?
Most compact:
- online systems where latency and data volumns matter
Middle:
- upstream staging/master data where most consumers are data engineers
Most usable:
- when analytics is the main consumer (they are less technical)

Struct vs Array vs Map
Struct
- keys are rigidly defined (compression is good)
- values can be any type
Map
- keys are loosely defined (compression is ok)
- values all have to be same type
Array
- Ordinal
- List of values that all have to be the same type

Temporal Cardinality, explosions of dimensions
- when you add temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude
example:
airbnb has ~6 million listings, if we want to know the nightly pricing and available of each night for the next year thats 365 * 6 million or about ~2 billion nights
Should this be a dataset of:
- listing level with an array of nights?
- listing night level with 2 billion rows?
if you do the sorting, Parquet will keep these two about the same size
(encoding will remove duplicated rows)

Run length encoding can be very useful for duplicates (be careful spark may mix up ordering after a join), you could re-sort this but sorting should only be done once. We should instead explode out the data that different between rows.


Lecture 2
Slowly Changing Dimensions and Idempotency
--------------------------------------------

Idempotent:
denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself 

pipelines should product the same result
regardless of the day
regardless of how many times
regardless of the hour

If you have data that not idempotent the data will be inconsistent
(which is bad!)

What can make a pipeline not idempotent?
- INERT INTO without TRUNCATE
	use MERGE or INSERT OVERWRITE everytime!
- Using Start_date > without a corresponding end_date <
- Not using a full set of partition sensors
	pipeline might run when there is no/partial data
- Not using depends_on_past for cumulative pipelines
(INERT INTO without TRUNCATE if you run this twice you have twice the amount of data, you'll duplicate the data, youll VIOLATE a idemotent rule)
MERGE will notice that data matches and won't do anything
INSERT OVERWRITE just overwrites the data
If you have a date greater than yesterday and you run the pipline today fine but you run the pipline tomorrow you'll have an extra day of data etc. violates the when clause of idempotentency.
Not using a full set of partition sensors: could happen when you don't have all the data at that moment (i.e. runs at different times in different all data input might not be ready)
depends_on_past (sequencial processing). This happens when you need to use cumulative pipeline and cannot run parallel processing say because yesterdays data is not ready yet (backfill and production behavior of the pipeline should be the same)

More causes that could make a pipleline not idempotent
- Relying on the "latest" partition of a not properly modeled SCD table
	SCD (slowly changing dimensions) if you have a properly modeled SCD table and your backfilling (not in production) that is the only exception
- Relying on the "latest" partition of anything else
if you have a non idempotent pipeline that introduces a bug because it gives the incorrect data (inconsistent data) and you have a cumalitve pipeline that depends on that data then it just carrys those bugs forward everyday

The pains of not having idempotent pipelines
- Backfilling causes inconsistencies between the old and restated data
- Very hard to troubleshoot bugs
- Unit testing cannot replicate the production behavior
- silent failures

Should you model as Slowly Changing dimensions?
- what are the options here?
	latest snapshot
	daily/monthly/yearly
	SCD
SCD: a way to collapse daily snapshots, based on whether the data changed day over day.

Why do dimentions change?
- Someone decides they hate Windows and want MacOS now
- Someone decides to move to another country
- you're fav food over time
etc

How can you model dimensions that change?
- Singular snapshots
	Becareful since these are not idempotent
- Daily partitioned snapshots
- SCD types 1,2,3
Dont backfill data with only latest snapshot of dimensions, which might not be correct for the older values because you might need the old dimensions of the data and not the new version

Types of SCD
Type 0
	- Aren't actually slowly changing (eye color)
	(no temporal component, data doesn't change)
Type 1
	Only care about the latest value
	- NEVER use this type because it makes your pipelines not idempontent anymore
	(when your dealing with daily online transactions this might be okay, but not for a consumer such as for data analytics)
Type 2
	- You care about what the value was from start_date to end_date
	- Current values usually have either an end_date that is:
		NULL
		Far into the future (9999-12-31)
	- Hard to use
		Since theres more than 1 row per dimension, you need to be careful about filtering on time
	- The only type of SCD that is purley idempotent
Type 3
	- You only care about "original" and "current"
		Benfits: you only have 1 row per dimension
		Drawbacks: you lose the history in between original and current
		is this idempotent: partially (which means its not)

Which types are idempotent?
	- Type 0 and Type 2 and idempotent
		type 0 values are unchanging
		type 2 need to be careful how start_date and end_date is used
	- Type 1 is not idempotent
		if the data is backfilled you'll get the dimension as it is now not as it was then.
	- Type 2 isn't idempotent
		if you backfill with this dataset, its impossible to know when to pick original vs current

SCD2 (slowly changing dimensions type 2) Loading
	- Load the entire history in one query
		inefficient but nimble
		1 query and you're done
	- Incrementally load the data after the previous SCD is generated
		has the same "depends_on_past" constraint
		efficient but cumbersome

Its good to make your data small and effecient and you can spend a lot of time doing this but whats the value? Sometimes its important to work on new things!


Lecture 3
Graph db & Additive dimensions
--------------------------------------------

What makes a dimension additive?
Additive dimensions mean that you don't "double count"
Age is additive
	- the population is equal to 20 + 30 + 40 year olds ...
Application interface is NOT additive
	- the number of active users != # of users on web + # of users on Android + # users on iphone
	(I can have two different types of phones!)
Counting drivers by cars is NOT additive
	The number of Honda drivers != # of Civic drivers + # of Odyssey driver + # of Accord driver
	(a driver can drive two different cars the same day)

The essential nature of additivity
	A dimension is additive over a specific window of time, iff the grain of data over that window can only ever be one value at a time

How does additivity help?
	You don't need to use COUNT(DISTINCT) on preaggregated dimensions
	Non-additive dimensions are usually non-additive with respect to COUNT aggregations but not SUM aggregations
can a user be two of these at the same time?

When should you use Enums?
	Great for low-to-medium cardinality (usually ~50 is limit)
	Country is where enums start to struggle

Why enums?
	built in data quality
	built in static fields
	built in documentation

Enumerations and subpartitions
	Enumerations make superb subpartitions:
		you have an exhaustive list
		they chunk up the big data problem into manageable pieces

Little book of enums is a pattern used in pipelines
if theres a lot of datasets (say 50) how can they all be managed? we can group them in a numerated group.
all Data quality checks
customized data quality checks on each partition
if you need a new source you just add another value to the enum
qualtiy goes up and you have documentation
The little book of enums is generated by an enumeration defined in Scala or Python and then a job that turns the list into a table with ~20-30 rows thats how you can share it between DQ-checks and source functions

What type of use cases is this enum pattern useful?
Whenever you have tons of sources mapping to a shared schema
	- unit economics (fees, coupons, credits, insurance, infrastructure cost, taxes, etc)
	- infrastructure graph (applications, dbs, servers, code bases, CI/CD jobs)
	- family of apps (oculus, instagram, facebook, messenger, etc)

How do you model data from disparate sources into a shared schema?
With a flexable schema
	Benifits:
		- you don't have to run ALTER TABLE commands
		- you can manage a lot more columns
		- your schemas don't have a ton of NULL columns
		- "Other_properties" column is great for rarely-used but needed columns
	Downsides:
		- compression is usually worse (especially if you use JSON)
		- readability, queryability

Graph data modeling is different
Shifting the focus on from what the data is to how is the data connected
it is a RELATIONSHIP focused not ENTITY focused
because of this you can do a very poor job at modeling the entities
	- usually the model looks like
		Identifier: STRING
		Type: STRING
		Properties: MAP<STRING, STRING>
(Graph data models usually have the same schema always)
The relationships are modeled a little bit more in depth:
usually the model looks like:
	- subject_identifier: STRING
	- subject_type: VERTEX_TYPE
	- object_identifier: STRING
	- object_type: VERTEX_TYPE
	- edge_type: EDGE_TYPE
	- properties: MAP<STRING, STRING>


Lecture 4
Factual data
--------------------------------------------
What is a fact? (an action)
A fact is something that happened or occurred
	- A user logs into an app
	- A transaction is made
	- You run a 5k with your apple watch
Facts are not slowly changing which makes them easier to model than dimensions

Fact modeling is hard
Fact data is usually 10-100x the volume of dimensional data
	- Reddit has 2B active uisers and sent 50B notifications every day
Fact data can need a lot of context for effective analysis
Duplicates in facts are more common than in dimensional data

How does fact modeling work?
Normalization vs Denormalization
	- Normalized facts don't have any dimensional attributes, just IDs to join to get that information
	- Denormalized facts bring in some dimensional attributes for quicker analyusis at the cost of more storage
Both normalized and denormalized facts are important, theres trade offs for both
Fact data and raw logs are not the same thing
	- Raw logs
		ugly schemas designed for online systems that make data analysis sad
		potentially contains duplicates and other errors
		usually have shorter retention
	- Fact data
		nice column names
		quality guarantees like uniqueness, not null, etc
		Longer retention
A good data engineer can make raw logs into clean effecient fact data
Who, What, Where, When, and How
	- "Who" fields are usually pushed out as IDs (this user clicked this button, we only hold the user_id not the entire user object)
	- "Where" fields
		most likely modeled out like Who with "IDs" to join, but more likely to bring in dimensions, especially if they're high cardinality like "device_id"
	- "How" fields
		similar to "Where", "She used an iphone to make this click"
	- "What" fields are fundamentally part of the nature of the fact
		notifications: "GENERATED","SENT","CLICKED","DELIVERED",
		a transaction, a like, a post, a step (think atomic)
	- "When" fields are fundamentally part of the nature of the fact
		mostly an "event_timestamp", "event_data"
A lot of companies are moving to client side logging rather than server side logging. Theres high fidelity on a client device you get all the interactions that the user takes not just the user web requests.
	- Fact datasets should have quality guarantees
		otherwise analysis would just go to the raw logs
	- Fact data should generally be smaller than raw logs
	- Fact data should parse out hard-to-understand columns
"What" and "When" should always be not NULL and most of the time "Who"

Broadcast joins:
A broadcast join is a type of join used in distributed systems where a smaller dataset is broadcast to all worker nodes, allowing each node to join it with the larger dataset locally. It is most effective when one of the datasets is small enough to fit into memory on each node. This technique minimizes the need for shuffling data across the network and can lead to significant performance improvements in certain scenarios.

When should you model in dimensions?
imagine a company wanting to upgrade to IPv6 and you already have a lot of network request data being logged and broadcast joined on IPv4, you know that upgrading to IPv6 will cause issues with broadcast join or even a shuffle join. Instead in this case it makes sense to add additional dimension field with each network request to rid the join entirely. Each microservice adopts a "sidecar proxy" that enabled logging of app they were (dimension).
This can be a large oranizational effort to solve/implement the solution.

How does logging fit into fact data?
	- Logging brings in all the critical context for your fact data
		usually done in collaboration with online system engineers
	- Don't log everything
		Log only what you really need
	- Logging should conform to values specified by the online teams
		Thrift is what is used at Airbnb and Netflix for this.
should have a shared schema between teams

Options when working with high volume fact data
	- Sampling
		doesn't work for all use cases, works best for metric-driven use-cases where imprecision isn't an issue (law of large numbers)
	- Bucketing
		Fact data can be bucketed by one of the important dimensions (usually user)
		Bucket joins can be much faster than shuffle joins
		sorted-merge bucket (SMB) joins can do joins without suffle at all!

How long should you hold onto fact data?
	- High volumes make fact data more costly to hold onto a long time
	- An approach:
		any fact tables <10TBs retention didn't matter much
			- anonymization of facts usually happened after 60-90 days though and the data would be moved to a new table with PII stripped
		any face tables >100TBs very short retention (~14 days or less)
What is the ROI on holding onto this data?

Deduplication of fact data
	- Facts can often by duplicated
		You can click a notification multiple times
	- How do you pick the right window for deduplication?
		No duplicates in an hour? a day? a week?
		looking at distributions of duplicates is a good idea
	- Intraday deduping options
		streaming
		microbatch
Does it make sense to have this duplicate (prob not)

Streaming to deduplicate facts
	- Streaming allows you to capture most duplicates efficiently
		windowing matters (usually duplicates happen closely in time)
	- Entire day duplicates can be harder for streaming becasue it needs to hold onto a big window of memory
	- Large memory of duplicates usually happen within a short time of first event
	- 15-hourly windows are a sweet spot

Hourly Microbatch Dedupe
Used to reduce landing time of daily tables that dedupe slowly
	- Dedupe each hour with a GROUP BY
	- Use SUM and COUNT to aggregate duplicates use COLLECT_LIST to collect metadata about duplicates that might be different
	STEP 1:
		SELECT
			product_id,
			event_type,
			MIN(event_timestamp_epoch) AS min_event_timestamp_epoch,
			MAX(event_timestamp_epoch) AS max_event_timestamp_epoch,
			MAP_FROM_ARRAYS(
				COLLECT_LIST(event_location)
				COLLECT_LIST(event_timestamp_epoch)
			) AS event locations
		FROM event_source
		GROUP BY product_id, event_type
Ran every hourly
	STEP 2:
	- dedupe bewteen hours with FULL OUTER JOIN like branches of a tree
	- use left.value + right.value to keep duplicates aggregation correctly counting or CONCAT to build a continuous list
		WITH earlier AS (
			SELECT * FROM hourly_deduped_source
			WHERE {ds_str} AND hour = {earlier_hour} AND product_name = {product_name}
		), later AS (
			SELECT * FROM hourly_deduped_source
			WHERE {ds_str} AND hour = {later_hour} AND product_name = {product_name}
		)
		SELECT,
			COALESCE(e.product_id, l.product_id) AS product_id,
			COALESCE(e.event_type, l.event_type) AS event_type,
			COALESCE(e.min_event_timestamp_epoch, l.min_event_timestamp_epoch) AS min_event_timestamp_epoch,
			COALESCE(e.max_event_timestamp_epoch, l.max_event_timestamp_epoch) AS max_event_timestamp_epoch,
			CONCAT(e.event_locations, l.event_locations) AS event_locations
		FROM earlier e
			FULL OUTER JOIN later l
				ON e.product_id = l.product_id
				AND e.event_type = l.event_type
This dedupes hourly, then merges every 2 hours, @ end of the day you have your final merged data (daily micobatch)


Lecture 5
Blurry line between fact and dimension
--------------------------------------------
Is it a fact or a dimension
- Did a user log in today?
	The log in event would be a fact that informs the dim_is_active dimension
	VS the state "dim_is_activated" which is something that is state-driven, not activity driven
- You can aggregate facts and turn them into dimensions
	is this person a "high engager"? a "low engager"?
		think scoring_class from lecture 1
	CASE WHEN to bucketize aggregated facts can be very useful to reduce the cardinality
Be careful not to have high cardinality! Rarely if ever you'll have one entity in a bucket, losses normaility (predictive power) should usually be formed by statistical distributions

Properties of Facts vs dimensions
- Dimensions
	- usually show up in GROUP BY when doing analytics
	- can be "high cardinality" or "low cardinality" depending
	- generally come from a snapshot of state
- Facts
	- usually aggregated when doing analytics by things like SUM, AVG, COUNT
	almost always higher volume than dimentions, although some fact sources are low-volume, think "rare events"
	- gernerally come from events and logs
"CDC" change data capture (extreme blurry line between the two) model a state change of a dimension as a fact, and re-create your dimensions at any one moment in time based on the stack of changes that has happened.

A blurry example
Is the price of a night on Airbnb a fact or a dimension
the host can set price which sounds like an event
it can easily be SUM, AVG, COUNT'd like regular facts
Prices on AirBnB are doubles, therefore ectremely high cardinality
The fact in this case would be the host changing the setting that impacted the price, think a fact has to be logged, a dimension comes from the state of things, price being derived from settings is a dimension.
Since price is a state it becomes a dimension (but feels like a fact)

Boolean / Existence based
Fact/Dimensions
- dim_is_active, dim_bought_something, etc
	These are usually on the daily/hour grain
- dim_has_ever_booked, dim_ever_active, dim_ever_labeled_fake
	These "ever" dimensions look to see if there has "ever" been a log and once it flips one way, it never goes back
	Interesting, simple and powerful features for machine learning
		An Airbnb host with active listings who has never been booked looks sketchier and sketchier over time
- "Days since" dimensions (e.g. days_since_last_active, days_since_signup)
	Very common in retention analytical patterns
	look up J curves

Categorical Fact/Dimensions
- Scoring class in week 1
	A dimension that is derived from fact data
- Often calculated with CASE WHEN logic and "bucketizing"
	Example: Airbnb superhost <- you have to look at a bunch of columns
Once these are set they are hard to change

Should you use dimensions or facts to analyze users?
- Is the "dim_is_activated" state or "dim_is_active" logs a better metric?
	Great data science question because it depends
- It's the difference between "signups" and "growth" in some perspectives

The exteremly efficent date list data structure
- Read Max Sung's write up: https://www.linkedin.com/pulse/datelist-int-efficient-data-structure-user-growth-max-sung/
- extremely efficient way to manage user growth
- imagine a cumulated schema like `users_cumulated`
	user_id
	date
	dates_active - an array of all the recent days that a user was active
- You can turn that into a structure:
	| user_id |   date   |  datelist_int |
	|    32   |2023-01-01|100000010000001|
	where the 1s in the integer represent activity for 2023-01-01 bit position (zero indexed) ones reprsents days so: 2023-01-01 active next one represents 7 days in past 2022-12-24. A cumulative table design otherwise we would have 30 days of data and that can get expensive. This is good compression.


Lecture 6
Reducing shuffle with reduced facts
--------------------------------------------
Why should shuffle be minimized?
- Big data leverages parallelism as much as it can
- Some steps are inherently less "parallelizable" than others

What types of queries aare highly parallelizable
Extremely parallel
	- SELECT, FROM, WHERE
Kinda parallel
	- GROUP BY, JOIN, HAVING
Painfully not parallel
	- ORDER BY (bad at end of query (by itself) not as a window function)
All shuffle is:
Move all the data from a specific key onto a specific machine
If we have a row of data on each machine and we use ORDER BY it all has to go through one machine to see if its global sorted (opposite of parallel) this is bad for data with millions of records
Shuffle partition default is 200 records

How do you make GROUP BY more efficient?
- Give GROUP BY some buckets and guarantees (modulus grouping)
- REDUCE THE DATA VOLUME AS MUCH AS POSSIBLE!
if we use buckets we don't need to use GROUP BY (without shuffle)

Reduced fact data modeling gives you superpowers
- Fact data often has this schema
	user_id, event_time, action, date_partition
	very high volume, 1 row per event
- Daily aggregate often has this schema
	user_id, action_cnt, date_partition
	medium sized volume, 1 row per user per day
- Reduced fact take this one step futher
	user_id, action_cnt array, month_start_partition / year_start_partition
	low volume, 1 row per user per month/year
Minimize the amount of shuffling by always using reduced

Example fact data:
user_id |    event_time  | action |   date   | other_properties           |
   3    |2024-07-08T11:00| like   |2024-07-08|{"os":"Andorid","post": 141}|
   3    |2024-07-09T09:33| comment|2024-07-09|{"os":"iphone","post": 111} |
   3    |2024-07-10T03:33| comment|2024-07-10| {"os":"Andorid","post": 2} |
This data is great for asking very specific questions, very granular. But caution you can only ask for recent or a small window of time of data because the data is high volume causing your queries to be really slow, they might not even run (out of memory).

Example Daily Aggregated data:
user_id |  metric_name  |     date   | value |
   3    |  likes_given  | 2023-07-08 |   34  |
   3    |  likes_given  | 2023-07-09 |   1   |
   3    |  likes_given  | 2023-07-10 |   3   |
2 partitions is date and metric_name
We can do better, we can make this smaller

Example Long-Array Metrics:
user_id |  metric_name  |     date   |         value_array        |
   3    |  likes_given  | 2023-07-01 |[6,4,2,4,2,4,5,5,3,2,2,2,1] |
   3    |  likes_given  | 2023-08-01 |[2,4,2,4,1,1,1,5,3,4,2,3,4] |
   3    |  likes_given  | 2023-09-01 |[18,4,3,3,1,2,4,5,3,4,2,1,5]|
One row per month (note this is not a monthly aggregate)
The date is an index
position of the array is also the day
user 3 gave 6 likes on July 1st, 1 like on July 31st.

How reduced fact data modeling gives you superpowers
- Daily dates are stored as an offset of month_start / year_start
	first index is for date month_start + zero days
	last index is for date month_start + array_length - 1
- Dimensional joins get weird if you want things to stay performant
	SCD accuracy becomes the same as month_start or year_start
		you give up 100% accurate SCD tracking for massively increased performance
	Pick snapshots in time (month start or month end or both) and treat the dimensions as fixed.
- Impact of analysis
	multi-year analyses took hours instead of weeks
	unlocked "decades-long slow burn" analyses at Facebook
- Allowed for fast correlation analysis between user-level metrics and dimensions

RCA (root cause analyses)
