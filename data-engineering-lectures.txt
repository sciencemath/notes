Lecture 1
Complex data types and Cumulation
--------------------------------------------

Dimensions are attributes of an entity (e.g. user's bday, fav food)
some may identify an entity (user's ID)
Dimensions can be slow changing (e.g. time dependent)
Dimensions can be fixed, like your bday, a phone manufacture

Knowing your customer for data (how is your data being used)
- Data analysts / data scientists
should be easy to query, this is probabily flat
- Other data engineers
should be compact and might be harder to query, nested types are ok
- ML models
depends on the model and how its trained
- Customers
should be very easy to interpret (good annotations)

OLTP vs OLAP vs Master data
OLTP (online transaction processing)
optimizes for low-latency, low-volume queries. (3rd normal form)
MySQL, Postegres, performing a lot of joins to get the data we want, looks at one user (already filtered to one entity)
OLAP (online analytical processing)
optimizes for large volume, GROUPBY queries, minimizes, JOINS
looks at the entire dataset or a subset 
Master data
optimizes for completeness of entity definitions, deduped
(sites in the middle of OLTP and OLAP)

OLTP & OLAP is a continuum
Production db snapshots -> master data -> OLAP cubes -> metrics
Master data should be one table with all the joins of the transactions so were not using 40 different joins on a query.
OLAP cubes: slice and dice (flatten the data, can have multiple rows per entity, data scientists/analystis like this) can easily do GROUP BY methods on this
metrics: distill the data from all tables

Cumulative Table design
Core Components consists of: 
- 2 dataframes (yesterday and today)
- FULL OUTER JOIN the two data frames together
- COALESCE values to keep everything around
- Hang onto all of history
Usage:
- Growth analytics @ Facebook (dim_all_users)
- state transition tracking
(if a user is active yesterday but not today they are considered "churned", but if not active yesterday but active today "resurrected", theres lots of transitions we can classify here from yesterday to today, another one is "new" modeling patterns)
some filtering can occur because the data is bigger each day (maybe users that haven't logged in 180 days/deleted users)
cumulative output of today will become yesterdays input
strengths:
- historical analysis without shuffle
- easy transition analysis
drawbacks:
- can only be backfilled sequentially
- handing PII data can be a mess since deleted/inactive users get carried forward

Compactness vs usability tradeoffs
Most usable tales:
- have no complex data types
- easily can be manipulated with WHERE and GROUP BY
Most compact tables (not human readable):
- compressed so small they can't be queired directly until decoded
(this can be useful because Network I/O is slow! AirBnB does this for pricing/availability sends down compact data and decodes on the app)
Middle-ground tables:
- use complex data types (array, map, struct), making querying trickier but also compacting more.

When would you use compact vs usability tables?
Most compact:
- online systems where latency and data volumns matter
Middle:
- upstream staging/master data where most consumers are data engineers
Most usable:
- when analytics is the main consumer (they are less technical)

Struct vs Array vs Map
Struct
- keys are rigidly defined (compression is good)
- values can be any type
Map
- keys are loosely defined (compression is ok)
- values all have to be same type
Array
- Ordinal
- List of values that all have to be the same type

Temporal Cardinality, explosions of dimensions
- when you add temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude
example:
airbnb has ~6 million listings, if we want to know the nightly pricing and available of each night for the next year thats 365 * 6 million or about ~2 billion nights
Should this be a dataset of:
- listing level with an array of nights?
- listing night level with 2 billion rows?
if you do the sorting, Parquet will keep these two about the same size
(encoding will remove duplicated rows)

Run length encoding can be very useful for duplicates (be careful spark may mix up ordering after a join), you could re-sort this but sorting should only be done once. We should instead explode out the data that different between rows.


Lecture 2
Slowly Changing Dimensions and Idempotency
--------------------------------------------

Idempotent:
denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself 

pipelines should product the same result
regardless of the day
regardless of how many times
regardless of the hour

If you have data that not idempotent the data will be inconsistent
(which is bad!)

What can make a pipeline not idempotent?
- INERT INTO without TRUNCATE
	use MERGE or INSERT OVERWRITE everytime!
- Using Start_date > without a corresponding end_date <
- Not using a full set of partition sensors
	pipeline might run when there is no/partial data
- Not using depends_on_past for cumulative pipelines
(INERT INTO without TRUNCATE if you run this twice you have twice the amount of data, you'll duplicate the data, youll VIOLATE a idemotent rule)
MERGE will notice that data matches and won't do anything
INSERT OVERWRITE just overwrites the data
If you have a date greater than yesterday and you run the pipline today fine but you run the pipline tomorrow you'll have an extra day of data etc. violates the when clause of idempotentency.
Not using a full set of partition sensors: could happen when you don't have all the data at that moment (i.e. runs at different times in different all data input might not be ready)
depends_on_past (sequencial processing). This happens when you need to use cumulative pipeline and cannot run parallel processing say because yesterdays data is not ready yet (backfill and production behavior of the pipeline should be the same)

More causes that could make a pipleline not idempotent
- Relying on the "latest" partition of a not properly modeled SCD table
	SCD (slowly changing dimensions) if you have a properly modeled SCD table and your backfilling (not in production) that is the only exception
- Relying on the "latest" partition of anything else
if you have a non idempotent pipeline that introduces a bug because it gives the incorrect data (inconsistent data) and you have a cumalitve pipeline that depends on that data then it just carrys those bugs forward everyday

The pains of not having idempotent pipelines
- Backfilling causes inconsistencies between the old and restated data
- Very hard to troubleshoot bugs
- Unit testing cannot replicate the production behavior
- silent failures

Should you model as Slowly Changing dimensions?
- what are the options here?
	latest snapshot
	daily/monthly/yearly
	SCD
SCD: a way to collapse daily snapshots, based on whether the data changed day over day.

Why do dimentions change?
- Someone decides they hate Windows and want MacOS now
- Someone decides to move to another country
- you're fav food over time
etc

How can you model dimensions that change?
- Singular snapshots
	Becareful since these are not idempotent
- Daily partitioned snapshots
- SCD types 1,2,3
Dont backfill data with only latest snapshot of dimensions, which might not be correct for the older values because you might need the old dimensions of the data and not the new version

Types of SCD
Type 0
	- Aren't actually slowly changing (eye color)
	(no temporal component, data doesn't change)
Type 1
	Only care about the latest value
	- NEVER use this type because it makes your pipelines not idempontent anymore
	(when your dealing with daily online transactions this might be okay, but not for a consumer such as for data analytics)
Type 2
	- You care about what the value was from start_date to end_date
	- Current values usually have either an end_date that is:
		NULL
		Far into the future (9999-12-31)
	- Hard to use
		Since theres more than 1 row per dimension, you need to be careful about filtering on time
	- The only type of SCD that is purley idempotent
Type 3
	- You only care about "original" and "current"
		Benfits: you only have 1 row per dimension
		Drawbacks: you lose the history in between original and current
		is this idempotent: partially (which means its not)

Which types are idempotent?
	- Type 0 and Type 2 and idempotent
		type 0 values are unchanging
		type 2 need to be careful how start_date and end_date is used
	- Type 1 is not idempotent
		if the data is backfilled you'll get the dimension as it is now not as it was then.
	- Type 2 isn't idempotent
		if you backfill with this dataset, its impossible to know when to pick original vs current

SCD2 (slowly changing dimensions type 2) Loading
	- Load the entire history in one query
		inefficient but nimble
		1 query and you're done
	- Incrementally load the data after the previous SCD is generated
		has the same "depends_on_past" constraint
		efficient but cumbersome

Its good to make your data small and effecient and you can spend a lot of time doing this but whats the value? Sometimes its important to work on new things!


Lecture 3
Graph db & Additive dimensions
--------------------------------------------

What makes a dimension additive?
Additive dimensions mean that you don't "double count"
Age is additive
	- the population is equal to 20 + 30 + 40 year olds ...
Application interface is NOT additive
	- the number of active users != # of users on web + # of users on Android + # users on iphone
	(I can have two different types of phones!)
Counting drivers by cars is NOT additive
	The number of Honda drivers != # of Civic drivers + # of Odyssey driver + # of Accord driver
	(a driver can drive two different cars the same day)

The essential nature of additivity
	A dimension is additive over a specific window of time, iff the grain of data over that window can only ever be one value at a time

How does additivity help?
	You don't need to use COUNT(DISTINCT) on preaggregated dimensions
	Non-additive dimensions are usually non-additive with respect to COUNT aggregations but not SUM aggregations
can a user be two of these at the same time?

When should you use Enums?
	Great for low-to-medium cardinality (usually ~50 is limit)
	Country is where enums start to struggle

Why enums?
	built in data quality
	built in static fields
	built in documentation

Enumerations and subpartitions
	Enumerations make superb subpartitions:
		you have an exhaustive list
		they chunk up the big data problem into manageable pieces

Little book of enums is a pattern used in pipelines
if theres a lot of datasets (say 50) how can they all be managed? we can group them in a numerated group.
all Data quality checks
customized data quality checks on each partition
if you need a new source you just add another value to the enum
qualtiy goes up and you have documentation
The little book of enums is generated by an enumeration defined in Scala or Python and then a job that turns the list into a table with ~20-30 rows thats how you can share it between DQ-checks and source functions

What type of use cases is this enum pattern useful?
Whenever you have tons of sources mapping to a shared schema
	- unit economics (fees, coupons, credits, insurance, infrastructure cost, taxes, etc)
	- infrastructure graph (applications, dbs, servers, code bases, CI/CD jobs)
	- family of apps (oculus, instagram, facebook, messenger, etc)

How do you model data from disparate sources into a shared schema?
With a flexable schema
	Benifits:
		- you don't have to run ALTER TABLE commands
		- you can manage a lot more columns
		- your schemas don't have a ton of NULL columns
		- "Other_properties" column is great for rarely-used but needed columns
	Downsides:
		- compression is usually worse (especially if you use JSON)
		- readability, queryability

Graph data modeling is different
Shifting the focus on from what the data is to how is the data connected
it is a RELATIONSHIP focused not ENTITY focused
because of this you can do a very poor job at modeling the entities
	- usually the model looks like
		Identifier: STRING
		Type: STRING
		Properties: MAP<STRING, STRING>
(Graph data models usually have the same schema always)
The relationships are modeled a little bit more in depth:
usually the model looks like:
	- subject_identifier: STRING
	- subject_type: VERTEX_TYPE
	- object_identifier: STRING
	- object_type: VERTEX_TYPE
	- edge_type: EDGE_TYPE
	- properties: MAP<STRING, STRING>


Lecture 4
Facts
--------------------------------------------
What is a fact? (an action)
A fact is something that happened or occurred
	- A user logs into an app
	- A transaction is made
	- You run a 5k with your apple watch
Facts are not slowly changing which makes them easier to model than dimensions

Fact modeling is hard
Fact data is usually 10-100x the volume of dimensional data
	- Reddit has 2B active uisers and sent 50B notifications every day
Fact data can need a lot of context for effective analysis
Duplicates in facts are more common than in dimensional data