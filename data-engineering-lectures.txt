Lecture 1
Complex data types and Cumulation
--------------------------------------------

Dimensions are attributes of an entity (e.g. user's bday, fav food)
some may identify an entity (user's ID)
Dimensions can be slow changing (e.g. time dependent)
Dimensions can be fixed, like your bday, a phone manufacture

Knowing your customer for data (how is your data being used)
- Data analysts / data scientists
should be easy to query, this is probabily flat
- Other data engineers
should be compact and might be harder to query, nested types are ok
- ML models
depends on the model and how its trained
- Customers
should be very easy to interpret (good annotations)

OLTP vs OLAP vs Master data
OLTP (online transaction processing)
optimizes for low-latency, low-volume queries. (3rd normal form)
MySQL, Postegres, performing a lot of joins to get the data we want, looks at one user (already filtered to one entity)
OLAP (online analytical processing)
optimizes for large volume, GROUPBY queries, minimizes, JOINS
looks at the entire dataset or a subset 
Master data
optimizes for completeness of entity definitions, deduped
(sites in the middle of OLTP and OLAP)

OLTP & OLAP is a continuum
Production db snapshots -> master data -> OLAP cubes -> metrics
Master data should be one table with all the joins of the transactions so were not using 40 different joins on a query.
OLAP cubes: slice and dice (flatten the data, can have multiple rows per entity, data scientists/analystis like this) can easily do GROUP BY methods on this
metrics: distill the data from all tables

Cumulative Table design
Core Components consists of: 
- 2 dataframes (yesterday and today)
- FULL OUTER JOIN the two data frames together
- COALESCE values to keep everything around
- Hang onto all of history
Usage:
- Growth analytics @ Facebook (dim_all_users)
- state transition tracking
(if a user is active yesterday but not today they are considered "churned", but if not active yesterday but active today "resurrected", theres lots of transitions we can classify here from yesterday to today, another one is "new" modeling patterns)
some filtering can occur because the data is bigger each day (maybe users that haven't logged in 180 days/deleted users)
cumulative output of today will become yesterdays input
strengths:
- historical analysis without shuffle
- easy transition analysis
drawbacks:
- can only be backfilled sequentially
- handing PII data can be a mess since deleted/inactive users get carried forward

Compactness vs usability tradeoffs
Most usable tales:
- have no complex data types
- easily can be manipulated with WHERE and GROUP BY
Most compact tables (not human readable):
- compressed so small they can't be queired directly until decoded
(this can be useful because Network I/O is slow! AirBnB does this for pricing/availability sends down compact data and decodes on the app)
Middle-ground tables:
- use complex data types (array, map, struct), making querying trickier but also compacting more.

When would you use compact vs usability tables?
Most compact:
- online systems where latency and data volumns matter
Middle:
- upstream staging/master data where most consumers are data engineers
Most usable:
- when analytics is the main consumer (they are less technical)

Struct vs Array vs Map
Struct
- keys are rigidly defined (compression is good)
- values can be any type
Map
- keys are loosely defined (compression is ok)
- values all have to be same type
Array
- Ordinal
- List of values that all have to be the same type

Temporal Cardinality, explosions of dimensions
- when you add temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude
example:
airbnb has ~6 million listings, if we want to know the nightly pricing and available of each night for the next year thats 365 * 6 million or about ~2 billion nights
Should this be a dataset of:
- listing level with an array of nights?
- listing night level with 2 billion rows?
if you do the sorting, Parquet will keep these two about the same size
(encoding will remove duplicated rows)

Run length encoding can be very useful for duplicates (be careful spark may mix up ordering after a join), you could re-sort this but sorting should only be done once. We should instead explode out the data that different between rows.


Lecture 2
Slowly Changing Dimensions and Idempotency
--------------------------------------------

Idempotent:
denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself 

pipelines should product the same result
regardless of the day
regardless of how many times
regardless of the hour

If you have data that not idempotent the data will be inconsistent
(which is bad!)

What can make a pipeline not idempotent?
- INERT INTO without TRUNCATE
	use MERGE or INSERT OVERWRITE everytime!
- Using Start_date > without a corresponding end_date <
- Not using a full set of partition sensors
	pipeline might run when there is no/partial data
- Not using depends_on_past for cumulative pipelines
(INERT INTO without TRUNCATE if you run this twice you have twice the amount of data, you'll duplicate the data, youll VIOLATE a idemotent rule)
MERGE will notice that data matches and won't do anything
INSERT OVERWRITE just overwrites the data
If you have a date greater than yesterday and you run the pipline today fine but you run the pipline tomorrow you'll have an extra day of data etc. violates the when clause of idempotentency.
Not using a full set of partition sensors: could happen when you don't have all the data at that moment (i.e. runs at different times in different all data input might not be ready)
depends_on_past (sequencial processing). This happens when you need to use cumulative pipeline and cannot run parallel processing say because yesterdays data is not ready yet (backfill and production behavior of the pipeline should be the same)

More causes that could make a pipleline not idempotent
- Relying on the "latest" partition of a not properly modeled SCD table
	SCD (slowly changing dimensions) if you have a properly modeled SCD table and your backfilling (not in production) that is the only exception
- Relying on the "latest" partition of anything else
if you have a non idempotent pipeline that introduces a bug because it gives the incorrect data (inconsistent data) and you have a cumalitve pipeline that depends on that data then it just carrys those bugs forward everyday

The pains of not having idempotent pipelines
- Backfilling causes inconsistencies between the old and restated data
- Very hard to troubleshoot bugs
- Unit testing cannot replicate the production behavior
- silent failures

Should you model as Slowly Changing dimensions?
- what are the options here?
	latest snapshot
	daily/monthly/yearly
	SCD
SCD: a way to collapse daily snapshots, based on whether the data changed day over day.

Why do dimentions change?
- Someone decides they hate Windows and want MacOS now
- Someone decides to move to another country
- you're fav food over time
etc

How can you model dimensions that change?
- Singular snapshots
	Becareful since these are not idempotent
- Daily partitioned snapshots
- SCD types 1,2,3
Dont backfill data with only latest snapshot of dimensions, which might not be correct for the older values because you might need the old dimensions of the data and not the new version

Types of SCD
Type 0
	- Aren't actually slowly changing (eye color)
	(no temporal component, data doesn't change)
Type 1
	Only care about the latest value
	- NEVER use this type because it makes your pipelines not idempontent anymore
	(when your dealing with daily online transactions this might be okay, but not for a consumer such as for data analytics)
Type 2
	- You care about what the value was from start_date to end_date
	- Current values usually have either an end_date that is:
		NULL
		Far into the future (9999-12-31)
	- Hard to use
		Since theres more than 1 row per dimension, you need to be careful about filtering on time
	- The only type of SCD that is purley idempotent
Type 3
	- You only care about "original" and "current"
		Benfits: you only have 1 row per dimension
		Drawbacks: you lose the history in between original and current
		is this idempotent: partially (which means its not)

Which types are idempotent?
	- Type 0 and Type 2 and idempotent
		type 0 values are unchanging
		type 2 need to be careful how start_date and end_date is used
	- Type 1 is not idempotent
		if the data is backfilled you'll get the dimension as it is now not as it was then.
	- Type 2 isn't idempotent
		if you backfill with this dataset, its impossible to know when to pick original vs current

SCD2 (slowly changing dimensions type 2) Loading
	- Load the entire history in one query
		inefficient but nimble
		1 query and you're done
	- Incrementally load the data after the previous SCD is generated
		has the same "depends_on_past" constraint
		efficient but cumbersome

Its good to make your data small and effecient and you can spend a lot of time doing this but whats the value? Sometimes its important to work on new things!


Lecture 3
Graph db & Additive dimensions
--------------------------------------------

What makes a dimension additive?
Additive dimensions mean that you don't "double count"
Age is additive
	- the population is equal to 20 + 30 + 40 year olds ...
Application interface is NOT additive
	- the number of active users != # of users on web + # of users on Android + # users on iphone
	(I can have two different types of phones!)
Counting drivers by cars is NOT additive
	The number of Honda drivers != # of Civic drivers + # of Odyssey driver + # of Accord driver
	(a driver can drive two different cars the same day)

The essential nature of additivity
	A dimension is additive over a specific window of time, iff the grain of data over that window can only ever be one value at a time

How does additivity help?
	You don't need to use COUNT(DISTINCT) on preaggregated dimensions
	Non-additive dimensions are usually non-additive with respect to COUNT aggregations but not SUM aggregations
can a user be two of these at the same time?

When should you use Enums?
	Great for low-to-medium cardinality (usually ~50 is limit)
	Country is where enums start to struggle

Why enums?
	built in data quality
	built in static fields
	built in documentation

Enumerations and subpartitions
	Enumerations make superb subpartitions:
		you have an exhaustive list
		they chunk up the big data problem into manageable pieces

Little book of enums is a pattern used in pipelines
if theres a lot of datasets (say 50) how can they all be managed? we can group them in a numerated group.
all Data quality checks
customized data quality checks on each partition
if you need a new source you just add another value to the enum
qualtiy goes up and you have documentation
The little book of enums is generated by an enumeration defined in Scala or Python and then a job that turns the list into a table with ~20-30 rows thats how you can share it between DQ-checks and source functions

What type of use cases is this enum pattern useful?
Whenever you have tons of sources mapping to a shared schema
	- unit economics (fees, coupons, credits, insurance, infrastructure cost, taxes, etc)
	- infrastructure graph (applications, dbs, servers, code bases, CI/CD jobs)
	- family of apps (oculus, instagram, facebook, messenger, etc)

How do you model data from disparate sources into a shared schema?
With a flexable schema
	Benifits:
		- you don't have to run ALTER TABLE commands
		- you can manage a lot more columns
		- your schemas don't have a ton of NULL columns
		- "Other_properties" column is great for rarely-used but needed columns
	Downsides:
		- compression is usually worse (especially if you use JSON)
		- readability, queryability

Graph data modeling is different
Shifting the focus on from what the data is to how is the data connected
it is a RELATIONSHIP focused not ENTITY focused
because of this you can do a very poor job at modeling the entities
	- usually the model looks like
		Identifier: STRING
		Type: STRING
		Properties: MAP<STRING, STRING>
(Graph data models usually have the same schema always)
The relationships are modeled a little bit more in depth:
usually the model looks like:
	- subject_identifier: STRING
	- subject_type: VERTEX_TYPE
	- object_identifier: STRING
	- object_type: VERTEX_TYPE
	- edge_type: EDGE_TYPE
	- properties: MAP<STRING, STRING>


Lecture 4
Factual data
--------------------------------------------
What is a fact? (an action)
A fact is something that happened or occurred
	- A user logs into an app
	- A transaction is made
	- You run a 5k with your apple watch
Facts are not slowly changing which makes them easier to model than dimensions

Fact modeling is hard
Fact data is usually 10-100x the volume of dimensional data
	- Reddit has 2B active uisers and sent 50B notifications every day
Fact data can need a lot of context for effective analysis
Duplicates in facts are more common than in dimensional data

How does fact modeling work?
Normalization vs Denormalization
	- Normalized facts don't have any dimensional attributes, just IDs to join to get that information
	- Denormalized facts bring in some dimensional attributes for quicker analyusis at the cost of more storage
Both normalized and denormalized facts are important, theres trade offs for both
Fact data and raw logs are not the same thing
	- Raw logs
		ugly schemas designed for online systems that make data analysis sad
		potentially contains duplicates and other errors
		usually have shorter retention
	- Fact data
		nice column names
		quality guarantees like uniqueness, not null, etc
		Longer retention
A good data engineer can make raw logs into clean effecient fact data
Who, What, Where, When, and How
	- "Who" fields are usually pushed out as IDs (this user clicked this button, we only hold the user_id not the entire user object)
	- "Where" fields
		most likely modeled out like Who with "IDs" to join, but more likely to bring in dimensions, especially if they're high cardinality like "device_id"
	- "How" fields
		similar to "Where", "She used an iphone to make this click"
	- "What" fields are fundamentally part of the nature of the fact
		notifications: "GENERATED","SENT","CLICKED","DELIVERED",
		a transaction, a like, a post, a step (think atomic)
	- "When" fields are fundamentally part of the nature of the fact
		mostly an "event_timestamp", "event_data"
A lot of companies are moving to client side logging rather than server side logging. Theres high fidelity on a client device you get all the interactions that the user takes not just the user web requests.
	- Fact datasets should have quality guarantees
		otherwise analysis would just go to the raw logs
	- Fact data should generally be smaller than raw logs
	- Fact data should parse out hard-to-understand columns
"What" and "When" should always be not NULL and most of the time "Who"

Broadcast joins:
A broadcast join is a type of join used in distributed systems where a smaller dataset is broadcast to all worker nodes, allowing each node to join it with the larger dataset locally. It is most effective when one of the datasets is small enough to fit into memory on each node. This technique minimizes the need for shuffling data across the network and can lead to significant performance improvements in certain scenarios.

When should you model in dimensions?
imagine a company wanting to upgrade to IPv6 and you already have a lot of network request data being logged and broadcast joined on IPv4, you know that upgrading to IPv6 will cause issues with broadcast join or even a shuffle join. Instead in this case it makes sense to add additional dimension field with each network request to rid the join entirely. Each microservice adopts a "sidecar proxy" that enabled logging of app they were (dimension).
This can be a large oranizational effort to solve/implement the solution.

How does logging fit into fact data?
	- Logging brings in all the critical context for your fact data
		usually done in collaboration with online system engineers
	- Don't log everything
		Log only what you really need
	- Logging should conform to values specified by the online teams
		Thrift is what is used at Airbnb and Netflix for this.
should have a shared schema between teams

Options when working with high volume fact data
	- Sampling
		doesn't work for all use cases, works best for metric-driven use-cases where imprecision isn't an issue (law of large numbers)
	- Bucketing
		Fact data can be bucketed by one of the important dimensions (usually user)
		Bucket joins can be much faster than shuffle joins
		sorted-merge bucket (SMB) joins can do joins without suffle at all!

How long should you hold onto fact data?
	- High volumes make fact data more costly to hold onto a long time
	- An approach:
		any fact tables <10TBs retention didn't matter much
			- anonymization of facts usually happened after 60-90 days though and the data would be moved to a new table with PII stripped
		any face tables >100TBs very short retention (~14 days or less)
What is the ROI on holding onto this data?

Deduplication of fact data
	- Facts can often by duplicated
		You can click a notification multiple times
	- How do you pick the right window for deduplication?
		No duplicates in an hour? a day? a week?
		looking at distributions of duplicates is a good idea
	- Intraday deduping options
		streaming
		microbatch
Does it make sense to have this duplicate (prob not)

Streaming to deduplicate facts
	- Streaming allows you to capture most duplicates efficiently
		windowing matters (usually duplicates happen closely in time)
	- Entire day duplicates can be harder for streaming becasue it needs to hold onto a big window of memory
	- Large memory of duplicates usually happen within a short time of first event
	- 15-hourly windows are a sweet spot

Hourly Microbatch Dedupe
Used to reduce landing time of daily tables that dedupe slowly
	- Dedupe each hour with a GROUP BY
	- Use SUM and COUNT to aggregate duplicates use COLLECT_LIST to collect metadata about duplicates that might be different
	STEP 1:
		SELECT
			product_id,
			event_type,
			MIN(event_timestamp_epoch) AS min_event_timestamp_epoch,
			MAX(event_timestamp_epoch) AS max_event_timestamp_epoch,
			MAP_FROM_ARRAYS(
				COLLECT_LIST(event_location)
				COLLECT_LIST(event_timestamp_epoch)
			) AS event locations
		FROM event_source
		GROUP BY product_id, event_type
Ran every hourly
	STEP 2:
	- dedupe bewteen hours with FULL OUTER JOIN like branches of a tree
	- use left.value + right.value to keep duplicates aggregation correctly counting or CONCAT to build a continuous list
		WITH earlier AS (
			SELECT * FROM hourly_deduped_source
			WHERE {ds_str} AND hour = {earlier_hour} AND product_name = {product_name}
		), later AS (
			SELECT * FROM hourly_deduped_source
			WHERE {ds_str} AND hour = {later_hour} AND product_name = {product_name}
		)
		SELECT,
			COALESCE(e.product_id, l.product_id) AS product_id,
			COALESCE(e.event_type, l.event_type) AS event_type,
			COALESCE(e.min_event_timestamp_epoch, l.min_event_timestamp_epoch) AS min_event_timestamp_epoch,
			COALESCE(e.max_event_timestamp_epoch, l.max_event_timestamp_epoch) AS max_event_timestamp_epoch,
			CONCAT(e.event_locations, l.event_locations) AS event_locations
		FROM earlier e
			FULL OUTER JOIN later l
				ON e.product_id = l.product_id
				AND e.event_type = l.event_type
This dedupes hourly, then merges every 2 hours, @ end of the day you have your final merged data (daily micobatch)


Lecture 5
Blurry line between fact and dimension
--------------------------------------------
Is it a fact or a dimension
- Did a user log in today?
	The log in event would be a fact that informs the dim_is_active dimension
	VS the state "dim_is_activated" which is something that is state-driven, not activity driven
- You can aggregate facts and turn them into dimensions
	is this person a "high engager"? a "low engager"?
		think scoring_class from lecture 1
	CASE WHEN to bucketize aggregated facts can be very useful to reduce the cardinality
Be careful not to have high cardinality! Rarely if ever you'll have one entity in a bucket, losses normaility (predictive power) should usually be formed by statistical distributions

Properties of Facts vs dimensions
- Dimensions
	- usually show up in GROUP BY when doing analytics
	- can be "high cardinality" or "low cardinality" depending
	- generally come from a snapshot of state
- Facts
	- usually aggregated when doing analytics by things like SUM, AVG, COUNT
	almost always higher volume than dimentions, although some fact sources are low-volume, think "rare events"
	- gernerally come from events and logs
"CDC" change data capture (extreme blurry line between the two) model a state change of a dimension as a fact, and re-create your dimensions at any one moment in time based on the stack of changes that has happened.

A blurry example
Is the price of a night on Airbnb a fact or a dimension
the host can set price which sounds like an event
it can easily be SUM, AVG, COUNT'd like regular facts
Prices on AirBnB are doubles, therefore ectremely high cardinality
The fact in this case would be the host changing the setting that impacted the price, think a fact has to be logged, a dimension comes from the state of things, price being derived from settings is a dimension.
Since price is a state it becomes a dimension (but feels like a fact)

Boolean / Existence based
Fact/Dimensions
- dim_is_active, dim_bought_something, etc
	These are usually on the daily/hour grain
- dim_has_ever_booked, dim_ever_active, dim_ever_labeled_fake
	These "ever" dimensions look to see if there has "ever" been a log and once it flips one way, it never goes back
	Interesting, simple and powerful features for machine learning
		An Airbnb host with active listings who has never been booked looks sketchier and sketchier over time
- "Days since" dimensions (e.g. days_since_last_active, days_since_signup)
	Very common in retention analytical patterns
	look up J curves

Categorical Fact/Dimensions
- Scoring class in week 1
	A dimension that is derived from fact data
- Often calculated with CASE WHEN logic and "bucketizing"
	Example: Airbnb superhost <- you have to look at a bunch of columns
Once these are set they are hard to change

Should you use dimensions or facts to analyze users?
- Is the "dim_is_activated" state or "dim_is_active" logs a better metric?
	Great data science question because it depends
- It's the difference between "signups" and "growth" in some perspectives

The exteremly efficent date list data structure
- Read Max Sung's write up: https://www.linkedin.com/pulse/datelist-int-efficient-data-structure-user-growth-max-sung/
- extremely efficient way to manage user growth
- imagine a cumulated schema like `users_cumulated`
	user_id
	date
	dates_active - an array of all the recent days that a user was active
- You can turn that into a structure:
	| user_id |   date   |  datelist_int |
	|    32   |2023-01-01|100000010000001|
	where the 1s in the integer represent activity for 2023-01-01 bit position (zero indexed) ones reprsents days so: 2023-01-01 active next one represents 7 days in past 2022-12-24. A cumulative table design otherwise we would have 30 days of data and that can get expensive. This is good compression.


Lecture 6
Reducing shuffle with reduced facts
--------------------------------------------
Why should shuffle be minimized?
- Big data leverages parallelism as much as it can
- Some steps are inherently less "parallelizable" than others

What types of queries are highly parallelizable
Extremely parallel
	- SELECT, FROM, WHERE
Kinda parallel
	- GROUP BY, JOIN, HAVING
Painfully not parallel
	- ORDER BY (bad at end of query (by itself) not as a window function)
All shuffle is:
Move all the data from a specific key onto a specific machine
If we have a row of data on each machine and we use ORDER BY it all has to go through one machine to see if its global sorted (opposite of parallel) this is bad for data with millions of records
Shuffle partition default is 200 records

How do you make GROUP BY more efficient?
- Give GROUP BY some buckets and guarantees (modulus grouping)
- REDUCE THE DATA VOLUME AS MUCH AS POSSIBLE!
if we use buckets we don't need to use GROUP BY (without shuffle)

Reduced fact data modeling gives you superpowers
- Fact data often has this schema
	user_id, event_time, action, date_partition
	very high volume, 1 row per event
- Daily aggregate often has this schema
	user_id, action_cnt, date_partition
	medium sized volume, 1 row per user per day
- Reduced fact take this one step futher
	user_id, action_cnt array, month_start_partition / year_start_partition
	low volume, 1 row per user per month/year
Minimize the amount of shuffling by always using reduced

Example fact data:
user_id |    event_time  | action |   date   | other_properties           |
   3    |2024-07-08T11:00| like   |2024-07-08|{"os":"Andorid","post": 141}|
   3    |2024-07-09T09:33| comment|2024-07-09|{"os":"iphone","post": 111} |
   3    |2024-07-10T03:33| comment|2024-07-10| {"os":"Andorid","post": 2} |
This data is great for asking very specific questions, very granular. But caution you can only ask for recent or a small window of time of data because the data is high volume causing your queries to be really slow, they might not even run (out of memory).

Example Daily Aggregated data:
user_id |  metric_name  |     date   | value |
   3    |  likes_given  | 2023-07-08 |   34  |
   3    |  likes_given  | 2023-07-09 |   1   |
   3    |  likes_given  | 2023-07-10 |   3   |
2 partitions is date and metric_name
We can do better, we can make this smaller

Example Long-Array Metrics:
user_id |  metric_name  |     date   |         value_array        |
   3    |  likes_given  | 2023-07-01 |[6,4,2,4,2,4,5,5,3,2,2,2,1] |
   3    |  likes_given  | 2023-08-01 |[2,4,2,4,1,1,1,5,3,4,2,3,4] |
   3    |  likes_given  | 2023-09-01 |[18,4,3,3,1,2,4,5,3,4,2,1,5]|
One row per month (note this is not a monthly aggregate)
The date is an index
position of the array is also the day
user 3 gave 6 likes on July 1st, 1 like on July 31st.

How reduced fact data modeling gives you superpowers
- Daily dates are stored as an offset of month_start / year_start
	first index is for date month_start + zero days
	last index is for date month_start + array_length - 1
- Dimensional joins get weird if you want things to stay performant
	SCD accuracy becomes the same as month_start or year_start
		you give up 100% accurate SCD tracking for massively increased performance
	Pick snapshots in time (month start or month end or both) and treat the dimensions as fixed.
- Impact of analysis
	multi-year analyses took hours instead of weeks
	unlocked "decades-long slow burn" analyses at Facebook
- Allowed for fast correlation analysis between user-level metrics and dimensions

RCA (root cause analyses)


Lecture 7
Spark + Iceberg
--------------------------------------------
What is Apache Spark?
Spark is a distibuted compute framework that allows you to process very large amounts of data efficiently

Why is Spark so good?
- Spark leverages RAM much more effectively than previous iterations of
distributed compute (it's WAY faster than Hive/Java MR/etc)
- Spark is storage agnostic, allowing a decoupling of storage and compute
	- Spark makes it easier to avoid vendor lock-in
- Spark has a huge community of developers so Stackoverflow / ChatGPT
will help you troubleshot

When is Spark not so good?
- Nobody else in the company knows Spark
	- Spark is not immune to the bus factor!
- Your company already uses something else a lot
	- Inertia is often times not worth it to overcome
(use what the company is betting on)

How does Spark work?
Spark has a few peices to it
- The plan
- The driver
- The executors

The Plan
- This is the transformation you describe in Python, Scala, or SQL
- The plan is evaluated lazily
	- Lazy evaluation: "execution only happens when it needs to"
- When does execution "need to" happen
	- Writing output
	- When part of the plan depends on the data itself
		(e.g. calling dataframe.collect() to determine the next set of transformations)

Driver
- The Driver reads the plan
- Important Spark driver settings
spark.driver.memory:
For complex jobs or jobs that use dataframe.collect(), you may need
to bump this higher or else you'll experience an OOM (default 2GB)
spark.driver.memoryOverheadFactor:
What fraction the driver needs for non-heap related memory,
usually 10%, might need to be higher for complex jobs
(every other driver setting you shouldn't need to touch)
- Driver needs to determine a few things
	- When to actually start executing the job and stop being lazy
	- How to JOIN datasets (very important on performance)
	- How much parallelism each step needs

Executors (who do the actual work)
- The driver passes the plan to the executors
spark.executor.memory:
This determines how much memory each executor gets. A low number
here may cause Spark to "spill to disk" which will cause your job to much slower
spark.executors.cores:
How many tasks can happen on each machine
(default is 4, shouldn't go higher than 6)
spark.executor.memoryOverheadFactor:
What % of memory should an executor use for non-heap related tasks,
usually 10%. For jobs with lots of UDFs and complexity, you may need to bump this up!

The types of JOINs in Spark
- Shuffle sort-merge Join (least performant, but most versitile)
	- Default JOIN strategy since Spark 2.3
	- Works when both sides of the join are large
- Broadcast Hash Join (smallish datasets 8-10GB)
	- Works well if one side of the join is small
	- spark.sql.autoBroadcastJoinThreshold (default 10MBs, can go as high as 8GBs
	you'll experience weird memory problems > 1GBs)
	- A join WITHOUT shuffle!
- Bucket Joins
	- A join without shuffle!
(when your bucketing your tables always bucket your tables of powers of 2)

Shuffle
Shuffle partitions and parallelism are linked!
- Shuffle partitions and parallelism
	- spark.sql.shuffle.partitions and spark.default.parallelism
	- Just use spark.sql.shuffle.partitions Since the other is related to the
	RDD API you shouldn't be using
	(shouldn't be using RDD API, just use higher level APIs)
Is Shuffle good or bad?
- At low-to-medium volume
	- it's really good and makes our lives easier!
- At high volumes > 10 TBs
	- Painful!
	- At Netflix shuffle killed the IP enrichment pipeline

How to minimize Shuffle at high volumes?
- Bucket the data if multiple JOINs or aggregations are happening downstream
- Spark has the ability to bucket data to minimize or eliminate the need for shuffle
when doing joins
- Bucket joins are very efficient but have drawbacks
- Main drawback is the initial parallelism = number of buckets
- Bucket joins only work if the two tables number of buckets are multiples of each other!
	- Always use powers of 2 for # of buckets!

Shuffle and Skew
Sometimes some partitions have dramatically more data than others
This can happen because:
	- Not enough partitions
	- The natural way the data is
		- Beyonce gets a lot more notifications than the average Facebook user

How to tell if your data is skewed?
- Most common is a job getting to 99%, taking forever, and failing
- Another, more scientific way is to do a box-and-whiskers plot of the data to see
if there's any extreme outliers

Ways to deal with Skew
- Adaptive query execution - only in Spark 3+
	- Set spark.sql.adaptive.enabled = True
	(Spark has to calc extra statistics using this,
	makes the job slower but more resilient to skew)
- Salting the GROUP BY - best option before Spark 3
	- GROUP BY a random number, aggregate + GROUP BY again
	- Be careful with things like AVG - break it into SUM and COUNT and divide
	df.withColumn("salt_random_column", (rand * n).cast(IntegerType))
	  .groupBy(groupByFields, "salt_random_column")
	  .agg(aggFields)
	  .groupBy(groupByFields)
	  .agg(aggFields)
You can filter out your outliers, or partition your downstream table where you have
one side of the pipeline that process everyone else and the other side of the pipeline
that process the outliers 

Spark on Databricks vs regular Spark
						 | managed spark    		 | unmanaged spark       |
						 | (i.e. databricks)		 | (i.e. big tech)       |
should you use notebook? |         YES!     		 | Only proof of concept |
How to test job?         | Run the notebook 		 | spark-submit from CLI |		
Version control          | Git or notebook versioning| Git

How to look at Spark query plans
- Use explain() on your dataframes
	- This will show you the join strategies that Spark will take

How can Spark read data?
- From the lake
	- Delta Lake, Apache Iceberg, Hive metastore
- From an RDBMS
	- Postgres, Oracle, etc
- From an API
	- Make a REST call and turn into data
		- Be careful because this usally happens on the Driver
- From a flat file (CSV, JSON)
Spark might not be the best option for REST API,
Spark likes to read directly from a source "guts"

Spark output datasets
- Should almost always be partitioned on "date"
	- This is the execution date of the pipeline
	- In big tech this is called "ds partitioning" (Data Structure Partitioning)
By partitioning the data based on certain criteria (like geographical location or date ranges), you can distribute the data across several servers, enabling more efficient querying and better resource utilization.


Lecture 8
Advanced Apache Spark
--------------------------------------------

Spark Server vs Spark Notebooks
- Spark Server (Airbnb)
	- Every run is fresh, things get uncached automatically
	- Nice for testing
	- can be slower but better
	- CLI
- Notebook (Netflix)
	- Make sure to call .unpersist() <- (we want to know how data will run in prod without cache)
	- Start and Stop manually

Caching and Temporary Views
- Temporary views (similar to CTEs)
	- Always get recomputed unlessed cached
	- passing around the result sets will always call the query unless .cache is applied
- Caching
	- Storage levels
		- MEMORY_ONLY
		- DISK_ONLY
		- MEMORY_AND_DISK (the default)
	- Caching really only is good if it fits into memory
		- Otherwise there's probably a staging table in your pipeline your missing!
	- In notebooks
		- Call unpersist when you're done otherwise you'll have dead cache data
Matrialize view and caching to disk is very similar but you shoudln't use dish at all when caching it should all fit in memory (unless an edge case).
Caching the dataframe to disk (never the right answer) and writing it out in a table is the same thing

Caching vs Broadcast
- Caching
	- Stores pre-computed values for re-use
	- Stays partitioned
- Broadcast Join (generally ~2GB or smaller)
	- Small data that gets cached and shipped in entirely to each executor 
Both puts data into memory
An example of caching when dealing with big data
100GB we want to cache but its in 200 partitions, each executor is going to get 2GB and will have 4 tasks and each task will have 500MB cached

Broadcast JOIN optimization
- Broadcast JOINs prevent Shuffle
	- GOOD!
- The threshold is set
	- spark.sql.autoBroadcastJoinThreshold (defualt is 10MB can crank this up ~2GB+)
- You can explicity wrap a dataset with broadcast(df)
Most important opimization in spark, it gets triggered automatically when it can
Small is a rule of thumb
Small data set you don't have to split it up, nor shuffle
Its almost always better to use .broadcast(df) because you state this explicitly

UDFs (user defined functions)
PySpark quirks vs Scala
Apache Arrow optimizations in recent versions of Spark have helped PySpark
UDFs become more inline with Scala Spark UDFs
Dataset API allows you to not even need UDFs, you can use pure Scala functions instead
allows for complex logic and processing
Scala serialize -> Python Deserializing -> Python Logic -> Python serializing -> Scala deserialize
Scala spark gives you more performat UDAFs (user defined aggregate functions, very rare)
Scala spark gives you DataSetAPI over python
Scala compiles to JAVA byte code
BUT Scala is a learning curve and more people and jobs are using PySpark

DataFrame vs Dataset vs SparkSQL
Dataset is Scala only
Dataframe vs SparkSQL
	- DataFrame is more suited for pipelines that are more hardend and less likely to experience change
	- SparkSQL is better for pipelines that are used in collaboration with data scientists
	- Dataset is best for pipelines that require unit and integration tests
SparkSQL is lowest bar for entry, if your looking to change things quickly
Dataset allows you to create mock data more easily, handles null better (enforces quality control)
Use dataframe if your using PySpark, ScalaSpark Dataset API

Parquet
- An amazing file format
	- Run-length encoding allows for powerful compression
- Don't use global .sort()
	- Painful, slow, annoying
- Use .sortWithinPartitions
	- Parallizable = good distribution

Spark Tuning
- Executor Memory
	- Don't just set to 16GBs and call it a day, wastes a lot
- Driver Memory
	- Only needs to be bumped up if:
		- You're calling df.collect()
		- Have a very complex job
- Shuffle Partitions
	- Default is 200
	- Aim for ~100 MBs per partition to get the right sized output datasets
- AQE (adaptive query execution)
	- Helps with skewed datasets, wasteful if the dataset isnt skewed
How do you pick the right number of partitions?
If your data is uniform you want between 100-200MB per partition


Lecture 9
Unit/Integration testing Spark pipelines
--------------------------------------------
Where can you catch quality bugs?
- In development (best case) never surfaced to anyone
- In production, but not showing up in tables (still good)
- In production, production tables (terrible and destroys trust)
^ can go unnoticed for a period of time

How do you catch bugs in dev?
- Unit tests and integration tests of your pipelines
Such as UDFs or a spark lookup you want to write tests especially if they call a library (your not in control of third party libraries that may change)
Make sure to have a linter to make code more readable!

How do you catch bugs in production?
- Use the write-audit-publish pattern
write your data to a test or a stage table that has your same schema as your production table. Then you run your quality checks if they pass then you move your staging data into prod

Whats the worst situation?
- Data analyst finds bugs in production
- Ruins trust
- Ruins mood
- Nobody wins

Software engineering has higher quality standards than data engineering
- Risks
	- Server going down bigger impact than pipeline delay
	- Frontend being non-responsive stops the business too
- Maturation
	- Software engineering is a more mature field
	- Test-driven development and behavior-driven development are new in data engineering
- Talent
	- Data engineers come from a much more diverse background than SWEs

How will data engineering become riskier
- Data delays impact machine learning
	- Every day that notification ML was behind resulted in a ~10% drop in effectiveness and click through rates (as data gets stale)
- Data quality bugs impact experimentation
- As trust rises in data, risk rises too
What is the consequence of something breaking ande how to midigate that
We need to hold ourselfs to a higher standard because people rely on us more we have more responsibility. we have to build things that have provable quality constraints

Why do most organizations miss the mark on data engineering quality?
- When juniors start their first job
	- they don't write data quality check or unit tests their first months on the job
- What's happening here?
	- Data analytics doesn't have a culture of automated excellence
	- "The chart looks weird" is enough for an alarming number of analytics organizations
Data analytics are about answering questions quickly and software engineers build things that last
You can have both for data!

The tradeoff between business velocity and sustainability
- Business wants answers fast
- Engineers don't want to die from a mountain of tech debt
- Who Wins?
- Depends on the strength of your engineering leaaders and how much you push back
	- don't let other people burn you out, push back if needed!
- Don't cut corners to go faster in a more sustainable fashion (unless maybe its a one off)
We're not answering questions, we're building highways and roads.

Data Engineering is Engineering!

Data engineering capability standards will increase over time
- Latency
	- Solved with streaming pipelines and microbatch pipelines
- Quality
	- Solved with best practices and frameworks like Great Expectations
- Completeness
	- Solved with communication with domain experts
- Ease-of-access and usability
	- Solved by data products and proper data modeling
This creates value over time

How to do data engineering with a software engineering mindset
- Code is 90% read by humans and 10% executed by machines
- Silent failures are your enemy
- Loud failures are your friend
	- Fail the pipeline when bad data is introduced
	- Loud failures are accomplished by testing and CI/CD
The more readable your code is the better.
Throw an error if you run across a loud failure (could be bad parsing)

How to do data engineering with a software engineering mindset
- Dry code and YAGNI principles
	- SQL and DRY are very combative (SQL is hard to encapsulate)
- Design documents are your friend
- Care about efficiency
	- Data structures and algorthims
	- understanding JOIN and shuffle
YAGNI (your aren't gonna need it)

Should data engineers learn software engieering best practices
(the reverse should be true for myself)
- Short answer, YES
- LLMs and other things will make the analytics and SQL layer of data engineering job more susceptible to automation
- If you don't wnat to learn these things
	- GO INTO ANALYTICS ENGINEERING


Lecture 10
Build a Gold Pipeline the MIDAS Process
--------------------------------------------
think about trust:
Are the datasets your creating having the important impact to the business?
Is it answering important business questions?
Build good data documentation (should be the first step could discover things along the way)

What does data quality mean?
- DISCOVERABLE
- Misunderstood / incomplete definitions of data quality
	- Black Swan events
- There aren't any NULLs or duplicates
- There is business value being derived from the data
- The data is easy to use
- The data arrives in a timely manner
	- Have an agreed upon arival time
Every single data pipeline we write is linked to cost or/and revenue.
All revenue is sometimes not a clear path
Remember naming convention! columns prefix with m (metric) dim (dimension)

Data Quality is data trust + data impact

How do you build high trust in data sets?
- Before you start coding, LEAN INTO EMPATHY!
- A spec review of the pipeline design
	- Never review your own spec have someone else do it (fresh eyes)
- Clarify the business impact BEFORE coding
	- great for performance reviews
- Ask downstream stakeholders about all current and future needs
- Follow the Airbnb MIDAS process
ask stakeholders: if cost and time wasn't an objection what data would you want?

Airbnb MIDAS process
(9 steps)
1) Design Spec - this is the pipeline we are going to build this is how
2) Spec Review (locked) - tech review from a data architect and stakeholder
3) Build & Backfill Pipeline - Not all of history just one month
4) SQL Validation - Needs to happen by someone who isn't you (analytics engineer)
5) Minerva Validation - someone looking at the metrics
6) Data Review + Code Review (locked)
7) Minerva Migration - migrate the metrics
8) Minerva review (locked) - looked at by the staff data scientist
9) Launch PSA - We've created the dataset!

Why all this upfront work?
- Pre-build stakeholder buy-in dramatically increases data trust
- They feel like they have skin in the game
- Prevents painful backfills when you miss a requirement
Loop people in the process so they feel included
all communication is properly convayed
backfilling is painful

When should you do this heavy process?
- Are important decisions being made on this data?
- Is this data not going to change much over the next year?
Sometimes not doing the heavy process you can deliver more value

What is in a good design spec?
- Description (why are you building this?)
- Flow Diagrams (could use something like lucidchart)
- Schemas (DDL statements, create table statements)
- Quality checks
- Metric definitions
- Example queries

What does a good schema look like?
- Good names (fct, dim, scd, or agg in the table name)
- Column comments on every column
- Follow the naming conventions your company chooses
- If they don't have naming conventions, there's a spot for huge impact

Quality checks
- Basic checks
	- Not NULL, no duplicates, enum values are all valid
- Intermediate checks
	- Row count looks good, week-over-week row count cut by dimensions looks good
	- Are buiness rules enforced?
- Advanced checks
	- Seasonality adjusted row counts look good

Dimensional Quality Checks
- Dimensional tables usually:
	- Grow or are flat day-over-day -> table is growing check is very common
	- Don't grow sharply (percent difference from last week should be small)
	- Have complex relationships that should be checked -> think foreign keys in relational world

Fact Quality Checks
- Fact tables usually:
	- Have a seasonality to them (week-over-week is much better than day-over-day)
	- Affected by holidays (Xmas, New Years false positive a lot)
	- Can grow and shrink (table is growing is not a good check here)
	- More prone to duplicates (deduplicate is a must)
	- More prone to NULLs and other row-level quality issues
	- Have references to entities that should exist (similar to FK checks in relational)

These data quality checks could help the business in ways you least expect!

Fact Data Quality checks
- If you're using Presto, use APPROX_COUNT_DISTINCT instead of COUNT(DISTINCT) for deduped checks its 99.9% the same but dramatically more efficient


Lecture 11
WAP vs Signal for Data Quality
--------------------------------------------
What causes bad data?
- Logging errors (more for fact data)
	someone could change the schema of the logs, and it doesn't match table schema,
	or a dev could implement wrong behavior on a site.
- Snapshotting errors (more for dimensional data)
	happens rarely if at all, missing dimensions, missing users
- Production data quality issues
- Schema evolution issues
- Pipeline mistakes making it into production
	errors in JOIN or CASE WHEN etc, when you don't follow the contract process
- Non-idempotent pipelines and backfill errors
- Not thorough enough validation
Integrations can also break your validation due to you having no control of what the third party does!

Validation best practices
- Backfill a small amount of data (~1 month)
- Have someone else check all your assumptions
- Produce a validation report
	- Duplicates
	- Nulls
	- Violations of business rules
	- Time series / volume

Writing to Production is a Contract
- You checked the data the best you could
- The components of the contract:
	- Schema
	- Quality checks
	- How data shows up in production

Two flavors of the contact
- WAP (write-audit-publish)
- Signal table

WAP                                     yes
Pipeline->staging table->qaility pass? /--->exchange stag&prod partitions->prod table->down pipe
									   \--->is the check blocking^->manully troubleshoot DQ issue
									    no                     (yes->)(no^)
Writes to a staging table, should have same schema as production then run quality checks, if they passed move the data from stag to prod.
Blocking quality check very serious data quality issue you have to troubleshoot.
non-blocking check data weirdness fire an alert if non-blocking publish anyways.

Signal table (simplier pattern)
How its different is that there is no staging table:
Pipline->prod table->quality pass->publish signal table
                        ^- no fire alert
The downstream pipeline needs to wait on the signal table not the prod table
Does not catar to AdHoc quieres very well, everyone uses production table not the signal table so the prod could have bad data.

WAP vs Signal Table (different contracts)
PROS WAP:
	- Downstream pipelines can intuitively depend on the prod table directly
	- No chance of production data getting written without passing the audits
CONS WAP:
	- Partition exchange can be something that delays the pipeline by several minutes. More likely to miss SLA
PROS Signal Tables:
	- Data lands in just 1 spot and never has to be moved
	- More likely to hit SLA, data lands sooner
CONS Signal Tables:
	- Non-intuitive for your downstream users. What if they forget that you have a signal table? Higher likelihood of propagating DQ errors because of this non-intuitive design Adhoc queries
	- may return results from data that has failed audits
(Seems like we would always use WAP)

What happens when these contracts are violated?
- Bad data propagation (one of the worst issues)
- if you have no downstream dependencies, not a big issue
- if you have a lot, BIG ISSUE
	lots of downstream dependencies (similar to a tree for output data)
As you create datasets that are more critical and more used you need to follow these contracts so you can publish data with confidence!
If you don't follow these contracts for big companies it could end up costing a lot of $ with bad data/pipeline

Bad metric definitions can cause bad data as well
- The more different data points and dimensions your metric depends on, the more prone it is to error.
	- Knowing the engagement per qualified minute of Ethiopian children who use 7 year old iphone devices on xmas day is going to be a "noisy" metric
when coming up with metrics it should be broad (i.e. female iphone users in USA)
be careful not to be so narrow it could mess with your thresholds, also be careful with dimensional cuts

Lecture 12
Real-time Data Pipelines with Kafka and Flink
--------------------------------------------
What is a streaming pipeline?
- Streaming pipelines process data in a low-latency way
~1min - 30min when the data is generated to when it is processed
intra day means its going to run more than once a day

What's the difference between streaming, near real-time and real-time?
- Streaming (or continuous)
	- Data is processed as it is generated
	- Example: Apache Flink
- Near real-time
	- Data is processed in small/micro batches every few minutes
	- Example: Spark Structured Streaming (eventually this will support streaming)
Real-time and streaming are often synonymous but not always!
Less technical people might think real-time and batch are synonymous
Understand the latency requirements and not take your stakeholders word
(Streaming requirements happens less than 1/10 times, just understand that this concept means different things to different people)

What does real-time mean from stakeholders?
- It rarely means streaming
- It usually means low-latency or predictable refresh rate
You and stakeholders can have an SLA discussion (service level agreement) ask when do you want the data to refresh (usually they want data to refresh hours after midnight)

Should you use streaming?
- Considerations
	- Skills on the team
	- What is the incremental benefit
	- Homogeneity of your pipelines
	- The tradeoff between daily batch, hourly bath, microbatch, and streaming
	- How should data quality be inserted (batch pipelines have easier DQ paths)

Streaming-only use cases
- KEY: Low latency makes or breaks the use case
	- Detecting fraud, preventing bad behavior (don't let bad actors go on a spending spree!)
	- High-frequency trading
	- live event processing (sport analytics)
Should be somewhat obvious when to use streaming

Gray-area use cases (micro-batch may work too)
- Data that is served to customers
- Reducing the latency of upstream master data
	- Notifications dataset had an 9 hour after midnight latency
	- micro batch cut it to an hour

No-go Streaming use cases (use batch)
- Ask the question
	- What is the incremental benefit of reduced latency?
- Analysts complaining that the data isn't up-to-date
	- Yesterday's data by 9am is good enough for most analytical use cases
sometimes it could be a waste of your time to reduce the latency.
a business shouldn't focus on noise and wait for a signal its like looking at min to min stock changes which could waste time when it reality should just look at day open and day close
If someone from data analytics complains the data is not up to date just tell them what would be different if it was up to date (most of the time it would be nothing).

How are streaming pipelines different from batch pipelines?
- Streaming pipelines run 24/7, batch pipelines run for a small percentage of the day (maybe 5 mintues out of the day not even 1% of the day)
- Streaming pipelines are much more software engineering oriented
	- They act a lot more like servers than DAGs
- Streaming pipelines need to be treated as such and have more unit test and integration test coverage like servers

The lower the latency the higher the engineering complexity

The streaming -> batch continuum
- Real time is a myth (network latency)
	- You'll have seconds of latency just for the event generation -> Kafka -> Flink -> sink
- Pipelines can be broken into 4 categories
	- Daily batch
	- Hourly batch (near real-time)
	- Microbatch (near real-time)
	- continuous processing (real-time)
Even with streaming it might take minutes to know that you've been hacked

The structure of a streaming pipeline
- The sources
	- Kafka, RabbitMQ (queues - data in motion) (Kafka usually scales better, rabbit has more compleex routing i.e. message broker pub/sub)
- Enriched dimensional sources (i.e. side inputs)
- The compute engine
	- Flink
	- Spark Structured Streaming
- These engines make sense of the incoming streams of data

