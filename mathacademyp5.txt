Probability
Random Variables
Transformations of random variables
Expectation
Discrete Probility Distributions
Continuous Probability Distributions

NOTE: Some of these problems are easy so showing steps would just take up space and time, the answer should be intutive on how to get there.

"Rationality is not about knowing facts, it’s about recognizing which facts are relevant."

NOTE: 
Probability Distrubtion functions (PDF) is f(x) tells you the shape of the graph (expontential, normal, or uniform distribution).
Cumulative Probability Distribution functions (CDF), gives you the area under the curve up to a point. (usually to the left) P(X <= x)
dF(x)/dx = f(x) from CDF -> PDF
∫^x_-∞ f(x)dx = F(x) PDF -> CDF

NOTE:
Another identity sin(pi/2 +/- y) = cos(y)

--------------------------------------------

The law of total probability
three events B1, B2, and B3 exist within a sample space S
B1, B2, and B3 are mutually exculsive and the events cover the entire sample space.
When these two conditions are satisfied, S is the disjoint union of B1, B2, and B3 we write
S = B1 ⊔ B2 ⊔ B3

This is shown below:
|-------------------|S
|       /    \      |
|  B4  /  B2  \ B3  |
|     /        \    |
|    /          \   |
|-------------------|

We now wish to compute the probability of an event A that lies within S, and its intersections with B1, B2, B3
(the inner box is A, outer is S)

|-------------------|S
| ______/____\______| 
| |A∩B1/ A∩B2 \A∩B3|| 
| |__ /________\___||
|    /          \   |
|-------------------|

The law of total probability can be deduced from this diagram, it states that
P(A) = P(A ∩ B1) + P(A ∩ B2) + P(A ∩ B3)
the law of total probability can be extended to arbitrary number of events
P(A) = P(A ∩ B1) + P(A ∩ B2) + ...
= Σ_i P(A ∩ Bi)
where S = B1 ⊔ B2 ⊔ ... is disjoint union of S

A worked example
suppose that a particular garden is divided into three small plots. 10 tulips in the first plot 5 are yellow, 12 tulips in the second plot 6 are yellow and 14 tulips in the third plot 4 are yellow.
Let A be the event that a chosen tulip is yellow
B1 the event that the selected tulip comes from the first plot
B2 " " second plot
B3 " " third plot
disjoint union
B1 ⊔ B2 ⊔ B3
covers the entire sample space, and that there a total of 10+12+14 = 36
to calculate P(A ∩ B1) we take the number of yellow tulips in the first plot (5) and divide by total (36)
P(A ∩ B1) = 5/36
P(A ∩ B2) = 6/36
P(A ∩ B3) = 4/36
the probability that a randomly chosen tulip is yellow
5/36 + 6/36 + 4/36 = 15/36
= 5/12
there is an easier way to solve this problem sum total of yellow tulips (5+6+4=15) and divide by the total number of tulips (10+12+14=36) we have to do it the way that we did to get used to the law of probability because they sometimes aren't intuitive and understanding law of probability will help when things get trickier.

Applying the law of total probability
A store has 40 employees of which 21 are male. The store is divided into three departments A,B, and C, there are 7 male workers in department A, 8 in C what is the probability that a randomly selected store worker is a male that works in department B?
Let M be the event that the chosen worker is a male
A be the event that the selected worker works in department A
B " " department B
C " " department C
the disjoint union covers the entire sample space
A ⊔ B ⊔ C
law of total probability states:
P(F) = P(F ∩ A) + P(F ∩ B) + P(F ∩ C)
we want to find P(F ∩ B)
P(F ∩ A) 7 male workers in dep A by total 7/40
P(F ∩ C) 8/40
we are given P(F) = 21/40
substituting the above info into the law of total probability and solving for P(F ∩ B)
P(F) = P(F ∩ A) + P(F ∩ B) + P(F ∩ C)
21/40 = 7/40 + P(F ∩ B) + 8/40
P(F ∩ B) = 21/40 - 7/40 - 8/40
P(F ∩ B) = 6/40
P(F ∩ B) = 3/20

example:
Melissa has three email accounts, the first account received 16 emails, of which 5 spam. Second account received 10 emails, and 2 were spam. Third account 14 emails 3 were spam. What is the probability that a randomly selected email across all her accounts is spam?
10/40 = 1/4

example:
Mathias has three ranches A,B, and C. There are 100 goats in total across all three ranches. Ranch A has 14 ice cream goats, in ranch B there are 9 ice cream goats. If 35% of all goats are ice cream goats, what is the probability that a randomly selected goat is a ice cream goat from ranch C.
(35% are ice cream goats from the 100 total)
Ranch A 14/100
Ranch B 9/100
Ranch C 12/100
12% is the probability that I would select an ice cream goat from ranch C

Its more helpful to express the law of total probability using conditional probabilities. From the multiplication law for conditional probability:
P(A ∩ B1) = P(A|B1)P(B1)
P(A ∩ B2) = P(A|B2)P(B2)
P(A ∩ B3) = P(A|B3)P(B3)
substituting the above form into the law of total probability
P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + P(A|B3)P(B3)

example:
Applying the law of total probability in terms of conditional probability
Three factories produce the same spare part for a particular car model. Factory A produces 32% of the overall parts, 1.5% are defective. Factory B produces 33% parts, 2% are defective, factory C produces 35% parts, 1.4% are defective. If one part is selected randomly, what is the probability that is defective?
D be the event that a randomly selected part is defective
A be the event that the selected part was produced by Factory A
B " " Factory B
C " " Factory C
the disjoint union covers the whole sample space
A ⊔ B ⊔ C
The probability that a randomly selected part is defective, we can use the law of total probability
P(D) = P(D ∩ A) + P(D ∩ B) + P(D ∩ C)
= P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C)
Factory A produces 32% of the parts, factory B 33%, factory C 35%
P(A) = 0.32, P(B) = 0.33, P(C) = 0.35
1.5% parts from factory A, 2% parts from factory B, 1.4% parts from factory C are defective
P(D|A) = 0.015, P(D|B) = 0.02, P(D|C) = 0.014
subtituting
P(D) = P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C)
= (0.015)(0.32) + (0.02)(0.33) + (0.014)(0.35)
= 0.0048 + 0.0066 + 0.0049
= 0.0163
this is the probability of randomly selecting part is defective

example:
Apple manufactures iphones. of the iphones manufactured, 45% are produced by the first machine, 25% are produced by second machine, third machine produces the remaining. If 8% of the iphones produced by the first machine, 4% by second, 5% by third are defective what is the probability that a randomly selected iphone is defective? 
(0.08)(0.45) + (0.04)(0.25) + (0.05)(0.30)
0.036 + 0.01 + 0.015 = 0.061

example:
We have three backpacks containing bananas. The probability of picking a bruised banana from backpack A is 1/6, backpack B is 1/5, backpack C 1/3. A backpack is selected at random and a banana is randomly selected from the backpack. What is the probability that the selected banana is bruised?
backpack is random (3 total)
let N equal the bruised banana event
P(A) = P(B) = P(C) = 1/3
P(N|A) = 1/6, P(N|B) = 1/5, P(N|C) = 1/3
P(N) = P(N|A)P(A) + P(N|B)P(B) + P(N|C)P(C)
= (1/6)(1/3) + (1/5)(1/3) + (1/3)(1/3)
= 1/18 + 1/15 + 1/9
= 21/90
= 7/30

finding a conditional probability using the law of total probability
Three bags A,B, and C contains pears. It is known that 2/5 pears in bag A and 1/4 pears in bag B are green. A bag is selected at random, and a pear is randomly chosen from the bag. If the probability that the selected pear is green is 1/3 what proportion of pears from bag C are green?
Let G be the event that the selected pear is green
Let A be the event that the selected pear is from bag A
Let B " " bag B
Let C " " bag C
A ⊔ B ⊔ C
P(A) = P(B) = P(C) = 1/3
2/5 of the pears in bag A and 1/4 of pears in bag B are green
P(G|A) = 2/5, P(G|B) = 1/4
the probability of picking a green pear from any of the three bags is 1/3
P(G) = 1/3
1/3 = (2/5)(1/3) + (1/4)(1/3) + P(G|C)(1/3)
1/3 = 2/15 + 1/12 + 1/3 * P(G|C)
1/3 * P(G|C) = 1/3 - 2/15 - 1/12
1/3 * P(G|C) = 7/60
P(G|C) = 7/20
7/20 of pears from bag C are green

example:
A science teacher splits her chemistry class into three groups. 35% of all students are in Group A, 35% are in group B, and 30% are in group C. If 20% of the students in Group A obtained the top grade 10% in group B obtained top grade, and the probility that a randomly selected student from any group obtained the top grade is 0.255, what is the probability that a randomly selected student from Group C obtained top grade?
P(A) = 0.35, P(B) = 0.35, P(C) = 0.30
P(T|A) = 0.20, P(T|B) = 0.10
P(G) = 0.255
0.255 = (0.20)(0.35) + (0.10)(0.35) + P(T|C)(0.30)
0.255 = 0.07 + 0.035 + 0.30P(T|C)
0.30P(T|C) = 0.15
P(T|C) = 0.5

At a grocery store 40% of the customers by store brand toilet paper, 45% buy charmen toilet paper and 15% buy quiltern. It is known that 65% of the customers who purchase store brand toilet paper and 50% of customers who buy charmen pay with credit card. If the probability that a randomly selected customer pays with a credit card is 57.5% what percentage of quiltern toilet paper customer pay using a credit card?
.575 = (0.40)(0.65) + (0.45)(0.50) + P(Q|C)(0.15)
.575 = 0.26 + 0.225 + 0.15P(Q|C)
.575 = 0.485 + 0.15P(Q|C)
.575 - 0.485 = 0.15P(Q|C)
0.09 = 0.15P(Q|C)
P(Q|C) = 0.6 = 60%
60% of quilten toilet paper customers pay with credit card

===================================================
https://www.youtube.com/watch?v=HZGCoVF3YvM

a little confusing for me was what should be on the left of | and on the right.
example: 
If you know a student is in math class and you want to determine their likelihood of being female we use F(F|M)
if you know the student is female and want to determine their likelihood of being in math class use P(M|F)

subtle difference but I think the example helps.

example:
If you have a bag of candies and you know that some candies are red (event R) and some are square (event S), asking:
What’s the chance a square candy is red? P(R|S)
What’s the chance a red candy is square? P(S|R)
what you know versus what you’re trying to find out. Always ask: "Given what?" to determine the correct format.

For two events A and B, Bayes' theorem states that
P(A|B) = P(B|A)P(A)/P(B)
Deriving Bayes' theorem, recall that the multiplication law states that
P(A|B) = P(A ∩ B)/P(B) (*)
Swapping A and B in the multiplication law and noting that P(B ∩ A) = P(A ∩ B)
P(B|A) = P(A ∩ B)/P(A)
which can be written as 
P(A ∩ B) = P(B|A)P(A)
substituting the above expression for P(A ∩ B) into (*) gives Bayes' theorem

Computing a conditional probability using bayes' theorem
given that P(K|L) = 0.15, P(K) = 0.6 and P(L) = 0.4 compute P(L|K)
P(L|K) = P(K|L)P(L)/P(K)
= (0.15)(0.4)/0.6
= 0.06/0.6
= 0.1

example:
given P(B|A) = 0.2, P(A) = 0.3 and P(B) = 0.4 compute P(A|B)
P(A|B) = (0.2)(0.3)/(0.4) = 0.15

example:
given P(C|A) = 0.6, P(A) = 0.4 and P(C) = 0.3 compute P(A|C)
P(A|C) = (0.6)(0.4)/(0.3) = 0.8

Bayes' theorem
P(A|B) = P(B|A)P(A)/P(B)
("|" given that)
we can rewrite Bayes' theorem in a slightly different way
P(A|B) = P(A) * P(B|A)/P)B
Bayes' theorem is significant because it allows us to update our belief that a particular hypothesis is true when new evidence comes to light.
Suppose a doctor wishes to asses the probability that a certain patient is infected with a particular disease. It is known that those aged 65 or older have a higher change of carrying a disease.
A = the event that a randomly selected patient has the disease
B = the event that a randomly selected patient is 65 years old or more
P(has disease | patient 65+) = P(has disease) * P(patient 65+ | has disease)/P(patient 65+)
P(has diesease) is called prior. It tells us the probability that a randomly selected patient has disease without considering other factors
P(has diesease | patient 65+) is the likelihood. It tells us the proportion of infected people that are 65 or older. This quantity could be established by taking a random sample from the population of infected people and computing the ratio of those that are 65 or older
Bayes' theorem allows us to update the hypothesis (the paitent has the disease) taking into account information about the patient (they are 65 or older) making use of existing available evidence (an estimate of the proportion of infected people that are 65 or older)
if more evidence is introduced we will further improve the accuracy of the posterior.

Applying Bayes' theorem
Ireland citizens showed that 17% of all males smoke. Overall 27.3% of the population smoke, 51% of the population are male. If a smoker from this population is selected at random, what is the probability of them being male?
Let S be the event that a randomly selected person smokes and let M be the event that they are male, we require P(M|S) we can compute using Bayes' theorem
P(M|S) = P(S|M)P(M)/P(S)
P(S|M) = 0.17, 17% of males smoke
P(M) = 0.51 since 51% of the population are male
P(S) = 0.273 since 27% of population smoke
= (0.17)(0.51)/0.273
= 32% (rounded)

example:
On a particular road, 80% of all vehicles are cars. It its known that 5% of all cars are white and 10% of all vehicles are white. If a white vehicle is selected at random, what is the probability that it is a car?
P(C|W) = P(W|C)P(C)/P(W)
P(W|C) = 0.05, 5% of all cars are white
P(C) = 0.8, since 80% of all vehicles are cars
P(W) = 0.1, 10% of all vehicles are white
= (0.05)(0.8)/0.1

example:
A country showed 90% of all people with a degree speak 2 languages. Overall 63% of the population speaks 2 languages, and 31% of population has a degree. A person who speaks 2 languages is selected at random, what is the probability of them having a degree?
P(D|L) = P(L|D)P(D)/P(L)
= (0.90)(0.31)/0.63
P(L|D) 0.9, 90% of people have a degree speak 2 languages
P(D) 0.31, 31% has degree
P(L) 0.63, 63% speak two languages

Expressing Bayes' theorem in an alternative form using the law of total probability
Bayes' theorem
P(A|B) = P(B|A)P(A)/P(B)
the law of total probability tells us
P(B) = P(B|A)P(A) + P(B|A')P(A')
substituting into bayes' theorem
P(A|B) = P(B|A)P(A)/(P(B|A)P(A) + P(B|A')P(A'))

Applying Bayes' Theorem with the law of total probability
A bag contains 2 black pretzels and 4 green pretzels, a second bag has 2 black pretzels and 2 green pretzels, a bag is chosen at random and a pretzel is selected from the chosen bag also at random. Given that the selected pretzel is green what is the probability that it came from the second bag?
Let G be the event that the selected pretzel is green and let S be the event that the second bag is chosen, then we requre P(S|G) which we can compute using Bayes' theorem
P(S|G) = P(G|S)P(S)/P(G) = P(G|S)P(S)/P(G|S)P(S) + P(G|S')P(S')
first bag contains 4 green pretzels and 2 black
P(G|S') = 4/4+2 = 2/3
second bag 2 green, 2 black pretzels
P(G|S) = 2/2+2 = 1/2
bags are selected at random
P(S) = P(S') = 1/2
using Bayes' Theorem:
P(G|S)P(S)/P(G|S)P(S) + P(G|S')P(S')
= ((1/2)(1/2))/((1/2)(1/2) + (2/3)(1/2))
= (1/4)/(7/12)
= 6/14
= 3/7

example:
Two identical backpacks contains books. First backpack has 1 math book and 1 science book, the second backpack has 3 math books and 2 science books. A backpack is selected at random, and one book is chosen from the selected backpack also at random. If the selected book is a science book what is the probability that is is from second backpack?
first backpack 1 science and 1 math book
P(S|B') = 1/2
second backpack 3 math, 2 science books
P(S|B) 2/3+2 = 2/5
backback selected at random
P(B) = P(B') = 1/2
P(B|S) = P(S|B)P(B)/P(S|B)P(B) + P(S|B')P(B')
= ((2/5)(1/2))/((2/5)(1/2) + (1/2)(1/2))
= (2/10)/(2/10) + (1/4)
= (2/10)/(4/20) + (5/20)
= (2/10)/(9/20)
= 40/90
= 4/9

example:
At a college 10% of males and 15% of females play baseball. If 40% of students in the college are male what is the probability that a randomly chosen baseball player is male? round to 4 decimals
P(B|M) = (randomly chosen a baseball player, hypothesis are they male)
P(M|B)P(M) = (males that play baseball)(total males)
P(M|B')P(B') = (females that play baseball)(total females)
= P(B|M) = P(M|B)P(M)/P(M|B)P(M) + P(M|B')P(B')
= (0.10)(0.40)/(0.10)(0.40) + (0.15)(0.60)
0.04/0.13
~= 0.3077

Accuracy of medical tests
A lab test for a particular disease is conducted on a few patients. 94% patients are infected and 2% are not infected tested positive for disease. 5% of the entire population is infected with the disease. What is the probablity that a randomly chosen patient that tested postive is not infected. round to percent.
D = the event that a randomly chosen patient is infected with the disease
T = the event they tested positive
P(D'|T) = P(T|D')P(D')/P(T) = P(T|D')P(D')/P(T|D)P(D) + P(T|D')P(D')
An infected person tests positive is 94%, probability that a non-infected person tests positive is 2%
P(T|D) = 0.94, P(T|D') = 0.02
only 5% of the population has the disease
P(D) = 0.05, P(D') = 1 - 0.05 = 0.95
P(T|D')P(D')/P(T|D)P(D) + P(T|D')P(D')
= (0.02)(0.95)/(0.94)(0.05) + (0.02)(0.95)
~= 0.287
~= 29%
when a non-infected patient tests positive for a disease, it is a false postive
when an infected patient tests negative for a disease, it is a false negative.

example:
(the wording messed me up here)
a lab tests for a specific disease, 97% of patients are infected, 1% of those are not infected tested positive for the disease. Only 1% of the entire population is infected with the disease. What is the probability that a randomly chosen patient that tested positive is infected? round to 2 decimals
D chosen patient is infected with disease
T tested postive
P(D|T) = P(T|D)P(D)/P(T) = P(T|D)P(D)/P(T|D)P(D) + P(T|D')P(D')
infected person testing positive is 97%, not infected testing positve is 1%
P(T|D) = 0.97, P(T|D') = 0.01
only 1% of population has the disease
P(D) = 0.01, P(D') = 1 - 0.01 = 0.99
= (0.97)(0.01)/(0.97)(0.01) + (0.01)(0.99)
~= 0.49

example:
(same question worded differently)
95% infected, and 2% of those positive for disease. 20% of entire population is infected with the diesease. What is the probability a random chosen patient tested negative is infected
Infected person testing positive: 95%, non-infected 2%
P(T'|D) = 1 - 0.95 = 0.05,  P(T'|D') = 1 - 0.02 = 0.98
only 20% of population has disease
P(D) = 0.20, P(D') = 0.80
P(D|T') = P('T|D)P(D)/P('T|D)P(D) + P(T'|D')P(D')
(0.05)(0.2)/(0.05)(0.2) + (0.98)(0.8)
~= 0.0126

Generalize Bayes' theorem where sample space S is partitioned into disjoint union of multiple events
if the disjoint union A1 ⊔ A2 ⊔ ... ⊔ An covers the entire sample space, Bayes' theorem combined with the law of total probability
P(Ak|B) = P(B|Ak)P(Ak)/(Σ^n_i=1 P(B|Ai)P(Ai))

computing a conditional probability using bayes' theorem
The disjoint union A1 ⊔ A2 ⊔ A3 of events A1, A2, and A3 covers the entire sample space, compute P(A1|B) given the following probabilities:
P(B|A1) = 0.23
P(B|A2) = 0.26, P(A2) = 0.3
P(B|A3) = 0.18, P(A3) = 0.4
if the disjoint union A1 ⊔ A2 ⊔ ... ⊔ An covers the entrie sample space, Bayes' theorem combined with the law of total probability
P(Ak|B) = P(B|Ak)P(Ak)/P(B) = P(B|Ak)P(Ak)/(Σ^n_i=1 P(B|Ai)P(Ai))
where k = 1,2,...,n
in this case n = 3 A1 ⊔ A2 ⊔ A3 covers the entire sample space
P(A1) + P(A2) + P(A3) = 1
P(A1) = 1 - (P(A2) + P(A3))
= 1 - (0.3 + 0.4)
= 0.3
P(A1|B) = P(B|A1)P(A1)/Σ^n_i=1 P(B|Ai)P(Ai)
= P(B|A1)P(A1)/P(B|A1)P(A1) + P(B|A2)P(A2) + P(B|A3)P(A3)
= (0.23)(0.3)/(0.23)(0.3) + (0.26)(0.3) + (0.18)(0.4)
= 0.069/0.219
~= 0.315

example:
The disjoint union A1 ⊔ A2 ⊔ A3 of events A1, A2, and A3 covers the entire sample space, compute P(A2|B) given the probabilities
P(B|A1) = 0.4, P(A1) = 0.2
P(B|A2) = 0.5, P(A2) = 0.3
P(B|A3) = 0.3, P(A3) = 0.5
(0.5)(0.3)/(0.5)(0.3) + (0.4)(0.2) + (0.3)(0.5)
0.15/0.38
~= 0.395

example 
The disjoint union A1 ⊔ A2 ⊔ A3 of events A1, A2, and A3 covers the entire sample space, compute P(A3|B) given the probabilities
P(B|A1) = 0.5, P(A1) = 0.1
P(B|A2) = 0.6, P(A2) = 0.2
P(B|A3) = 0.1, P(A3) = 0.7
(0.1)(0.7)/(0.5)(0.1) + (0.6)(0.2) + (0.1)(0.7)
= 0.07/0.24
~= 0.29

Applying Bayes' Theorem
A travel agency offers three itineraries, itinerary 1, itinerary 2, and 3. For each itinerary customers can choose to travel using their own car or the agency bus
itinerary 1 is chosen by 25% of the customers, and 60% of those customers travel by bus
itinerary 2 " " 30% " " 30%
itinerary 3 " " 45% " " 20%
if 65% of all customers travel with children and 40% customers with children choose the bus, what is the probability that a randomly chosen customer that chooses the bus travels with children?
Let C be the event that a customer travels with children, B be the event that a customer takes the bus, and I1, I2, I3 be the event that a customer chooses itinerary 1,2, or 3
P(I1) = 0.25, P(B|I1) = 0.6
P(I2) = 0.3, P(B|I2) = 0.3
P(I3) = 0.45, P(B|I3) = 0.2
disjoint union
P(I1) + P(I2) + P(I3) = 0.25 + 0.3 + 0.45 = 1
65% of customers have children
P(C) = 0.65
40% of customers with children chose bus
P(B|C) = 0.4
We want to find the probability that a randomly chosen customer that chooses the bus travels with children, which is represented as P(C|B)
Using Bayes' theorem combined with the law of total probability
P(C|B) = P(B|C)P(C)/P(B)
= P(B|C)P(C)/P(B|I1)P(I1) + P(B|I2)P(I2) + P(B|I3)P(I3)
= (0.4)(0.65)/(0.6)(0.25) + (0.3)(0.3) + (0.2)(0.45)
= 0.26/0.33
~= 0.788

example:
Students in a school are split amount three classes, class 1, class 2, and class 3
Class 1 contains 25% of the students in the school, 55% of those students take math
Class 2 contains 35% of the students in the school, 40% of those students take math
Class 3 contains 40% of the students in the school, 60% of those students take math
if 45% of students in the entire school are female and 80% of female students take math, what is the probability that a randomly chosen math student is female?
P(C1) = 0.25, P(M|C1) = 0.55
P(C2) = 0.35, P(M|C2) = 0.40
P(C3) = 0.40, P(M|C3) = 0.60
P(F) = 0.45
P(M|F) = 0.80
P(F|M) = P(M|F)P(F)/P(M|C1)P(C1) + P(M|C2)P(C2) + P(M|C3)P(C3)
= (0.80)(0.45)/(0.55)(0.25) + (0.40)(0.35) + (0.60)(0.40)
= (0.36)/(0.5175)
~= 0.696

example:
Matt and Molly are finding the best recipe for vanilla cookies, they test three different oven cooking modes, mode1, mode2, and mode3. They also try two types of dough A and B
They use mode1 to cook the 30% of their cookies and 90% of cookies turn out well
They use mode2 to cook the 40% of their cookies and 85% of cookies turn out well
They use mode3 to cook the 30% of their cookies and 88% of cookies turn out well
if 20% of all cookies was prepared with dough A and 87% of dough A cookies turn out well, what is the probability that a randomly chosen good cookie is done with dough A?
P(M1) = 0.30, P(W|M1) = 0.90
P(M2) = 0.40, P(W|M2) = 0.85
P(M3) = 0.30, P(W|M3) = 0.88
P(A) = 0.20 P(A') = 0.80
P(W|A) = 0.87
P(W|A)P(A)/P(W|M1)P(M1) + P(W|M2)P(M2) + P(W|M3)P(M3)
P(A|W) = (0.87)(.20)/(0.90)(0.30) + (0.85)(0.40) + (0.88)(0.30)
= (0.174)/(0.874)
~= 0.199

solving for an unknown conditional probability
corn produced by a particular farm in three different varieties, variety1, variety2, variety3
36% of the corn are of variety1, and 20% of those corn weigh more than 100g
17% of the corn are of variety2, and 6% of those corn weigh more than 100g
47% of the corn are of the variety3
if 16% of corn from the entire farm are sold to a local restaurant, 40% of corn sold to the resturant weigh more than 100g and 15% of corn weighs more than 100g are sold to the restaurant, what is the probability that a randomly selected potato form variety 3 weighs more than 100g?
R = corn is sold to restaurant
W = corn weighs more than 100g
V1,V2,V3 corn variety
P(V1) = 0.36, P(W|V1) = 0.2
P(V2) = 0.17, P(W|V2) = 0.06
P(V3) = 0.47
disjoint union V1 ⊔ V2 ⊔ V3 covers entire sample space
P(V1) + P(V2) + P(V3) = 0.36 + 0.17 + 0.47 = 1
16% of corn from entire farm are sold to local restaurant
P(R) = 0.16
40% corn sold to the resturant weigh more than 100g
P(W|R) = 0.4
15% of corn that weigh more than 100g are sold to the restaurant
P(R|W) = 0.15
We want to find the probability that a random chosen corn of variety 3 weighs more than 100g P(W|V3)
Using Bayes' theorem combined with the law of total probability
P(R|W) = P(W|R)P(R)/P(W)
P(R|W) = P(W|R)P(R)/P(W|V1)P(V1) + P(W|V2)P(V2) + P(W|V3)P(V3)
0.15 = (0.4)(0.16)/(0.2)(0.36) + (0.06)(0.17) + P(W|V3)(0.47)
0.15 = 0.064/0.0822 + 0.47P(W|V3)
solving for P(W|V3)
0.15(0.0822 + 0.47P(W|V3)) = 0.064
0.0822 + 0.47P(W|V3) = 0.064/0.15
0.0822 + 0.47P(W|V3) = 32/75
0.47P(W|V3) = 32/75 - 0.0822
P(W|V3) = 1/0.47(32/75 - 0.0822)
P(W|V3) ~= 0.733

Students at a particular school are split among three class1, class2, class3
Class 1 contains 25% of the students in the school and 55% of those students take math
Class 2 contains 35% of the students in the school and 55% of those students take math
Class 3 contains 40% of students in the school.
if 45% of students in the entire school are female, 80% of female students take math and 70% of math students are female, what is the probability that a randomly selected student from class 3 takes math?
P(C1) = 0.25, P(M|C1) = 0.55
P(C2) = 0.35, P(M|C2) = 0.40
P(C3) = 0.40
P(F) = 0.45
P(F|M) = 0.80 (female students take math)
P(M|F) = 0.70 (math students are female)
0.70 = (0.80)(0.45)/(0.55)(0.25) + (0.40)(0.35) + P(M|C3)(0.40)
0.70 = 0.36/0.2775 + P(M|C3)(0.40)
P(M|C3) = 1/0.40(0.36/0.70 - 0.2775)

example:
Kaz, and Sam are finding the best recipe for vanilla cookies they test three different oven cooking modes, mode1, mode2, and mode3 they also try two types of dough A and B
they use mode1 to cook 45% of their cookies and 80% of those cookies turn out good.
they use mode2 to cook 35% of their cookies
they use mode3 to cook 20% of their cookies and 90% of those cookies turn out good.
if 65% of all cookies were prepared with dough A, 82% of dough A cookies turn out good, and 68% of good cookies were prepared with dough A, what is the probability that a randomly chosen cookie cooked with mode2 turned out good.
P(M1) = 0.45, P(G|M1) = 0.80
P(M2) = 0.35
P(M3) = 0.20, P(G|M3) = 0.90
P(A) = 0.65
P(G|A) = 0.82 P(A|G) = 0.68
0.68 = (0.82)(0.65)/(0.80)(0.45) + P(G|M2)(0.35) + (0.90)(0.20)
0.68 = 0.533/0.36 + P(G|M2)(0.35) + 0.18
0.68 = 0.533/0.54 + P(G|M2)(0.35)
P(G|M2) = 1/0.35(0.533/0.68 - 0.54)
~= 0.697

===================================================

A random variable X is a variable whose value is deteremined by the outcome of a trail or experiment. A random variable can take on one of many values from set S called the range or support of X.
A continuous random variable is a random variable whose set of possible values S is given by an interval of real numbers

The height of a randomly selected person could be modeled as a continuous random variable because its possible values from an interval of an interval of real numbers. For modeling purposes we usually assume that the height of a randomly selected person could be any number in the real interval (0, infinity), although we could also restrict this to a finite interval [a,b] if we wanted

The result of rolling 6-sided die is not a continuous random variable because its possible values are 1,2,3,4,5,6, which does not correspond to an interval of real numbers.
physical quantities such as mass, length, area, volume, time, fluid concentration, etc. are usually modeled as continuous variables

Identifying continuous random variables
what is a continuous random variable?
the length of randomly selected baseball field
the weight, in grams, of a slice cut from a cake weighing 1kg
the percent concentation of fruit concentrate in a radomly selected fruit drink
the weight in kilograms of sugar in a randomly selected bag with capacity of 1kg

Probability density functions
The probability density function (pdf) of a continuous random variable X defined over a set S is a function f(x) such that the probability that X lies in the interval [a,b]
P(a <= X <= b) = ∫^b_a f(x) dx
the probability that X lies between a and b can be interpreted as the area bounded between f(x) and the x-axis over the interval x ∈ [a,b]
For a function f(x) to be a valid probability density function over a set S it must satisfy the following
f(x) >= 0 for all x in S
∫_S f(x) dx = 1
let random variable X be the result of randomly selecting a real number between 0 and 6. In this case the pdf of X is given by
f(x) = {
	1/6, 0 <= x <= 6
	0,   otherwise
}
this pdf tells us that any number between 0 and 6 has an equal chance of being selected and no number outside this interval can be selected.
this is a valid pdf because its always nonnegative
∫_S f(x) dx = ∫^6_0 1/6 dx
= [1/6(x)]|_0-6
= [1/6 * 6] - [1/6 * 0]
= 1
to compute the probability that our randomly delected number is no greater than 3 we integrate the pdf over the interval [0,3]
P(0 <= X <= 3) = ∫^3_0 1/6 dx
= [1/6(x)]|_0-3
= [1/6 * 3] - [1/6 * 0]
= 1/2
results matches our intuition since every number has an equal chance of being selected, and [0,3] is half the length of [0,6]

Interpreting probability density functions
The probability density function of a continuous random variable X is somewhat analogous to the probability mass function of a discrete random variable in the sense that it describes that probability distribution of X
Probability density is not the same as probability, instead we can think of a pdf as a tool for computing probabilites for continuous random variables.

two counterinteruitive yet important points to be aware of when it comes to interpreting a pdf
the probability that a continuous random variable takes on any particular value is always equal to 0
continuing the previous example, the probability that our random variable takes on the value X = 5 is equal to
P(X = 5) = P(5 <= X <= 5) = ∫^5_5 1/6 dx = 0
for any value a and any continuous random variable X with probability density function f(x)
P(X = a) = P(a <= X <= a) = ∫^a_a dx = 0
the values of a probability density function may be greater than 1
let Y be the result of randomly selecting a real number between 0 and 1/2 then the pdf of Y is given by
f(y) = 2, y ∈ [0,1/2]
this pdf tells us that all numbers between 0 and 1/2 are equally likely. f(y) is nonnegative
∫^(1/2)_0 f(y) dy = ∫^(1/2)_2 dy = 1
f(y) is a valid probability function
Since the probability that a continuous random variable takes on a particular value is always equal to 0, it doesn't matter wheter or not we include the endpoints of a particular interval for any continuous random variable X
P(a <= X <= b) = P(a < X < b) = P(a < X <= b) = P(a <= X < b)
= ∫^b_a f(x) dx

example:
(a graph going from x = -1 to x = 1 looks like a smile and y is above the x-axis)
f(x) = {
	3/8 (x^2 + 1),  -1 < x <= 1
	0,              otherwise
}
which is true?
1. f(x) >= 0 for all x
2. ∫^1_-1 f(x) dx = 1
3. f(x) is a valid probility density function
for a function f(x) to be a valid probability density function on set S, it must satisify the following conditions
f(x) >= 0 for all x
∫_S f(x) dx = 1
statement 1 is true, f(x) is nonnegative for all x
statement 2 is true
∫^1_-1 f(x) dx = ∫^1_-1 3/8 (x^2 + 1) dx
= 3/8 ∫^1_-1 x^2 + 1 dx
= 3/8 [x^3/3 + x]|_-1-1
= 3/8 ([1^3/3 + 1] - [(-1)^3/3 + (-1)])
= 3/8 ([1/3] + 1] - [-1/3 - 1])
= 3/8 ([4/3 - (-4/3)])
= 3/8 * 8/3
= 1
statement 3 is true, since statements 1 and 2 are true f(x) is a valid probability density function

determining the value of an unknown constant given a pdf
solve for k given the following function is a probability density function
f(x) = {
	2x^3 + kx^2,  0 < x < 1
	0,            otherwise
}
for a function f(x) to be a valid probability density function on a set S it must satisfy the following conditions
f(x) >= 0 for all x ∈ S
∫_S f(x) dx = 1
for the given function the second condition states
∫^1_0 (2x^3 + kx^2) dx = 1
= [x^4/2 + kx^3/3]|_0-1
= [1/2 + k/3] - 0
= k/3 + 1/2
k/3 + 1/2 = 1  =>  k = 3/2

example:
solve for k given the following function is a probability density function
f(x) = {
	k/x^2,  3 <= x <= 9
	0,      otherwise
}
∫_S f(x) dx = 1
∫^9_3 k/x^2 dx = 1
= k [x^-2]|_3-9
= k [-1/x]|_3-9
= k (-1/9) - (-1/3)
= k (-1/9) - (-3/9)
= k 2/9
2k/9 = 1  =>  k = 9/2

example:
solve for k given the following function is a probability density function
f(x) = {
	x^2/4 + k,  0 < x < 2
	0,          otherwise
}
∫_S f(x) dx = 1
∫^2_0 [x^2/4 + k] dx
= [x^3/12 + kx]|_0-2
= [8/12 + 2k] - 0
= 2/3 + 2k
= 2/3 + 2k = 1  =>  k = 1/6

===================================================

Probability that a random variable lies within a bounded interval
compute P(1 < X < 3) given that the random variable X has the probability density function
f(x) = {
	1/18(x^2 - 1), 1 <= x <= 4
	0,             otherwise
}
if a continuous random variable X has the probability density function f(x)
P(a < X < b) = ∫^b_a f(x) dx
∫^3_1 f(x) dx
= ∫^3_1 1/18(x^2 - 1) dx
= 1/18 ∫^3_1 (x^2 - 1) dx
= 1/18[x^3/3 - x]|_1-3
= 1/18([(3)^3/3 - (3)] - [(1)^3/3 - (1)])
= 1/18([9 - 3] - [1/3 - 1])
= 1/18(6 + 2/3)
= 1/18 * 20/3
= 20/54 = 10/27

example:
compute P(2 < X < 8) given that the random variable X has the probability density function
f(x) = {
	1/50(x), 0 <= x <= 10
	0,       otherwise
}
∫^8_2 1/50 [x^2/2] dx
= 1/50 [x^2/2]|_2-8
= 1/50 (64/2) - (4/2)
= 30/50
= 3/5

example:
compute P(1 <= X < 2) given that the random variable X has the probability density function
f(x) = {
	3/32(x - 1)(5 - x), 1 <= x <= 5
	0,                  otherwise
}
∫^2_1 3/32(x - 1)(5 - x) dx
= 3/32 ∫^2_1 (x - 1)(5 - x) dx
= 3/32[(x - 1)(5 - x)]|_1-2
(x - 1)(5 - x)
= 6x - x^2 - 5 + x
= 3/32[(-x^2 + 6x - 5)]|_1-2
= 3/32[-x^3/3 + 3x^2 - 5x]|_1-2
= 3/32[-2^2/3 + 3(2)^2 - 5(2)] - [-1^3/3 + 3(1)^2 - 5(1)]
= 3/32([-8/3 + 12 - 10] - [-1/3 + 3 - 5])
= 3/32([-2/3] - [-7/3])
= 3/32 * 5/3
= 5/32

Computing the probability that a random variable lies with an unbounded interval
compute P(X <= 3) given that the random variable X has the probability density function
f(x) = {
	2x/45,   2 <= x <= 7
	0,       otherwise
}
if a continuous random variable X has the probability density function f(x)
P(a <= X <= b) = ∫^b_a f(x) dx
f(x) is identical to zero for all x < 2
P(X <= 3) = ∫^3_2 f(x) dx
= ∫^3_2 2x/45 dx
= [1/45(x^2)]|_2-3
= 1/45(3^2 - 2^2)
= 1/45 * 5
= 1/9

example:
compute P(X < 2) given that the random variable X has the probability density function
f(x) = {
	1/18(x^2),  -3 <= x <= 3
	0,          otherwise
}
∫^2_-3 = 1/18(x^2) dx
= [1/54(x^3)]|_-3-2
= 1/54(2^3 - (-3)^3)
= 1/54(8 + 27)
= 35/54

example:
compute P(X >= 0) given that the random variable X has the probability density function
f(x) = {
	2/9(x + 2)(1 - x),  -2 <= x <= 1
	0,                  otherwise
}
∫^1_0 2/9(x + 2)(1 - x) dx
= 2/9 ∫^1_0 (x + 2)(1 - x) dx
= 2/9 ∫^1_0 (-x^2 - x + 2) dx
= 2/9 [-x^3/3 - x^2/2 + 2x]|_0-1
= 2/9 [-1^3/3 - 1^2/2 + 2(1)] - [(0)]
= 2/9 (-1/3 - 1/2 + 2)
= 2/9 * 7/6
= 7/27

calculating probabilities with piecewise PDFs
compute P(2 <= X <= 4) given that the random variable X has the probability density function
f(x) = {
	1/9(x),  0 <= x < 3
	x - 3,   3 <= x <= 4
	0,       otherwise
}
if a continuous random variable X has the probabillity density function f(x) then
P(a <= X <= b) = ∫^b_a f(x) dx
P(2 <= X <= 4) = ∫^4_2 f(x) dx
= ∫^3_2 1/9(x) + ∫^4_3 (x - 3) dx
= [x^2/18]|_2-3 + [x^2/2 - 3x]|_3-4
= 1/18[3^2 - 2^2] + ([4^2/2 - 3(4)] - [3^2/2 - 3(3)])
= 5/18 - 4 + 9/2
= 14/18
= 7/9

example:
compute P(0 < X < 1) given that the random variable X has the probability density function
f(x) = {
	x^2, 0 <= x <= 1
	2/3, 1 < x <= 2
	0,   otherwise 
}
∫^1_0 = x^2 dx + ∫^2_1 2/3 dx
= [x^3/3]|_0-1 + [2/3x]|_1-2
= (1/3) - (0) + 2/3
= 1
(got this wrong, we only needed one interval: ∫^1_0 = x^2 dx)
= 1/3

example:
compute P(0 < X < 2) given that the random variable X has the probability density function
f(x) = {
	1 - x^2, 0 <= x <= 1,
	1/12(x), 1 <= x <= 3,
	0,       otherwise
}
∫^1_0 1 - x^2 dx + ∫^2_1 1/12(x) dx
[x - x^3/3]|_0-1 + [x^2/24]|_1-2
[1 - 1/3 - (0)] + [4/24 - 1/24]
2/3 + 3/24
16/24 + 3/24
= 19/24

===================================================

NOTE: the negative from integrating comes from the upper bound ∞ minus the lower bound.

Continuous random variables over the infinite domains
it is common for the support S of a continuous random variable X to be an infinite interval. We often need to evaluate improper integrals when determining whether a function is a validpdf and to compute probabilites associated with such a random variable.
f(x) = {
	e^-x, x > 0
	0,    otherwise
}
Is f(x) a valid probability density function?
when drawing the graph we can clearly see that f(x) >= 0 for all x
to check if f(x) satisfies second condition we need to compute the integral
∫_S f(x) dx = ∫^infinity_0 e^-x dx
= [-e^-x]|_0-infinity
= [-e^-infinity] - [-e^0]
= 0 - (-1)
= 1
f(x) satisfies both conditions, we conclude that f(x) is a valid probability density function

checking if a given function is a valid pdf
f(x) = {
	1/(x - 2)^2, x >= 3
	0,           otherwise
}
which are the statements that are true?
1. f(x) >= 0 for all x >= 3
2. ∫^infinity_3 f(x) dx = 1
3. f(x) is a valid probability density function
For a function f(x) to be a valid probability density function on a set S, it must satisfy the following conditions
f(x) >= 0 for all ∈ S
∫_S f(x) dx = 1
statement 1 is true, x >= 3, f(x) = 1/(x - 2)^2 >= 0
statement 2 is true
∫^∞_3 f(x) dx = ∫^∞_3 1/(x - 2)^2 dx
= ∫^∞_3 (x - 2)^-2 dx
= [-1/(x - 2)]|_3-∞
= - (0 - 1)
= 1
statment 3 is ture, since 1 and 2 are true, f(x) is a valid probability density function

example:
given f(x) which of the following is true?
f(x) = {
	2/x^3, x >= 1
	0,     otherwise
}
1. f(x) >= 0 for all x >= 1
2. ∫^∞_1 f(x) dx = 1
3. f(x) is a valid probability density function
statement 1 is true, x >= 1, f(x) >= 0
∫^∞_1 2/x^3 dx
= ∫^∞_1 2x^-3 dx
= ∫^∞_1 2x^-2 dx
= [-1/x^2]|_1-∞
= -(-1/1)
= 1
all three are true statements

example:
given the function f(x) which statements are true
f(x) = {
	e^(1-x), x >= 1,
	0,       otherwise
}
1. f(x) >= 0 for all x >= 1
2. ∫^∞_1 f(x) dx = 1
3. f(x) is valid probability density function
∫^∞_1 e^(1-x) dx
= [-e^(1 - x)]|_1-∞
= -(-e^0)
= 1
all three statements are true

solving for a constant given a PDF
solve for k given that the following function is a probability density function
f(x) = {
	k/(4x + 1)^3, x >= 0
	0,            otherwise
}
A function f(x) defined on a set S to be a valid probability density function, it must satisfy the following conditions:
f(x) >= 0 for all x ∈ S
∫_S f(x) dx = 1
f(x) = k/(4x + 1)^3 on the interval x >= 0 the second condition states that ∫^infinity_0 k/(4x + 1)^3 dx = 1
computing the integral on the left-hand side we get
∫^∞_0 k/(4x + 1)^3 dx
= -[k/8(4x - 1)^2]|_0-∞
= k/8
= k/8 = 1  =>  k = 8
(to get from k/(4x + 1)^3 to k/8(4x - 1)^2 using substitution)
(u = 4x + 1, du = 4dx, dx = du/4)
(k/(4x + 1)^3 dx = k/u^3 * du/4 = k/4 ∫ u^-3 du)
(∫ u^-3 du = u^-2/-2 = -1/2u^2)
(k/4(-1/2u^2) = -k/8u^2 = -k/8(4x - 1)^2)

example:
Solve for k given that the following function is a probability density function
f(x) = {
	k/x^2, x >= 4,
	0,     otherwise
}
∫^∞_4 k/x^2 dx
= -[-k/x]|_4-∞
k/4 = 1
k = 4 

Solve for k given the function is a probability density function
f(x) = {
	ke^(-3x), x >= 0
	0,        otherwise
}
∫^∞_0 ke^(-3x) dx
= k * [-1/3(e^(-3x))]|_0-∞
= [-k/3(e^-3x)]|_0-∞
= -(-k/3)
= k/3
k = 3

Computing a probability over an unbounded interval
compute P(X > 8) given that the random variable X has the probability density function 
f(x) = {
	5e^(-5x), x >= 0
	0,        otherwise
}
P(X > 8) = P(8 < X < ∞)
= ∫^∞_8 f(x) dx
= ∫^∞_8 5e^(-5x) dx
= [-e^(-5x)]|_8-∞
= e^(-40)

example:
compute P(X >= 5) given that the random variable X has the probability density function:
f(x) = {
	8e^(-8x), x >= 0
	0, 		  x < 0
}
P(X >= 5) = P(5 <= X < ∞)
∫^∞_5 8e^(-8x) dx
= [-e^(-8x)]|_5-∞
= e^(-40)

Compute P(X > 8) given that the random cariable X has the probability density function
f(x) = {
	3/x^2, x >= 3
	0,     x < 3
}
P(X > 8) = P(8 < X < ∞)
∫^∞_8 3/x^2 dx
= [-3/x]|_8-∞
= -(-3/8)
= 3/8

===================================================

The cumulative distribution function (CDF) of a continuous random variable is defined the same way as for a discrete random variable
given a continuous random variable X, the CDF is the function F(x)
F(x) = P(X <= x)
if our random variable has the probability density function (PDF) f(x)
F(x) = P(X <= x) = ∫^x_-∞ f(t) dt
for example, consider the random variable X with the following PDF:
f(x) = {
	2/pi(1 + x^2), x ∈ [0, ∞)
	0,			   otherwise
}
to compute the CDF we integrate f(x):
F(x) = ∫^x_-∞ f(t) dt
∫^x_0 2/pi(1 + t^2) dt
= 2/pi arctan(t)|_0-x
= 2/pi arctan(x) - 2/pi(0)
= 2/pi arctan(x)
expressing our CDF as a function defined over the entire real line
F(x) = {
	0, 			    x < 0
	2/pi arctan(x), x >= 0
}

finding a CDF given a PDF with a lower bound
given that the random variable X has the probability density function
f(x) = {
	e^-x, x >= 0
	0,    x < 0
}
find the cumulative distribution function F(x)
Since our random variable X has a probability distribution function f(x) that is strictly positive only for x >= 0, the cumulative distribution function takes the following form:
F(x) = P(X <= x) = {
	0,    		    x < 0
	∫^x_0 f(t) dt,  x >= 0
}
for the given probability density function
∫^x_0 f(t) dt = ∫^x_0 e^-t td
= -e^-t|_0-x
= -e^-x + 1
= 1 - e^(-x)
so the cumulative distribution function is given by
F(x) = {
	0, 			x < 0
	1 - e^(-x), x >= 0
}

example:
given that the random variable X has the probability density function
f(x) = {
	1/x^2, x >= 1
	0,     x < 1
}
the cumulative distribution function is
F(x) = {
	0,   x < 1
	h(x) x >= 1
}
find the function h(x)
random variable X has a probability distribution function f(x) that is strictly positive only for x >= 1, the cumulative distribution function takes the following form:
F(x) = P(X <= x) = {
	0,             x < 1
	∫^x_1 f(t) dt  x >= 1
}
the given probability density function
∫^x_1 f(t) dt = ∫^x_1 1/t^2 dt
= -1/t|_1-x
= -1/x - (-1/1)
= -1/x + 1
= 1 - 1/x
so the cumulative function is given by
F(x) = {
	0,       x < 1
	1 - 1/x, x>= 1
}

example:
given that the random variable X has the probability density function
f(x) = {
	6e^(-6x), x >= 0
	0,        x < 0
}
its cumulative distribution function is
F(x) = {
	0,    x < 0
	h(x), x >= 0
}
∫^x_0 [6e^(-6x)] dx
= [-e^(-6x)]|_0-x
= -e^(-6x) + 1
= 1 - e^(-6x)
h(x) = 1 - e^(-6x) 

computing a CDF given a PDF thats nonzero over all real numbers
given that the random variable X has the probability density function
f(x) = e^-|2x|, x ∈ (-∞, ∞)
find the cumulative distribution function F(x)
Since our random variable X has a probability distribution function f(x) that involves an absolute value, the cumulative distribution function takes the following form:
F(x) = P(X <= x) = {
	∫^x_-∞ e^(2t) dt, 					 x < 0
	∫^0_-∞ e^(2t) dt + ∫^x_0 e^(-2t) dt, x >= 0
}
computing the integrals
if x <= 0
∫^x_-∞ e^(2t) dt
= [e(2t)/2]|_-∞-x
= e(2x)/2
therefore
∫^0_-∞ e^(2t) dt
= e^(2x)/2|_x=0
= 1/2
if x > 0
∫^x_0 e^(-2t) dt
= [-e^(-2t)/2]|_0-x
= -e^(-2x)/2 + 1/2
as a result
∫^0_-∞ e^(2t) dt + ∫^x_0 e^(-2t) dt
= 1/2 + (-e^(-2x)/2 + 1/2)
= 1 - e^(-2x)/2
the cumulative distribution
F(x) = {
	e^2x/2,        x < 0
	1 - e^(-2x)/2, x >= 0
}

example:
given that the random variable X has the probability density function
f(x) = e^(-|x/2|)/4, x ∈ (-∞, ∞)
the cumulative distribution function is
F(x) = {
	e^(x/2)/2,  x < 0
	h(x),       x >= 0
}
F(x) = P(X <= x) = {
	∫^x_-∞ e^(t/2)/4 dt, x < 0
	∫^0_-∞ e^(t/2)/4 dt + ∫^x_0 e^(-t/2)/4, x >= 0
}
if x <= 0
∫^x_-∞ e^(t/2)/4 dt
= [e^(t/2)/2]|_-∞-x
= e^(x/2)/2
therefore x = 0
∫^0_-∞ e^(t/2)/4 dt
= [e^(t/2)/2]|_x=0
= 1/2
if x > 0
∫^x_0 e^(-t/2)/4 dt
= [-e^(-t/2)/2]|_0-x
= -e^(-x/2)/2 + 1/2
∫^0_-∞ e^(t/2)/4 dt + ∫^x_0 e^(-t/2)/4
= 1/2 + (-e^(-x/2)/2 + 1/2)
= 1 - e^(-x/2)/2
cumulative distribution
F(x) {
	e^(-x/2)/2,     x < 0
	1 - e^(-x/2)/2, x >= 0
}

example:
given that the random variable X has the probability density function
f(x) = e^x/(1 + e^x)^2 x ∈ (-∞, ∞)
cumulative distribution function of a continuous random variable X
F(x) = P(X <= x) = ∫^x_-∞ f(t) dt
for the given probability density function
F(x) = ∫^x_-∞ f(t) dt
= ∫^x_-∞ e^t/(1 + e^t)^2
= [-1/1+e^t]|_-∞-x
= -1/1+e^x + 1
= 1 - 1/1 + e^x
= e^x/1 + e^x

finding a CDF given a PDF with lower and upper bounds
when a random variable X has a probability distribution function f(x) that is strictly positive on the interval a <= x <= b only
P(X <= x) = 0 for x < a
P(X <= x) = 1 for x > b
the cumulative distribution function in these cases
F(x) = P(X <= x) = {
	0,   		   x < a
	∫^x_a f(t) dt, a <= x <= b
	1, 			   x > b
}

probability density function
f(x) = {
	1/72(x), 0 <= x <= 12
	0,       otherwise
}
find the cumulative distribution function F(x)
random variable X has a probability distribution function f(x) that is strickly positive only for 0 <= x <= 12 the cumulative distribution function:
F(x) = P(X <= x) = {
	0, 			   x < 0
	∫^x_0 f(t) dt, 0 <= x <= 12
	1, 			   x > 12
}
probability density function
∫^x_0 f(t) dt
= ∫^x_0 1/72t dt
= [1/144(t^2)]|_0-x
= 1/144(x^2)
cumulative distribution function
F(x) = {
	0,       x < 0
	x^2/144, 0 <= x <= 12
	1, 		 x > 12
}

example:
probability density function
f(x) = {
	2/21(x),  2 <= x <= 5
	0,        otherwise
}
cumulative distribution function
F(x) = {
	0, 	  x < 2
	h(x), 2 <= x <= 5
	1,    x > 5
}
∫^x_2 2/21(t) dt
= [1/21(t^2)]|_2-x
= 1/21(x^2) - 4/21
= (x^2 - 4)/21
cumulative distribution function
F(x) = {
	0, 			  x < 2
	(x^2 - 4)/21, 2 <= x <= 5
	1, 			  x > 5
}

example:
probability density function
f(x) = {
	2/33(2 + x), 2 <= x <= 5
	0,           otherwise
}
cumulative distribution function
F(x) = {
	0,    x < 2
	h(x), 2 <= x <= 5
	1,    x > 5
}
∫^x_2 2/33(2 + x) dt
= [2/33(2 + x)]|_2-x
= (4x + x^2)/33 - 12/33
(x^2 + 4x - 12)
(x - 2)(x + 6)/33
F(x) {
	0, 				   x < 2
	(x - 2)(x + 6)/33, 2 <= x <= 5
	1, 				   x > 5
}

Using a CDF to compute a probability over an interval
As with discrete random variables, we can use the CDF of a continuous random variable to quickly compute a probability over an interval:
P(a <= X <= b) = F(b) - F(a)
For example, suppose that a random variable X has the following CDF
F(x) = 1/(1 + e^-x), -∞ < x < ∞
we can use the above CDF to quickly compute P(0 <= x <= 1)
= F(1) - F(0)
= 1/(1 + e^-1) - 1/(1 + e^0)
= 1/(1 + e^-1) - 1/2
~= 0.231
For continuous random variables it doesn't matter whether or not we include the endpoints of the interval so we have
P(0 <= X <= 1) = P(0 < X <= 1) = P(0 <= X < 1) = P(0 < X < 1) = 0.231
in general for a continuous randon variable X
P(a <= X <= b) = P(a < X <= b) = P(a <= X < b) = P(a < X < b) = F(b) - F(a)

NOTE: 
When computing a probability over an interval using a CDF
If both endpoints are non zero probabilities, they must be included in the calculations
(First example exludes F(2) in the final results)
(Last example includes F(1) in the final results)

Computing a probability over an interval using a CDF
F(x) = {
	0, 			   x < 5
	(x^2 - 25)/75, 5 <= x <= 10
	1, 			   x > 10
}
find P(2 <= X <= 6)
For continuous random variables it doesn't matter whether or not we include the endpoints of the interval. So we can perform this computation as follows
P(2 <= X <= 6)
= P(X <= 6) - P(X <= 2)
= F(6) - F(2)
= (6^2 - 25)/75 - 0
= 11/75

example:
given that the random variable X has the cumulative distribution function
F(x) = {
	0,        x < 0
	1 - e^-x, x >= 0
}
find P(X > 2)
= 1 - P(X <= 2)
= 1 - F(2)
= 1 - (1 - e^-2)
= e^(-2)
= 1/e^2

example:
the random variable X has the cumulative distribution function
F(x) = {
	0, 		x < 0
	x^2/25, 0 <= x <= 5
	1, 		x > 5
}
find P(1 <= X <= 3)
= P(X <= 3) - P(X <= 1)
= F(3) - F(1)
= 3^2/25 - 1^2/25
= 8/25

===================================================

Approximating Discrete random variables as continuous 
We often want to approximate a discrete random variable X by a continuous random variable Y. To do this we need to consider how to treat the values of Y that lie between the discrete values of X
Recall that if X = 2 to the nearest integer, then
1.5 <= X < 2.5
Consider the following scenario
X ∈ {..., 0,1,2,3 ...} is a discrete random variable with integer support
Y ∈ (-∞, ∞) is a continuous random variable that we will use to approximate X
To approximate the probability
P(X = 2)
using the random variable Y we need to calculate
P(1.5 <= Y < 2.5)
Also since Y is continuous, this probability is equal to
P(1.5 <= Y <= 2.5)
We can think of the required values of Y as lying within the interval contained within a circle of radius 0.5 centered at Y = 2
adjusting the values of a discrete random variable so that it can be approximated using a continuous random variable is called a continuity correction.

Approximating a Probability involving a discrete random variable at a single point
The discrete random variable X ∈ Z can be approximated by the continuous random variable Y. By applying an appropriate continuity correction, P(X = -1) can be approximated as P(a <= Y <= b) find the values of a and b.
We construct an interval around X = -1 with a radius of 0,5
P(X = -1) ~= P(-1 - 0.5 <= Y <= -1 + 0.5)
= P(-1.5 <= Y <= -0.5)
a = -1.5 and b = -0.5

example:
The discrete random variable X ∈ Z can be approximated by the continuous random variable Y by applying an appropriate continuity correct P(X = 3) can be approximated as
we construct an interval around X = 3 with a radius of 0.5
P(X = 3) ~= P(3 - 0.5 <= Y <= 3 + 0.5)
= P(2.5 <= Y <= 3.5)

example:
The discrete random variable X ∈ Z can be approximated by the continuous random variable Y by applying an appropriate continuity correction P(X = -4)
we construct an interval around X = -4 with a radius of 0.5
P(X = -4) ~= P(-4 - 0.5 <= Y <= -4 + 0.5)
= P(-4.5 <= Y <= -3.5)

Approximating a probability involving a discrete random variable over an interval
The discrete random variable X ∈ Z can be approximated by the continuous random variable Y by applying an appropriate continuity correction P(-1 < X < 2) can be approximated as P(a <= Y <= b) find the values of a and b
To apply the appropriate continuity correction we first rewrite the strict inequalities using non-strict inequalities
P(-1 < X < 2) = P(0 <= X <= 1)
we extend the endpoints of the interval 0 <= X <= 1 by 0.5
P(0 <= X <= 1) ~= P(0 - 0.5 <= Y <= 1 + 0.5)
= P(-0.5 <= Y <= 1.5)
a = -0.5, b = 1.5

example:
The discrete random variable X ∈ Z can be approximated by the continuous random variable Y by applying an appropiate continuity correction P(3 <= X <= 5) can be approximated
P(3 <= X <= 5) ~= P(3 - 0.5 <= Y <= 5 + 0.5)
= P(2.5 <= Y <= 5.5)

example:
P(1 < X < 5)
= P(2 <= X <= 4)
~= P(2 - 0.5 <= Y <= 4 + 0.5)
= P(1.5 <= Y <= 4.5)

Approximating a probability involving a discrete random variable over an unbounded interval
the discrete random variable X ∈ Z can be approximated by the continuous random variable Y by applying an appropiate correction, P(X > 5) can be approximated as P(Y >= a) find the value of a
To apply the appropiate continuity correction, we first rewrite the strict inequalities using non-strict inequalities
P(X > 5) = P(X >= 6)
extend the endpoints of the interval X >= 6 by 0.5
P(X >= 6) ~= P(Y >= 6 - 0.5)
= P(Y >= 5.5)
a = 5.5

example:
the discrete variable X ∈ Z can be approximated by the continuous random variable Y by applying continuity correction P(X < 7) 
apply continuity correct
P(X < 7) = P(X <= 6)
P(X <= 6) ~= P(Y <= 6 + 0.5)
= P(Y <= 6.5)

example:
P(X > 2)
= P(X >= 3)
we extend the endpoints of the interval X >= 3 by 0.5
P(X >= 3) ~= P(Y >= 3 - 0.5)
= P(Y >= 2.5)

===================================================

Simulating random observations
Suppose we have a random variable with the cumulative distribution function
F(x) = 1/(1 + e^-x), x ∈ (-∞, ∞)
(the graph looks like a relu function midpoint is y = 0.5)
suppose we want to generate a sample size n = 3 from this distribution
first draw a random sample of size n = 3 from the uniform distribution Y ~ U[0,1]
then find the values xi such that yi = F(xi)
xi = F^-1(yi), i = 1,2,3
F(x) is strictly increasing and therefore F^-1(y) exists for all y ∈ (0,1) moreover we set F^-1(0) = 0 and F^-1(1) = 1 to deal with the endpoints
Suppose that we run our simulation and generate n = 3 numbers from Y ~ U[0,1] and that our three numbers (rounded to 3 decimals places)
y1 = 0.214, y2 = 0.707, y3 = 0.819
to compute our xi's we first need to find the inverse of F(x) so we write y = F(x)
y = 1/(1 + e^-x)
solving for x
1 + e^-x = 1/y
e^-x = (1/y) - 1
e^-x = (1 - y)/y
-x = ln(1 - y/y)
x = -ln(1 - y/y)
x = ln(1 - y/y)^-1
x = ln(y/y - 1)
where 0 < y < 1
substituting the values y1,y2,y3 into the formula for x = F^-1(y)
x1 ~= -1.301, x2 ~= 0.881, x3 ~= 1.510
algorithm above can be formalized by the following theorem:
Let the random variable Y ~ U[0,1] be uniformly distributed. Suppose that the function F(x) is a cumulative distribution function, where F(a) = 0, F(b) = 1 and F is strictly monotonic on a < x < b where a and b could be infinite then the random variable X is given by
X = F^-1(Y)
is a continuous random variable with the cumulative distribution function F(x)

Simulating observations given a CDF
This random sample was drawn from the uniform distribution U[0,1]
0.352, 0.567, 0.801
use these numbers to generate three random numbers that follow a probability distribution with the CDF F(x) given by
F(x) = {
	0, 		 x < 1
	2 - 2/x, 1 <= x <= 2
	1, 		 x > 2
}
Let the random variable Y ~ U[0,1] be uniformly distributed suppose that the function F(x) is a cumulative distribution function where F(a) = 0, F(b) = 1 and F is strictly monotonic on a < x < b then the random variable X is given by
X = F^-1(Y)
is a continuous random variable with the cumulative distribution function F(x)
find the inverse of y = F(x) we solve the following equation for x taking into account that 1 <= x <= 2 and 0 < y < 1
y = 2 - 2/x
y - 2 = -2/x
2 - y = 2/x
x = 2/2-y
substitute drawn from U[0,1]
for y = 0.352, x = 2/2-0.352 ~= 1.214
for y = 0.567, x = 2/2-0.567 ~= 1.396
for y = 0.801, x = 2/2-0.801 ~= 1.668

example:
random sample was drawn from the uniform distribution U[0,1]
0.103, 0.534, 0.847
use these numbers to generate three random numbers that follow a probability distribution with the CDF F(x) given by
F(x) = arctan(x)/pi + 1/2, x ∈ (-∞, ∞)
y = arctan(x)/pi + 1/2
y - 1/2 = arctan(x)/pi
pi(y - 1/2) = arctan(x)
x = tan(pi(y - 1/2))
for y = 0.103, x = tan(pi(0.103 - 0.5)) ~= -2.982
for y = 0.534, x = tan(pi(0.534 - 0.5)) ~= 0.107
for y = 0.847, x = tan(pi(0.847 - 0.5)) ~= 1.918

example:
0.224, 0.389, 0.701
Use these numbers to generate three random numbers that follow a probability distribution with the CDF F(x)
F(x) = {
	0,  			    x < 2
	1 - (16/(2 + x)^2), x >= 2
}
y = 1 - (16/(2 + x)^2)
y - 1 = -16/(2 + x)^2
(1 - y)(2 + x)^2 = 16
(2 + x) = sqrt(16/(1 - y))
x = -2 +/- sqrt(16/(1 - y))
x = -2 +/- 4/sqrt(1 - y)
y = 0.224, x = 4/sqrt(1 - (0.224)) - 2 ~= 2.541
y = 0.389, x = 4/sqrt(1 - (0.389)) - 2 ~= 3.117
y = 0.701, x = 4/sqrt(1 - (0.701)) - 2 ~= 5.315

simulating observations given a PDF
random sample was drawn from the uniform distribution U[0,1]
0.354, 0.593, 0.792
use these numbers to generate three random numbers that follow a probability distribution with the PDF f(x) given by
f(x) = {
	cos(x), 0 <= x <= pi/2
	0,      otherwise
}
Let the random variable Y ~ U[0,1] be uniformly distributed. suppose F(x) is a cumulative distribution function where F(a) = 0, F(b) = 1 and F is strictly monotonic on a < x < b
X = F^-1(Y)
is continuous random variable with the cumulative distribution function F(x)
We must find the cumulative distribution function F(x) corresponding to our random variable. Since our PDF f(x) is non-negative only for 0 <= x <= pi/2
F(x) = {
	0, 			   x < 0
	∫^x_0 f(t) dt, 0 <= x <= pi/2
	1, 			   x > pi/2
}
evaluating the integral
∫^x_0 f(t) dt = ∫^x_0 cos(t) dt
= [sin(t)]|_0-pi
= sin(x)
the cumulative distribution
F(x) = {
	0,      x < 0
	sin(x), 0 <= x <= pi/2
	1,      x > pi/2
}
finding the inverse of y = F(x) so we solve the following equation for x taking into account that 0 <= x <= pi/2 and 0 < y < 1
y = sin(x)
x = arcsin(y)
substitute the numbers drawn from U[0,1] into the equation
y = 0.354, x = arcsin(0.354) ~= 0.362
y = 0.593, x = arcsin(0.593) ~= 0.635
y = 0.792, x = arcsin(0.792) ~= 0.914

example:
random numbers drawn from a unform distribution
0.231, 0.412, 0.602
use these numbers to generate three random numbers that follow a probability distribution with the PDF f(x)
f(x) = {
	x^2/72, 0 <= x <= 6
	0,      otherwise
}
CDF:
F(x) = {
	0, 			   x < 0
	∫^x_0 f(t) dt, 0 <= x <= 6
	1,             x > 6
}
∫^x_0 [x^2/72]|_0-6
= x^3/216
inverse of y = F(x)
y = x^3/216
216y = x^3
x = 6 * ∛y
substitute
y = 0.231, x = 6∛(0.231) ~= 3.681
y = 0.412, x = 6∛(0.412) ~= 4.465
y = 0.602, x = 6∛(0.602) ~= 5.066

example:
0.241, 0.532, 0.824
PDF f(x)
f(x) = {
	sin(x), 0 <= x <= pi/2
	0,      otherwise
}
CDF:
F(x) = {
	0,             x < 0
	∫^x_0 f(t) dt, 0 <= x <= pi/2
	1, 			   x > pi/2
}
CDF:
F(x) = {
	0,          x < 0
	1 - cos(x), 0 <= x <= pi/2
	1, 			x > 4
}
∫^x_0 [sin(x)]|_0-pi/2
= 1 - cos(x)
inverse of y = F(x)
y = 1 - cos(x)
solve for x
x = cos^-1(1 - y)
y = 0.241, x = arccos(1 - 0.241) = 0.709
y = 0.532, x = arccos(1 - 0.532) = 1.084
y = 0.824, x = arccos(1 - 0.824) = 1.394

===================================================

One-to-One transformations of discrete random variables
and Many-to-One tranformations of discrete random variables
(no notes needed for this section, simple algebra operations)

===================================================

NOTE:
Use 1 - F_X when calculating the probability of an event where you're considering the compliment (like X being greater than a certain value)
Use F_X when you are directly looking for the probabililty of X being less than or equal to a value
(it depends on whether you're working with CDF or their compliments)

The Distribution function method
to find the PDF of Y, we can use the distributed function method, and it consists of two steps
first, we find an expression for F_Y in terms of F_X where F_Y and F_X are the cumulative distribution functions of Y and X
then we differentiate F_Y to get the probability density function f_Y of Y

A worked example
a continuous random variable X is defined
f_X(x) = {
	e^-x, x > 0
	0,    otherwise
}
Y = 1 + 2X we can use the distribution function to find the probability density function of Y
Y = 1 + 2X is a strictly increasing function of X. Moreover for x > 0 we have y > 1
lets consider y > 1 we write down the definition of the cumulative distribution of Y and then use our transformation to isolate X in the parentheses, this will allow us to write F_Y in terms of F_X
F_Y(y) = P(Y <= y)
= P(2X + 1 <= y)
= P(2X <= y - 1)
= P(X <= (y - 1)/2)
= F_X((y - 1)/2)
compute the PDF f_Y(y) for y > 1 we differentiate F_Y with respect to y using the chain rule
f_Y(y) = F'_Y(y)
= d/dy(F_X((y - 1)/2))
= d/dy((y - 1)/2) * F'_X((y - 1)/2)
= 1/2 * F'_X((y - 1)/2)
= 1/2 * f_X((y - 1)/2)
applying the definition of f_X
f_Y(y) = 1/2 * f_X((y - 1)/2)
= 1/2 * e^-(y - 1)/2
= 1/2(e^(1-y)/2)
the full expression for the PDF of Y is
f_Y(y) = {
	1/2(e^(1-y)/2), y > 1
	0,              otherwise
}
sometimes its worthwild to check this is a valid PDF

Computing the PDF of a random variable under an affine transformation
Let X be a continuous random variable with the following probability density function
f_X(x) = {
	2x, 0 < x < 1
	0,  otherwise
}
if Y = 1 - 3X then what is the probability density function of Y
to compute the probability density function of a random variable Y where Y = u(X) is a continuous strictly monotonic function of a continuous random variable X
Find an expression for F_Y terms of F_X where F_Y and F_X are cumulative distribution functions of Y and X respectively
then we differentiate F_Y to get the probability density function f_Y
Y = 1 - 3X is a stictly decreasing function of X for 0 < x < 1 we have -2 < y < 1
using the definition of the cumulative distribution of Y for -2 < y < 1
F_Y(y) = P(Y <= y)
= P(1 - 3X <= y)
= P(-3X <= y - 1)
= P(X >= 1 - y/3)
= 1 - F_X(1 - y/3)
to compute the PDF f_Y(y) for -2 < y < 1 we differentiate F_Y with respect to y using the chain rule
f_Y(y) = F'_Y(y)
= 0 - (-1/3) * F'_X(1 - y/3)
= 1/3 * f_X(1 - y/3)
applying the definition of f_X
f_Y(y) = 1/3 * f_X(1 - y/3)
= 1/3 * 2 * (1 - y/3)
= 2/9(1 - y)
PDF
f_Y(y) = {
	2/9(1 - y), -2 < y < 1
	0,           otherwise
}

example:
X is a continuous random variable with the PDF
f_X(x) = {
	4e^(-4x), x > 0
	0,        otherwise
}
if Y = 2X then what is the probability density function of Y
F_Y(y) = P(Y <= y)
= P(2X <= y)
= P(X <= y/2)
F_X(y/2)
to compute the PDF f_Y(y) for y > 0 we differentiate F_Y with respect to y using the chain rule
f_Y(y) = F'_Y(y)
= 1/2 * F'_x(y/2)
= 1/2 * f_X(y/2)
apply the definition of f_X
f_Y(y) = 1/2 * f_X(y/2)
= 1/2 * 4e^(-4*y/2)
= 2e^(-2y)
full expression PDF
f_Y(y) = {
	2e^(-2y), y > 0
	0,        otherwise
}

example:
PDF:
f_X(x) = {
	3e^(-3x), x > 0
	0,        otherwise
}
if Y = 1 - 2X then what is the probability density function of Y?
= P(1 - 2X <= y)
= P(X >= 1 - y/2)
= 1 - F_X(1 - y/2)
chain rule
f_Y(y) = F'_Y(y)
= 0 - (-1/2) * F'_X(1 - y/2)
= 1/2 * f_X(1 - y/2)
applying f_X
f_Y(y) = 1/2 * f_X(1 - y/2)
= 1/2 * 3e^(-3(x))
= 1/2 * 3e^(-3(1 - y/2))
= 3/2(e^(-3(1 - y/2)))
full PDF expression
f_Y(y) = {
	3/2(e^(-3(1 - y/2))), y <= 1
	0,                    otherwise
}

computing the PDF of a random variable under a nonlinear transformation
Let X be a continuous random variable with the following probability density function
f_X(x) = {
	3/8(2 - x)^2, 0 < x < 2
	0,            otherwise
}
if Y = (2 - X)^3 what is the probility density function of Y
First find an expression for F_Y in terms of F_X where F_Y and F_X are the cumulative distribution functions of Y and X.
Next we differentiate F_Y to get the probabillity density function f_Y
Y = (2 - X)^3 is a strictly decreasing function of X for 0 < x < 2 we have 0 < y < 8
cumulative distribution of Y for 0 < y < 8
f_Y(y) = P(Y <= y)
= P((2 - X)^3 <= y)
= P(2 - X <= y^(1/3))
= P(-X <= y^(1/3) - 2)
= P(X >= 2 - y^(1/3))
= 1 - F_X(2 - y^(1/3))
compute PDF f_Y(y) for 0 < y < 8 respect to y chain rule
f_Y(y) = F'_Y(y)
= 0 - (-1/3y^(2/3)) * F'_X(2 - y^(1/3))
= 1/3y^(2/3) * F'_X(2 - y^(1/3))
applying the definition of f_X (substitute)
f_Y(y) = 1/3y^(2/3) * f_X(2 - y^(1/3))
= 1/3y^(2/3) * 3/8(2 - (2 - y^(1/3)))^2
= 1/3y^(2/3) * 3/8(2 - 2 + y^(1/3))^2
= 1/3y^(2/3) * 3/8(y^(1/3))^2
= 1/8
full PDF for Y
f_Y(y) = {
	1/8, 0 < y < 8
	0,   otherwise
}

example: 
probability density function
f_X(x) = {
	3x^2, 0 < x < 1
	0,    otherwise
}
Y = X^3 whats PDF of Y
= P(X^3 <= y)
= P(X <= y^(1/3))
= F_X(y^(1/3))
chain rule
f_Y(y) = F'_Y(y)
= 1/3y^(2/3) * F'_X(y^(1/3))
applying the definition of f_X (substitute)
= 1/3y^(-2/3) * f_X(y^(1/3))
= 1/3y^(-2/3) * f_X(y^(1/3))
definition of f_X
f_Y(y) = 1/3y^(-2/3) * f_X(y^(1/3))
= 1/3y^(-2/3) * 3(y^(1/3))^2
= 1/3y^(-2/3) * 3y^(2/3))
= 1

example:
PDF:
f_X(x) = {
	e^(-x), x > 0
	0,      otherwise
}
Y = ∛1 - X, PDF of Y?
= P(-X <= y^3 - 1)
= P(X >= 1 - y^3)
= 1 - F_X(1 - y^3)
chain rule
= 0 - (-3y^2) * F'_X(1 - y^3)
= 3y^2 * f_X(1 - y^3)
definition of f_X
f_Y(y) = 3y^2 * f_X(1 - y^3)
= 3y^2 * e^-(1 - y^3)
= 3y^2e^(y^3 - 1)
full expression for the PDF of Y
f_Y(y) = {
	3y^2e^(y^3 - 1), y < 1
	0,               otherwise
}
(arrived at y < 1, simply by graphing Y = ∛1 - X on desmos)

===================================================

The change-of-variables method for continuous random variables
It's possible to generalize the distribution function method when u(X) is strictly monotonic. This generalization is known as the change-of-cariable method

theorem:
If X ∈ (a,b) is a continuous random variable with probability density function f_X and Y = u(X) where the function u(x) is strictly monotonic then according to the change-of-variables method the probability density function of Y
f_Y(y) = {
	f_X(u^-1(y)) * |(u^-1)`(y)|, y is between u(a) and u(b)
	0,    						 otherwise
}
The advantage of the change-of-variables method is that its often easier and faster to implement than the distribution function method. The disadvantage is that it only works when u is monotonic whereas the distribution function method can be extended to deal with non-monotonic functions.

X has the probability density function
f_X(x) = {
	1/2(x), 0 < x < 2
	0,      otherwise
}
let the random variable Y = 3X then u(x) = 3x which is strictly monotonically increasing, the change-of-variables method can be used to compute the PDF of Y
first find u^-1(y)
u(y) = 3y
y = 3u^-1(y)
u^-1(y) = 1/3(y)
then find (u^-1)`(y)
u^-1(y) = 1/3(y)  =>  (u^-1)`(y) = 1/3
for non-zero f_Y
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(1/3(y)) * |1/3|
= 1/2(1/3(y)) * 1/3
= 1/18(y)
since f_X is nonzero on 0 < x < 2 the bounds are a = 0 and b = 2
u(0) = 0,  u(2) = 6
probability distribution of Y is
F_Y = {
	1/18(y), 0 < y < 6
	0,       otherwise
}

indentifying transformation where the change-of-variables method is applicable
X is a continuous random variable which of the transformations can the change-of-variables method be applied to find the probability density function of Y?
1. Y = X^4 where X ∈ (0,2)
2. Y = cos(X) where X ∈ (-∞, ∞)
3. Y = e^(-X) where X ∈ (0,∞)
Change-of-variables method can be applied when a transformation Y = u(X) is strictly increasing or stickly decreasing (i.e. strickly monotonic) over the entirity of X
Tranformation 1 is strictly monotonic, the function u(x) = x^4 is strictly increasing for x ∈ (0,2)
Tranformation 2 is NOT strictly monotonic, u(x) = cos(x) is decreasing on x ∈ (0, pi) and increasing on x ∈ (pi, 2pi).
Transformation 3 is strictly monotonic. The function u(x) = e^(-x) is strictly decreasing for x ∈ (0, ∞)
1 and 3 only.

example:
X is a continuous random variable, which of the transformations can the change-of-variables method be applied to find the probability density function of Y
1. Y = X^2, X ∈ (-1,1)
2. Y = 3 - 2X, X ∈ [2,5]
3. Y = e^X, X ∈ (-∞, ∞)
all transformations work except for 1 because u(x) = x^2 is decreasing at x ∈ (-1,0) and increasing at x ∈ (0,1)

example:
1. Y = X^2, X ∈ (0,5)
2. Y = sin(X), X ∈ (-∞, ∞)
3. Y = ln(X), X ∈ (0, ∞)
1 and 3 only. Transformation 2 is not the function u(x) = sin(x) sometimes increasing sometimes decreasing, it is increasing x ∈ (0, pi/2) and decreasing on x ∈ (pi/2, 3pi/2)

computing the PDF of a random variable under the affine transformation
X is a continuous random variable, its PDF:
f_X(x) = {
	1/156(x^3), 1 < x < 5
	0,          otherwise
}
if Y = 1 - X/4, use the change-of-variables method to find the probability density function of Y
the PDF function of Y
f_Y(y) = {
	f_X(u^-1(y)) * |(u^-1)`(y)|, y is between u(a) and u(b)
	0,                           otherwise
}
transformation
u(x) = 1 - x/4 is monotonically decreasing we can use change-of-variables
find u^-1(y)
u(y) = 1 - y/4
y = 1 - u^-1(y)/4
u^-1(y) = 1 - 4y
next find (u^-1)`(y):
(u^-1)`(y) = -4
change-of-variables method
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(1 - 4y) * |-4|
= 1/156(1 - 4y)^3 * 4
= 1/39(1 - 4y)^3
since f_X is nonzero on 1 < x < 5 the bounds are a = 1 and b = 5
u(1) = 0, u(5) = -1
the probability distribution of Y
f_Y(y) = {
	1/39(1 - 4y)^3, -1 < y < 0
	0,              otherwise
}

example:
X is a continuous random variable with the following probability density function
f_X(x) = {
	1/2(x), 0 < x < 2
	0,      otherwise
}
if Y = 1 - 2X, lets use the change-of-variables method to find the probability density function of Y
u(x) = 1 - 2X
find u^-1(y)
u(y) = 1 - 2y
y = 1 - 2u^-1(y)
u^-1(y) = 1 - y/2
finding (u^-1)`(y):
(u^-1)`(y) = -1/2
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(1 - y/2) * |-1/2|
= 1/2(1 - y/2) * 1/2
= 1/8(1 - y)
u(0) = 1, u(2) = -3
f_Y(y) = {
	1/8(1 - y), -3 < y < 1
	0,          otherwise
}

example:
f_X(x) = {
	2e^(-2x), x >= 0
	0,        otherwise
}
Y = -5X
u(x) = -5X
find u^-1(y)
u(y) = -5y
y = -5u^-1(y)
u^-1(y) = -y/5
finding (u^-1)`(y):
(u^-1)`(y) = -1/5
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(-y/5) * |-1/5|
= 2e^(-2(-y/5)) * 1/5
= 2e^(2y/5) * 1/5
= 2/5e^(2y/5)
u(0) = 0, u(x) = -∞
f_Y(y) = {
	2/5e^(2y/5), y <= 0
	0,           otherwise
}

computing the PDF of a random variable under a nonlinear transformation
f_X(x) = {
	3x^2, 0 < x < 1
	0,    otherwise
}
Y = sqrt(X), monotonically increasing
u(y) = sqrt(y)
y = sqrt(u^-1(y))
u^-1(y) = y^2
u^-1(y)`(y) = 2y
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(y^2) * |2y|
= 3(y^2)^2 * 2y
= 3y^4 * 2y
= 6y^5
u(0) = 0, u(1) = 1
f_Y(y) = {
	6y^5, 0 < y < 1
	0,    otherwise
}

example:
f_X(x) = {
	3/14(sqrt(x)), 1 < x < 4
	0,             otherwise
}
Y = ln(X)
u(y) = ln(y)
y = ln(u^-1(y))
u^-1(y) = e^y
u^-1(y)`(y) = e^y
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(e^y) * |e^y|
= 3/14(sqrt(e^y)) * e^y
= 3/14(e^(y/2)) * e^y
= 3/14(e^(y/2 + y))
= 3/14(e^(y/2 + 2y/2))
= 3/14(e^(3y/2))
u(1) = 0, u(4) = ln(4)
f_Y(y) = {
	3/14(e^(3y/2)), 0 < y < ln(4)
	0,              otherwise
}

example:
f_X(x) = {
	2ln(x)/x, 1 < x < e
	0,        otherwise
}
Y = 1/X
u(y) = 1/y
y = 1/u^-1(y)
u^-1(y) = 1/y
u^-1(y)`(y) = -1/y^2
f_Y(y) = f_X(u^-1(y)) * |(u^-1)`(y)|
= f_X(1/y) * |-1/y^2|
= 2ln(1/y)/(1/y) * 1/y^2
= ln(1/y) = ln(y^-1) = -ln(y) * 1/y^2
= 2(-ln(y))/1/y * 1/y^2
= 2(-ln(y)) * y * 1/y^2
= -2yln(y) * 1/y^2
= -2ln(y)/y
u(1) = 1, u(e) = e^-1

both proofs for change-of-variables
Case 1 u is monotonically increasing
Case 2 u is monotonically decreasing
well just see Case 1 as Case 2 is similar
first find the cumulative distribution of Y (assuming y is between u(a) and u(b))
F_Y(y) = P(Y <= y)
= P(u(X) <= y)
= P(X <= u^-1(y))
= ∫^(u^-1(y)_a) fx(t) dt
= F_X(t)|_a-u^-1(y)
= F_X(u^-1(y)) - F_X(a)
differentiate for the PDF
f_Y(y) d/dy[F_Y(y)]
f_Y(y) d/dy[F_X(u^-1(y)) - F_X(a)]
f_X(u^-1(y))(u^-1)`(y)
because u is monotonically increasing, u^-1 is monotonically increasing as well, its helpful to reflect a function over the line y = x to get its inverse
The slope of monotonically increasing function is always zero or postive we have (u^-1)`(y) >= 0 consequently (u^-1)`(y) = |(u^-1)`(y)|
f_Y(y) = f_X(u^-1(y))|(u^-1)`(y)|

===================================================

Distribution function method with many-to-one transformations
we can also use the distribution method in cases where u is not monotonic.

A worked example
Let X be a continuous random variable, PDF
f_X(x) = {
	3/16(x^2), 0 < |x| < 2
	0,         otherwise
}
Y = X^2 - 1
this is two-to-one over interval 0 < |x| < 2
for y values -1 < y < 3
we first write doen the definition of the cumulative distribution of Y and then use our transformation to isolate X in the parentheses. This will allow us to write F_Y in terms of F_X
F_Y(y) = P(Y <= y)
= P(X^2 - 1 <= y)
= P(X^2 <= y + 1)
= P(-(y + 1)^1/2 <= X <= (y + 1)^1/2)
= F_X((y + 1)^1/2) - F_X(-(y + 1)^1/2)
To compute the PDF f_Y(y) for -1 < y < 3 differentiate F_Y with respect to y using the chain rule
f_Y(y) = F'_Y(y)
= 1/2(y + 1)^-1/2 * F'_X((y + 1)^1/2) + 1/2(y + 1)^-1/2 * F'_X(-(y + 1)^1/2)
= 1/2(y + 1)^-1/2 * f_X((y + 1)^1/2) + 1/2(y + 1)^-1/2 * f_X(-(y + 1)^1/2)
= 1/2(y + 1)^-1/2(f_X((y + 1)^1/2) + f_X(-(y + 1)^1/2))
applying definition of f_X
f_Y(y) = 1/2(y + 1)^-1/2(f_X((y + 1)^1/2) + f_X(-(y + 1)^1/2))
= 1/2(y + 1)^-1/2(3/16((y + 1)^1/2)^2 + 3/16(-(y + 1)^1/2)^2)
= 1/2(y + 1)^-1/2(3/16(y + 1) + 3/16(y + 1))
= 3/16(y + 1)^-1/2(y + 1)
= 3/16(y + 1)^1/2
= 3/16(sqrt(y + 1))
PDF of Y
f_Y(y) = {
	3/16(sqrt(y + 1)), -1 < y < 3
	0,                  otherwise
}

Applying two-to-one transformations to random variables
let X be a continuous random variable with the PDF:
f_X(x) = {
	1/24(x + 1)^2, -3 < x < 3
	0,             otherwise
}
if Y = |X| whats the probability density function of Y?
over the interval -3 < x < 3
0 <= y < 3
definition of the cumulative distribution of Y for 0 <= y < 3
F_Y(y) = P(Y <= y)
= P(|X| <= y)
= P(-y <= X <= y)
= F_X(y) - F_X(-y)
chain rule
f_Y(y) = F'_Y(y)
= F'_X(y) - (-1)F'_X(-y)
= F'_X(y) + F'_X(-y)
= f_X(y) + f_X(-y)
apply definition of f_X
F_Y(y) = f_X(y) + f_X(-y)
= 1/24(y + 1)^2 + 1/24(-y + 1)^2
= 1/24[(y^2 + 2y + 1) + (y^2 - 2y + 1)]
= 1/24(2y^2 + 2)
= 1/12(y^2 + 1)
PDF of Y
f_Y(y) = {
	1/12(y^2 + 1), 0 <= y < 3
	0,             otherwise
}

example:
PDF:
f_X(x) = {
	1/18(x^2), -3 < x < 3
	0,         otherwise
}
Y = X^2 what is the PDF of Y?
Y nonzero on 0 < y < 9
= P(X^2 <= y)
= P(-(y)^1/2 <= X <= (y)^1/2)
= F_X((y)^1/2) - F_X(-(y)^1/2)
chain rule
= 1/2(y^-1/2) * F'_X((y)^1/2) + 1/2(y)^-1/2 * F'_X(-(y)^1/2)
= 1/2(y^-1/2) * f_X((y)^1/2) + 1/2(y)^-1/2 * f_X(-(y)^1/2)
= 1/2(y^-1/2)(f_X((y)^1/2) + f_X(-(y)^1/2))
applying definition of f_X
= 1/2(y^-1/2)(1/18((y)^1/2)^2 + 1/18(-(y)^1/2)^2)
= 1/2(y^-1/2)(1/18(y) + 1/18(y))
= 1/2(y^-1/2)(2/18(y))
= 1/18(y^-1/2)
= 1/18(sqrt(y))
full PDF of Y
f_Y(y) = {
	1/18(sqrt(y)), 0 < y < 9
	0,             otherwise
}

example:
PDF:
f_X(x) = {
	sin(x)/2, 0 < x < pi
	0,        otherwise
}
Y = |X - pi/2| whats the PDF of Y?
= P(X - pi/2 <= y)
= P(pi/2 - y <= X <= pi/2 + y)
= F_X((pi/2 + y)) - F_X(pi/2 - y)
chain rule
= (1)F'_X(pi/2 + y) - (-1)F'_X(pi/2 - y)
= F'_X(pi/2 + y) + F'_X(pi/2 - y)
= f_X(pi/2 + y) + f_X(pi/2 - y)
= sin(pi/2 + y)/2 + sin(pi/2 - y)/2
= cos(y)/2 + cos(y)/2
= 2cos(y)/2
= cos(y)
full PDF expression:
f_Y(y) = {
	cos(y), 0 <= y <= pi/2
	0,      otherwise
}

One-to-one and many-to-one transformations
cases where u(X) is both one-to-one and many-to-one over different intervals
as an example PDF:
f_X(x) = {
	e^(x-1), x < 1
	0,       otherwise
}
Y = |X|
over the interval x < 1
computing the PDF of Y we separately apply the distribution function method on the regions where Y is one-to-one and many-to-one
Transformation is one-to-one over the interval x <= -1 on interval y >= 1
Transformation is two-to-one over the inverval -1 < x < 1 on 0 <= y <1
(graphing the function for visual interval reference)
We need to consider these two cases separately
first y >= 1
F_Y(y) = P(Y <= y)
= P(|X| <= y)
= P(-X <= y)
= P(X >= -y)
= 1 - P(X <= -y)
= 1 - F_X(-y)
we used |X| = -X for X < 0
PDF for y >= 1
d/dy(1 - F_X(-y))
= F'_X(-y)
= f_X(-y)
definition of f_X
f_Y(y) = f_X(-y)
= e^-(y-1)
next 0 <= y < 1
= P(|X| <= y)
= P(-y <= X <= y)
= F_X(y) - F_X(-y)
PDF for 0 <= y < 1, chain
d/dy(F_X(y) - F_X(-y))
= F_X(y) + F'_X(-y)
= f_X(y) + f_X(-y)
definition of f_X
f_Y(y) = f_X(y) + f_X(-y)
= e^(y-1) + e^-(y-1)
full expression for the PDF:
f_Y(y) = {
	e^(y-1) + e^-(y-1), 0 <= y < 1
	e^-(y-1),           y >= 1
	0,				    otherwise
}

example:
X is a continuous random variable its PDF:
f_X(x) = {
	1/5, 0 < x < 5
	0,   otherwise
}
PDF of Y = (X - 3)^2 is given by
f_Y(y) = {
	g(y), 0 < y < 4
	h(y), 4 <= y < 9
	0,    otherwise
}
find the function h(y)
(first you can visulaize the transformation plug in Y = (X - 3)^2 and simply look at the interval 0 < x < 5)
Transformation is one-to-one over the interval 0 < x <= 1, on this interval Y is nonzero on 4 <= y < 9
Transformation is two-to-one over the interval 1 < x < 5 on this interval, Y is nonzero on 0 < y < 4
for this question we only look at 4 <= y < 9
definition of the cumulative distribution of Y, for 4 <= y < 9
F_Y(y) = P(Y <= y)
= P((X - 3)^2 <= y)
= P(sqrt((X - 3)^2) <= sqrt(y))
= P(|X - 3| <= sqrt(y))
= P(X - 3 >= -sqrt(y))
= P(X >= 3 - sqrt(y))
= 1 - P(X <= 3 - sqrt(y))
= 1 - F_X(3 - sqrt(y))
To compute the PDF f_Y(y) for 4 <= y < 9 chain rule:
f_Y(y) = F'_Y(y)
= d/dy(1 - F_X(3 - sqrt(y)))
= 1/2sqrt(y) * f_X(3 - sqrt(y))
applying definition of f_X
f_Y(y) = 1/2sqrt(y) * f_X(3 - sqrt(y))
= 1/2sqrt(y) * 1/5
h(y) = 1/10sqrt(y)

example:
PDF:
f_X(x) = {
	1/3(x^2), -1 < x < 2
	0,        otherwise
}
PDF of Y = X^2 is
f_Y(y) = {
	g(y), 0 < y < 1
	h(y), 1 <= y < 4
	0,    otherwise
}
find h(y)
F_Y(y) = P(Y <= y)
= P(X^2 <= y)
= P(X <= sqrt(y))
= P(X <= y^1/2)
F_X(y^1/2)
= 1/2y^-1/2 * F'_X(y^1/2)
= 1/2y^-1/2 * f_X(y^1/2)
applying the definition of f_X
f_Y(y) = 1/2y^-1/2 * f_X(y^1/2)
= 1/2y^-1/2 * 1/3(y^1/2)^2
= 1/2y^-1/2 * 1/3y
h(y) = 1/6(y^1/2)

example:
PDF:
f_X(x) = {
	1/3(x^2), -1 < x < 2
	0,        otherwise
}
PDF of Y = X^2 is given by
f_Y(y) = {
	g(y), 0 < y < 1
	h(y), 1 <= y < 4
	0,    otherwise
}
find g(y)
= P(X^2 <= y)
= P(-y^1/2 <= X <= y^1/2)
compute PDF f_Y(y) for 0 < y < 1 chain rule:
= 1/2y^-1/2 * F'_X(y^1/2) + 1/2y^-1/2 * F'_X(-y^1/2)
= 1/2y^-1/2 * f_X(y^1/2) + 1/2y^-1/2 * f_X(-y^1/2)
= 1/2y^-1/2(f_X(y^1/2) + f_X(-y^1/2))
apply definition of f_X
= 1/2y^-1/2(1/3(y^1/2)^2 + 1/3(-y^1/2)^2)
= 1/2y^-1/2(1/3(y) + 1/3(y))
= 1/2y^-1/2(2/3(y))
= 1/3(y^1/2)
g(y) = 1/3(y^1/2)

===================================================

Expected values of discrete random variables
the expected value (also known as the expectation or mean) of a discrete random variable is the average value we'd expect to get if we observed the random variable many times.
For a discrete random variable X with probability mass function f(x) defined over a set S, we can calculate the expected value using the following formula:
E[X] = Σ_x∈S x * f(x)
the expected value is the sum of products of each value of X and its associated probability taken over all possible values of X

calculating the expected value for random variable with uniform probability
what is the expected value of one roll of a tetrahedral dice?
A tetrahedral dice has four sides. Let X be the score attained on a random roll of the die. Then the support is S = {1,2,3,4} the probabilities are all 1/4
f(1) = f(2) = f(3) = f(4) = 1/4
summing up the products of each value of X and its associated probability
E[X] = 1 * f(1) + 2 * f(2) + 3 * f(3) + 4 * f(4)
= 1(1/4) + 2(1/4) + 3(1/4) + 4(1/4)
= 1/4 + 2/4 + 3/4 + 4/4
= 1+2+3+4/4
= 10/4
= 2.5

example:
A stack of twelve cards is labeled 1 through 12 if one card is selected what is its expected value?
S = {1,2,3,...,12} corresponding probabilities are all 1/12
(1+2+3+4+5+6+7+8+9+10+11+12)/12 = 78/12 = 13/2 = 6.5

example:
expected value of a six-sided die
1+2+3+4+5+6/6 = 3.5

calculating the expected value for a random variable with non-uniform probability
the rolls of an unfair tetrahedral die follow the probability mass function f(x), what is the expected value of the die?
x    | 1 | 2 | 3 | 4 |
----------------------
f(x)|1/10|3/10|2/5|1/5|
----------------------
E[X] = 1*f(1)+2*f(2)+3*f(3)+4*f(4)
= 1(1/10)+2(3/10)+3(2/5)
= 1/10 + 6/10 + 6/5 + 4/5
= 1/10 + 6/10 + 12/10 + 8/10
= 1+6+12+8/10
= 27/10
= 2.7

example:
rolling an unfair six-sided die probability mass function:
x   | 1 | 2 | 3 | 4 | 5 | 6 |
-----------------------------
f(x)|1/8|1/6|1/6|1/6|1/8|1/4|
-----------------------------
what is the expected value?
1/8 + 2/6 + 3/6 + 4/6 + 5/8 + 6/4
= 1/8 + 1/3 + 1/2 + 2/3 + 5/8 + 3/2
= 3/24 + 8/24 + 12/24 + 16/24 + 15/24 + 36/24
= 90/24
= 45/12
= 15/4

example:
rolling an unfair six-sided die probability mass function:
x   | 1 | 2  |  3 |  4 | 5  | 6 |
---------------------------------
f(x)|1/5|3/10|1/10|1/10|1/10|1/5|
---------------------------------
find expected value
= 1/5 + 6/10 + 3/10 + 4/10 + 5/10 + 6/5
= 2/10 + 6/10 + 3/10 + 4/10 + 5/10 + 12/10
= 32/10
= 16/5

calculating expected value given the frequency of a random variable
Several children were asked about how many pets they have. The results were recorded below if X is the number of pets that a randomly selected child has, what is expected value of X
# pets    | 0 1 2 |
# children| 5 6 5 |
start by computing the corresponding probabilities, the total number of children is 5 + 6 + 5 = 16 the corresponding probabilites are
P(X = 0) = f(0) = 5/16
P(X = 1) = f(1) = 6/16 = 3/8
P(X = 2) = f(2) = 5/16
E[X] = 0*f(0) + 1*f(1) + 2*f(2)
= 0 + 3/8 + 10/16
= 0 + 6/16 + 10/16
= 0+6+10/16
= 16/16
= 1

example:
several children were asked their age. The age of each child (in years) recorded below, X is the age of a randomly selected child, what is the expected value of X?
age        | 10 11 12 |
# children |  2  4  4 |
2 + 4 + 4 = 10
P(X = 10) = f(10) = 2/10 = 1/5
P(X = 11) = f(11) = 4/10 = 2/5
P(X = 12) = f(12) = 4/10 = 2/5
E[X] = 10(1/5) + 11(2/5) + 12(2/5)
= 2 + 22/5 + 24/5
= 10/5 + 22/5 + 24/5
= 10+22+24/5
= 56/5
= 11.2

example:
A bag contains 6 ten-dollar bills and 4 twenty-dollar bills, if one bill is randomly drawn from a bag what is its expected value?
(/total bills)
P(10) = 6/10 = 3/5
P(20) = 4/10 = 2/5
E[X] = 10*f(10) + 20*f(20)
= 10(3/5) + 20(2/5)
= 6 + 8
= 14

calculating the expected profit of a game
we play a game with a stack of five cards labeled 1-5. A card is drawn at random. Drawing 1,2,3,4 means that you lose $1 and drawing a 5 means you win $5 what is the expected profit?
p represent the profit on a draw in this game. On each draw in the game there are two possible outcomes
drawing a 1,2,3,4 the player loses $1 p = -1
drawing a 5, player wins $5 p = 5
probability outcomes are
P(-1) = 4/5, P(5) = 1/5
p   | -1 | 5 |
f(p)| 4/5|1/5|
expected value of p
E[p] = -1 * f(-1) + 5 * f(5)
= -1(4/5) + 5(1/5)
= -4/5 + 1
= 1/5
0.2
expected profit at each draw in this game is $0.20

example:
A game in which each play costs one dime, during each play the game dispenses a coin to the player to keep. The game machine contians equal numbers of pennies, nickels, dimes, and quaters. The dime that the play inserts does not go into the same pool of "reward" coins. What is the expected profit for a player who playes 10 games in a row
profit is revenue minus cost (winnings minus cost). Once we found the expected winnings after ten games, we need to subtract 10 * 10 = 100cents to account for the price of playing.
each four coins has an equal chance 1/4
w   | 1 | 5 | 10 | 25 |
f(w)|1/4|1/4| 1/4| 1/4|
E[W] = 1(1/4) + 5(1/4) + 10(1/4) + 25(1/4)
= 1+5+10+25/4
= 41/4
= 10.25
represents a single game, so
10 * E[W] = 102.5
subtracting the 100 cents paid to play
E[p] = 102.5 - 100 = 2.5

example:
A game featuring an unfair coin. If the coin lands on heads the player wins $3, if the coin lands on tails the player losses $1 the unfair coin has the probabilites
P(Heads) = 3/5, P(Tails) = 2/5
what is the expected profile at each toss in this game?
P(3) = 3/5, P(-1) 2/5
3(3/5) + (-1)(2/5)
= 7/5
= 1.4
expected profit is $1.40

===================================================

(most of the exercises in this section are simple, exluded most)
Properties of expectation for discrete random variables
there are several important properties that we can use to simplify expressions involving expecting values.
The first property is that the expected value of a constant is just the constant itself.
E[a] = a
for example
E[5] = 5, E[-99] = -99, E[sqrt(2)] = sqrt(2)
The idea that the expected value of a constant is equal to the same constant makes intuitive sense. If a is a constant then it does not vary.

We can combine the factor and constant rules into one
X is a random variable and a and b are constants by combining two rules we have
E[aX + b] = aE[X] + b
if we know that E[X] = 4
E[3X + 2] = E[3X + 2]
= 3E[X] + 2
= 3(4) + 2
= 12 + 2
= 14

example:
E[1 - 4X] if E[X] = 1/2
E[1 - 4X] = E[-4X + 1]
= -4E[X] + 1
= -4(1/2) + 1
= -2 + 1
= -1

example:
E[5 - 2X] if E[X] = 3
= E[-2X + 5]
= -2E[X] + 5
= -2(3) + 5
= -6 + 5
= -1

example:
x   | 0 | 1 |  2 |  3 | 4 |
f(x)|0.1|0.2|0.25|0.15|0.3|
---------------------------
calculate E[5x + 4]
= 0*f(0.1) + 1*f(0.2) + 2*f(0.25) + 3*f(0.15) + 4*f(0.3)
= 2.35
E[5(2.35) + 4]
= 15.75

===================================================

Moments of discrete random variables
We can calculate the expected value of the random variable X^2 as
E[X^2] = Σ_x∈S x^2 * f(x)
for example the random variable X has the following probability distribution
x   |  1 | 2 |  3 |
f(x)|0.25|0.5|0.25|
E[X^2] = Σ_x∈S x^2 * f(x)
= 1^2 * 0.25 + 2^2 * 0.5 + 3^2 * 0.25
= 1 * 0.25 + 4 * 0.5 + 9 * 0.25
= 4.5
its important to remember that we still evaluate the probability mass function as f(x) not f(x^2)

calculating the second moment of a random variable
A random variable X has the probability distribution
x   | 0 |  1 |  2 |  3 |  4 |
f(x)|0.2|0.25|0.15|0.35|0.05|
-----------------------------
[X^2] = Σ_x∈S x^2 * f(x)
= 0^2 * 0.2 + 1^2 * 0.25 + 2^2 * 0.15 + 3^2 * 0.35 + 4^2 * 0.05
= 0 * 0.2 + 1 * 0.25 + 4 * 0.15 + 9 * 0.35 + 16 * 0.05
= 4.8

example:
x   | 0 | 1 |  2 |  3 | 4 |
f(x)|0.1|0.2|0.25|0.15|0.3|
---------------------------
E[X^2]
= 0*0.1 + 1*0.2 + 4*0.25 + 9*0.15 + 16*0.3
= 7.35

example:
A random variable X has the probability distribution given below
f(x) = {
	3/10, x = 0
	1/2,  x = 1
	1/5,  x = 2
}
E[X^2]
1(1/2) + 4(1/5)
= 5/10 + 8/10
= 13/10

E[X] and E[X^2] are called first moment and second moment, its also possible to define higher moments E[X^3] is third moment
in general nth moment:
E[X^n] = Σ_x∈S x^n * f(x)
E[X], E[X^2], E[X^3] etc are sometimes referred to as first second and third raw moments, because we can define other types of moments
in general E[X^n] can be called nth moment or nth raw moment

example:
x   | 0 | 1 |  2 |  3 | 4 |
f(x)|0.1|0.2|0.25|0.15|0.3|
---------------------------
E[X^3]
= 0.2 + 2^3*0.25 + 3^3*(0.15) + 4^3*0.3
= 0.2 + 8*0.25 + 27*0.15 + 64*0.3
= 25.45

example:
A random variable X has the probability distribution
f(x) = {
	3/10, x = 0
	1/2,  x = 1
	1/5,  x = 2
}
E[X^4]
= 1/2 + 16(1/5)
= 5/10 + 32/10
= 37/10

simplifying expressions using the properties of expected value
E[X(X - 3) + 4] if E[X^2] = 4 and E[X] = 1
expanding out the product inside the expected value, using the properties of the expected value and substituting the given information
E[X(X - 3) + 4] = E[X^2 - 3X + 4]
= E[X^2] - 3E[X] + 4
= 4 - 3(1) + 4
= 5

example:
E[A(2A + 1) - 3] if E[A^2] = 5 and E[A] = 1
E[A(2A + 1) - 3] = 2E[A^2] + E[A] - 3
= 2(5) + 1 - 3
= 11 - 3
= 8

example
E[(3A - 2)^2 + 1] if E[A^2] = 2 and E[A] = 1
= E[(3A - 2)(3A - 2) + 1]
= E[(9A^2 - 6A - 6A + 4) + 1]
= E[(9A^2 - 12A + 4) + 1]
= 9E[A^2] - 12E[A] + 5
= 9(2) - 12(1) + 5
= 18 - 12 + 5
= 11

===================================================

The variance of a discrete random variable X with probability mass function f(x) defined over a set S
Var[X] = Σ_x∈S (x - E[X])^2 * f(x)
the variance is the expected value of the squared difference between X and the expected value of X
Var[X] = E[(X - E[X])^2]
The variance quantifies how "spread out" a probability distribution is. The expected value E[X] can be interpreted as the "center" of a probability distribution, and the squared difference (X - E[X])^2 represents the squared distance between X and the center. So the variance quantifies how far the possible values of X are from the center on average.
A probability distribution that is spread out over many possible values will have a high variance
A probability distribution that is concentrated near a single value will have a low variance.

computing variance using the defiinition given a probability mass function
let X be the number obtained by a spinner with four sections labeled 1,2,3,4 the probability mass function f(x) of X is shown in the table below, what is Var[X]?
x   | 1 | 2 | 3 | 4 |
f(x)|1/6|1/4|1/3|1/4|
---------------------
The variance of a discrete random variable X with probability mass function f(x) defined over a set S is given by
Var[X] = Σ_x∈S (x - E[X])^2 * f(x)
first compute E[X]
E[X] = 1(1/6) + 2(1/4) + 3(1/4) + 4(1/4)
= 8/3
substituting E[X] = 8/3 into the variance formula
Var[X] = Σ_x∈S (x - 8/3)^2 * f(x)
= (1 - 8/3)^2*f(1) + (2 - 8/3)^2*f(2) + (3 - 8/3)^2*f(3) + (4 - 8/3)^2*f(4)
= 25/9*f(1) + 4/9*f(2) + 1/9*f(3) + 16/9*f(4)
= 25/9(1/6) + 4/9(1/4) + 1/9(1/3) + 16/9(1/4)
= 19/18

example:
A fair seven sided die with sides labeled with numbers 1-7 is thrown once. If X is the outcome of the die, what is the variance of X
= 1(1/7) + 2(1/7) + 3(1/7) + 4(1/7) + 5(1/7) + 6(1/7) + 7(1/7)
= 4
substituting
Var[X] = (1 - 4)^2(1/7) + (2 - 4)^2(1/7) + (3 - 4)^2(1/7) 
+ (4 - 4)^2(1/7) + (5 - 4)^2(1/7) + (5 - 4)^2(1/7) + (6 - 4)^2(1/7)
+ (7 - 4)^2(1/7)
= 9(1/7) + 4(1/7) + 1(1/7) + 0 + 4(1/7) + 9(1/7)
= 4

example:
let X be the outcome of rolling an unfair six-sided die that follows the probability mass function f(x) shown in the table below what is Var[X]?
x   | 1 | 2 | 3 | 4 | 5 | 6 |
f(x)|1/4|1/4|1/8|1/8|1/8|1/8|
-----------------------------
= 1(1/4) + 2(1/4) + 3(1/8) + 4(1/8) + 5(1/8) + 6(1/8)
= 1/4 + 2/4 + 3/8 + 4/8 + 5/8 + 6/8
= 2/8 + 4/8 + 3/8 + 4/8 + 5/8 + 6/8
= 24/8
= 3
substituting
Var[X] = (1 - 3)^2(1/4) + (2 - 3)^2(1/4) + (3 - 3)^2(1/8)
+ (4 - 3)^2(1/8) + (5 - 3)^2(1/8) + (6 - 3)^2(1/8)
= 1 + 1/4 + 0 + 1/8 + 4/8 + 9/8
= 8/8 + 2/8 + 1/8 + 4/8 + 9/8
= 24/8
= 3

A formula for quickly computing the variance
using the properties of the expected value of a discrete random variable X we can arrive at the following alternative formula for the variance of X:
Var[X] = E[X^2] - E[X]^2
using this speeds up computing variance

computing variance given the expected value of a random variable and its square
if E[X] = 4 and E[X^2] = 21 what is Var[X]?
Var[X] = E[X^2] - E[X]^2
= 21 - 4^2
= 21 - 16
= 5

example:
E[X] = 3 and E[X^2] = 11 what is Var[X]?
11 - 9 = 2

example:
E[X] = 1/3 and E[X^2] = 2/3 what is Var[X]?
= 2/3 - 1/9
= 6/9 - 1/9
= 5/9

computing variance given a probability mass function
A fair die with sides labeled 1-6 is thrown once. If X is the outcome of the die what is the variance of X?
x   | 1 | 2 | 3 | 4 | 5 | 6 |
f(x)|1/6|1/6|1/6|1/6|1/6|1/6|
-----------------------------
E[X] = 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6)
= 7/2
next compute E[X^2]
= 1(1/6) + 4(1/6) + 9(1/6) + 16(1/6) + 25(1/6) + 36(1/6)
= 91/6
Var[X] = E[X^2] - E[X]^2
= 91/6 - (7/2)^2
= 91/6 - 49/4
= 35/12

example:
A fair five-sided die with sides labeled with numbers 1-5 is thrown once if X is the outcome of the die what the variance?
E[X] = 1(1/5) + 2(1/5) + 3(1/5) + 4(1/5) + 5(1/5)
= 1/5 + 2/5 + 3/5 + 4/5 + 5/5
= 3
Var[X] = E[X^2] - E[X]^2
E[X^2] = 1(1/5) + 4(1/5) + 9(1/5) + 16(1/5) + 25(1/5)
= 1/5 + 4/5 + 9/5 + 16/5 + 25/5
= 11
11 - 9 = 2

example:
what is Var[X] by the table below
x   | 1  | 2  | 3 | 4 | 5  | 6  |
f(x)|3/10|1/10|1/5|1/5|1/10|1/10|
---------------------------------
E[X] = 1(3/10) + 2(1/10) + 3(1/5) + 4(1/5) + 5(1/10) + 6(1/10)
= 3/10 + 2/10 + 3/5 + 4/5 + 5/10 + 6/10
= 3/10 + 2/10 + 6/10 + 8/10 + 5/10 + 6/10
= 30/10
= 3
Var[X] = E[X^2] - E[X]^2
E[X^2] = 1(3/10) + 4(1/10) + 9(1/5) + 16(1/5) + 25(1/10) + 36(1/10)
= 3/10 + 4/10 + 9/5 + 16/5 + 25/10 + 36/10
= 3/10 + 4/10 + 18/10 + 32/10 + 25/10 + 36/10
= 118/10
= 59/5
= 59/5 - 45/5
= 14/5

The standard deviation of a random variable X is defined as the square root of the variance:
SD[X] = sqrt(Var[X])
the standard deviation is often used when we observe a random variable and wish to quantify how "crazy" a particular observation was
normally we compute how many standard deviations the observed value was away from expected value: the further away, the more crazy the observation.

example:
A fair die with sides labeled with numbers 1-6 is thrown once, what is SD[X]?
SD[X] = sqrt(Var[X])
Var[X] = E[X^2] - E[X]^2
x   | 1 | 2 | 3 | 4 | 5 | 6 |
f(x)|1/6|1/6|1/6|1/6|1/6|1/6|
-----------------------------
E[X] = 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6)
= 7/2
E[X^2] = 1(1/6) + 4(1/6) + 9(1/6) + 16(1/6) + 25(1/6) + 36(1/6)
= 91/6
Var[X] = E[X^2] - E[X]^2
= 91/6 - (7/2)^2
= 91/6 - 49/4
= 35/12
SD[X] = sqrt(Var[X])
= sqrt(35/12)

example:
let X be the score obtained on the roll of a fair four-sided die with sides labeled with numbers 1,2,3,4 what is SD[X]?
x   | 1 | 2 | 3 | 4 |
f(x)|1/4|1/4|1/4|1/4|
---------------------
E[X] = 1(1/4) + 2(1/4) + 3(1/4) + 4(1/4)
= 1/4 + 2/4 + 3/4 + 4/4
= 10/4
= 5/2
E[X^2] = 1(1/4) + 4(1/4) + 9(1/4) + 16(1/4)
= 1/4 + 4/4 + 9/4 + 16/4
= 30/4
= 15/2
Var[X] = E[X^2] - E[X]^2
= 30/4 - 25/4
= 5/4
SD[X] = sqrt(5/4)
= sqrt(5)/2

example:
Several children were asked their age, if X is the age of a randomly selected child, what is the SD of X?
----------------------------
x (age)        | 4 | 6 | 8 |
f(x) #children | 1 | 2 | 3 |
----------------------------
= 1 + 2 + 3 = 6
P(X = 4) = 1/6
P(X = 6) = 2/6 = 1/3
P(X = 8) = 3/6 = 1/2
------------------
x    | 4 | 6 | 8 |
f(x) |1/6|1/3|1/2|
------------------
E[X] = 4(1/6) + 6(1/3) + 8(1/2)
= 4/6 + 6/3 + 8/2
= 4/6 + 12/6 + 24/6
= 40/6
= 20/3
E[X^2] = 16(1/6) + 36(1/3) + 64(1/2)
= 140/3
Var[X] = E[X^2] - E[X]^2
= 140/3 - (20/3)^2
= 140/3 - 400/9
= 20/9
SD[X] = sqrt(Var[X])
= sqrt(20/9)
= 2sqrt(5)/3

prove the following formula for the variance
Var[X] = E[X^2] - E[X]^2
the variance of random variable X is defined as
Var[X] = E[(X - E[X])^2]
expanding:
Var[X] = E[X^2 - 2E[X] * X + E[X]^2]
distributing the expectation over the sum
Var[X] = E[X^2] + E[-2E[X] * X] + E[E[X]^2]
simplify
since E[aX] = aE[X] for any constant a
E[-2E[X] * X] = -2E[X] * E[X] = -2E[X]^2
E[X] is a constant and E[a] = a for any constant
E[E[X]^2] = E[X]^2

Var[X] = E[X^2] + E[-2E[X] * X] + E[E[X]^2]
= E[X^2] - 2E[X]^2 + E[X]^2
= E[X^2] - E[X]^2

===================================================

The variance of a constant is always 0, for any constant a
Var[a] = 0

computing the variance of a constant
calculate Var[4k^2 - 1] where k is a constant
since k is a constant 4k^2 - 1 is also a constant
Var[4k^2 - 1] = 0

example:
Var[pi] = 0
Var[k^2] = 0

the variance of a scaled random variable
X is a random variable and a is a constant then
Var[aX] = a^2Var[X]
if we know that Var[X] = 2, then
Var[3X] = 3^2 * Var[X]
= 9 * Var[X]
= 9 * 2
= 18
similarly
Var[-4X] = (-4)^2 * Var[X]
= 16 * Var[X]
= 16 * 2
= 32
why do we sqare the constant when we factor it outside the variance?
Var[X] = E[(X - E[X])^2]
Var[aX] = E[(aX - E[aX])^2]
= E[(aX - aE[X])^2]
= E[a^2(X - E[X])^2]
= a^2E[(X - E[X])^2]
= a^2Var[X]

computing the variance of a scaled random variable
the probability distribution of the random variable X is given below, given that E[X] = 2.3 calculate Var[-10X]
------------------
x    | 1 | 2 | 3 |
f(x) |0.2|0.3|0.5|
------------------
Var[aX] = a^2*Var[X]
a = -10
Var[-10X] = (-10)^2 * Var[X]
= 100 * Var[X]
compute the variance of X using the formula
Var[X] = E[X^2] - E[X]^2
given E[X] = 2.3
E[X^2] = 0.2 + 4(0.3) + 9(0.5)
= 5.9
5.9 - (2.3)^2
= 0.61
Var[-10X] = 100 * Var[X]
= 100 * 0.61
= 61

example:
Var[X] = 8, calculate Var[2X]
Var[2X] = (2)^2 * 8
= 32

example:
E[X] = 7, calculate Var[5X]
------------------
x    | 5 | 6 | 9 |
f(x) |1/5|2/5|2/5|
------------------
E[X^2] = 25(1/5) + 36(2/5) + 81(2/5)
= 25/5 + 72/5 + 162/5
= 259/5 - 245/5
= 14/5
Var[5X] = 25 * 14/5
= 350/5
= 70

When a random variable is scaled by a factor of a and shifted by a constant b
the variance is scaled by a factor of a^2
the variance is unaffected by the shift
Shifting all values of X by a fixed amount does not change how far apart those values are form each other
if we know that Var[X] = 3
Var[2X + 1] = 2^2 * Var[X]
= 2^2 * 3
= 4 * 3
= 12

variance of a transformed random variable
Var[X] = 10 calculate Var[1 - 2X]
Var[aX + b] = a^2 * Var[X]
a = -2, and b = 1
Var[1 - 2X] = (-2)^2 * Var[X]
= 4 * 10
= 40

example:
Var[X] = 8, Var[4X + 3]
16 * 8 = 128

example:
E[X] = 7, calculate Var[2 - 5X]
------------------
x    | 5 | 6 | 9 |
f(x) |1/5|2/5|2/5|
------------------
E[X^2] = 25(1/5) + 36(2/5) + 81(2/5)
= 25/5 + 72/5 + 162/5
= 259/5 - 245/5
= 14/5
Var[2 - 5X] = 25 * Var[X]
Var[-5X] = 25 * 14/5
= 14 * 5
= 70

proof of general formula
Var[aX + b] = a^2Var[X]
by definition
Var[X] = E[aX - E[X])^2]
Var[aX + b] = E[(aX + b - E[aX + b])^2]
simplify
E[(aX + b - E[aX + b])^2] = E[(aX + b - aE[X] - b)^2]
= E[(aX - aE[X])^2]
= E[(a(X - E[X]))^2]
= E[a^2(X - E[X])^2]
= a^2E[(X - E[X])^2]
= a^2Var[X]

===================================================

Expected value (expectation or mean) of a continuous random variable is the average value we'd expect to get if we observed the random variable many times.
For a continuous random variable X with the probability density function f(x) defined over a set S we can calculate the expected value using the formula:
E[X] = ∫_S x * f(x) dx
for example consider the continuous random variable X with the following probability density function
f(x) = x/4, 1 < x < 3
compute the expected value
E[X] = ∫^3_1 x * x/4 dx
= ∫^3_1 x^2/4 dx
= [x^3/12]|_1-3
= 13/6

mean of a continuous random variable
PDF compute E[X]
f(x) = x^2/9, 0 <= x <= 3
E[X] = ∫_S x * f(x) dx
E[X] = ∫^3_0 x * x^2/9 dx
= ∫^3_0 x^3/9 dx
= [x^4/36]|_0-3
= 9/4

example:
PDF compute E[X]
f(x) = 2/x^3, x >= 1
E[X] = ∫^∞_1 x * 2/x^3 dx
E[X] = ∫^∞_1 2/x^2 dx
= [-2/x]|_1-∞
(0) - (-2/1)
= 2

example:
PDF compute E[X]
f(x) = 2x/3, 1 <= x <= 2
E[X] = ∫^2_1 x * 2x/3 dx
= ∫^2_1 2x^2/3 dx
= [2x^3/9]|_1-2
= (16/9) - (2/9)
= 14/9

mean of a continuous random variable with a piecewise PDF
PDF compute E[X]
f(x) = {
	1/4,       0 < x < 2
	x/2 - 3/4, 2 <= x <= 3
}
E[x] = ∫^3_0 x * f(x) dx
= ∫^2_0 x * f(x) dx + ∫^3_2 x * f(x) dx
= ∫^2_0 x * 1/4 dx + ∫^3_2 x * (x/2 - 3/4) dx
= ∫^2_0 x/4 dx + ∫^3_2 (x^2/2 - 3x/4) dx
= [x^2/8]|_0-2 + [(x^3/6 - 3x^2/8)]|_2-3
= 1/2 + 31/24
= 43/24

example:
PDF compute E[X]
f(x) = {
	x^2/12, 0 < x < 2
	7/9, 	2 <= x < 3
}
= ∫^2_0 x * x^2/12 dx + ∫^3_2 x * 7/9 dx
= ∫^2_0 x^3/12 dx + ∫^3_2 7x/9 dx
= [x^4/48]|_0-2 + [7x^2/18]|_2-3
= (16/48) - 0 + (63/18) - (28/18)
= 8/24 + 35/18
= 4/12 + 35/18
= 2/6 + 35/18
= 1/3 + 35/18
= 6/18 + 35/18
= 41/18

example:
PDF compute E[X]
f(x) = {
	3/8,   0 < x < 2
	2/x^3, x >= 2
}
= ∫^2_0 x * 3/8 dx + ∫^∞_2 x * 2/x^3 dx
= ∫^2_0 3x/8 dx + ∫^∞_2 2/x^2 dx
= [3x^2/16]|_0-2 + [-2/x]|_2-∞
= (12/16) - (0) + (0) - (-1)
= 6/8 + 1
= 3/4 + 4/4
= 7/4

computing mean of a transformed continuous random variable
PDF compute E[7X - 4]
f(x) = 3x^2/7, 1 < x < 2
first simplify
E[7X - 4] = 7E[X] - 4
calculate expected value
∫^2_1 x * 3x^2/7 dx
= ∫^2_1 3x^3/7 dx
= [3x^4/28]|_1-2
= 45/28
E[7X - 4] = 7E[X] - 4
7 * 45/28 - 4
= 29/4

example:
PDF compute E[3X - 4]
f(x) = x/8 - 1/4, 2 < x < 6
3E[X] - 4
∫^6_2 x * x/8 - 1/4 dx
= ∫^6_2 x^2/8 - x/4 dx
= [x^3/24 - x^2/8]|_2-6 dx
= (216/24 - 36/8) - (8/24 - 4/8)
= (216/24 - 108/24) - (8/24 - 12/24)
= 108/24 + 4/24
= 112/24
= 14/3
3E[X] - 4
= 3 * 14/3 - 4
= 42/3 - 16/4
= 168/12 - 48/12
= 120/12
= 10

example:
PDF compute E[5X - 2]
f(x) = {
	1 - x^3, 0 < x <= 1
	1/8,     1 <= x < 3
}
5E[X] - 2
∫^1_0 x * 1 - x^3 dx ∫^3_1 x * 1/8 dx
= ∫^1_0 x - x^4 dx ∫^3_1 x/8 dx
= [x^2/2 - x^5/5]|_0-1 + [x^2/16]|_1-3
= (1/2 - 1/5) - (0) + (9/16 - 1/16)
= (5/10 - 2/10) + 8/16
= (3/10) + 4/8
= (3/10) + 2/4
= (3/10) + 1/2
= 3/10 + 5/10
= 8/10
= 4/5
5 * 4/5 - 2
= 20/5 - 2
= 4 - 2
= 2

===================================================

Moments of continuous random variables
to find the second moment substitute x^2 for x in the formula as we did for the first moment
E[X^2] = ∫_S x^2 * f(x) dx
PDF of the second moment
f(x) = 3/2(sqrt(x)), 0 <= x <= 1
E[X^2] = ∫^1_0 x^2 * 3/2(sqrt(x)) dx
= 3/2 ∫^1_0 x^(5/2) dx
= 3/2[2/7x^(7/2)]|_0-1
= 3/22/7 - 0)
= 3/7

the second moment of a random variable
continuous random variable X PDF
f(x) = {
	1/8,  -2 < x < 0
	3/2x, 0 <= x < 1
}
calculate E[X^2]
compute the expected value of X^2
E[X^2] = ∫^1_-2 x^2 * f(x) dx
= ∫^0_-2 x^2 * 1/8 dx + ∫^1_0 x^2 * 3/2x dx
= 1/8 ∫^0_-2 x^2 dx + 3/2 ∫^1_0 x^3 dx
= [1/24(x^3)]|_-2-0 + [3/8(x^4)]|_0-1
= 1/24(0 + 8) + 3/8(1 - 0)
= 1/3 + 3/8
= 17/24

example:
PDF calculate E[X^2]
f(x) = 3/x^4, x >= 1
E[X^2] = ∫^∞_1 x^2 * 3/x^4 dx
= ∫^∞_1 3/x^2 dx
= [-3/x]|_1-∞
= (0) - (-3/1)
= 3

example:
PDF calculate E[X^2]
f(x) = {
	1/x^2, 1 < x <= 2
	x/5,   2 <= x < 3
}
E[X^2] = ∫^2_1 x^2 * 1/x^2 dx + ∫^3_2 x^2 * x/5 dx
= ∫^2_1 1 dx + ∫^3_2 x^3/5 dx
= 1 + [x^4/20]|_2-3
= 1 + 81/20 - 16/20
= 1 + 65/20
= 4/4 + 13/4
= 17/4

In general E[X^n] can be called either nth moment or nth raw moment
Calculating higher moments of a random variable
continuous random variable X has the PDF
f(x) = {
	1 - x, 0 <= x <= 1
	1/2,   1 < x <= 2
}
calculate E[X^4]
fourth raw moment
∫^2_0 x^4 * f(x) dx
= ∫^1_0 x^4 * (1 - x) dx + ∫^2_1 x^4 * 1/2 dx
= ∫^1_0 (x^4 - x^5) dx + 1/2 ∫^2_1 x^4 dx
= [x^5/5 - x^6/6]|_0-1 + [x^5/10]|_1-2
= ((1/5 - 1/6) - (0)) + (32/10 - 1/10)
= 1/30 + 31/10
= 47/15

example:
PDF of the third raw moment E[X^3]
f(x) = 3/4(2x - x^2), 0 <= x <= 2
∫^2_0 x^3 3/4(2x - x^2) dx
= 3/4 ∫^2_0 (2x^4 - x^5) dx
= 3/4 [2x^5/5 - x^6/6]|_0-2
= 3/4 [2x^5/5 - x^6/6]|_0-2
= 3/4(64/5 - 64/6) - (0)
= 3/4(384/30 - 320/30)
= 3/4(64/30)
= 3/4(32/15)
= 96/60
= 16/10
= 8/5

example:
PDF of the fourth raw moment E[X^4]
f(x) = {
	3/x^4,  1 < x <= 2
	4/3x^3, 2 <= x < 4 
}
∫^2_1 x^4 3/x^4 dx + ∫^4_2 x^4 4/3x^3 dx
= ∫^2_1 3 dx + ∫^4_2 x^4 4/3x^3 dx
= 3 + ∫^4_2 4x/3 dx
= 3 + [4x^2/6]|_2-4
= 3 + (64/6) - (16/6)
= 3 + (48/6)
= 3 + 8
= 11

computing the mean of a transformed random variable
PDF of a continuous random variable X
f(x) = 2x, 0 <= x <= 1
given that E[X] = 2/3 compute E[5x^2 - 2X]
assume that E[aX + bY] = aE[X] + bE[Y] for any random variables X and Y and constants a and b
first simplify
E[5x^2 - 2X] = 5E[X^2] - 2E[X]
E[X^2] = ∫^1_0 x^2 * 2x dx
= ∫^1_0 2x^3 dx
= [2x^4/4]|_0-1
= [x^4/2]|_0-1
= 1/2 - (0)
= 1/2
substituting
5 * 1/2 - 2 * 2/3
= 5/2 - 4/3
= 7/6

example:
PDF continuous random variable compute E[4X^2 - 3]
E[4X^2 - 3] = 4E[X^2] - 3
f(x) = x/4, 1 < x < 3
∫^3_1 x^2 * x/4 dx
= ∫^3_1 x^3/4 dx
= [x^4/16]|_1-3
= (81/16 - 1/16)
= 80/16
= 5
4E[X^2] - 3
= 4 * 5 - 3
= 17

example:
PDF continuous random variable compute 
given E[X] = 11/20, E[2X - X^3 + 1]
f(x) = (3x^2 + 4)/5, 0 < x < 1
2E[X] - E[X^3] + 1
∫^1_0 x^3 * (3x^2 + 4)/5 dx
= ∫^1_0 (3x^5 + 4x^3)/5 dx
= [3x^6/30 + 4x^4/20]|_0-1
= 6/60 + 12/60
= 18/60
= 9/30
= 3/10
2E[X] - E[X^3] + 1
2 * 11/20 - 3/10 + 1
= 22/20 - 3/10 + 1
= 22/20 - 6/20 + 1
= 16/20 + 1
= 16/20 + 20/20
= 36/20
= 18/10
= 9/5

===================================================

The variance of a continuous random variable is the expected squared difference between the variable and its expected value
For a continuous random variable X with PDF f(x) defined over a set S the variance
Var[X] = E[(X - E[X])^2]
= ∫_S (x - E[X])^2 * f(x) dx
its easier to use
Var[X] = E[X^2] - E[X]^2
for example consider this PDF with the continuous random variable X
f(x) = x/4, 1 < x < 3
first compute E[X]
= ∫^3_1 x * x/4 dx
= ∫^3_1 x^2/4 dx
= [x^3/12]|_1-3
= 13/6
E[X^2] = ∫^3_1 x^2 * x/4 dx
= ∫^3_1 x^3/4 dx
= [x^4/16]|_1-3
= 5
variance
Var[X] = E[X^2] - E[X]^2
= 5 - (13/6)^2
= 11/36

computing the variance of a continuous random variable given two-branch PDF
the continuouis random variable X has the PDF f(x) given E[X] = 10/3 compute Var[X]
f(x) = {
	8 - 2x, 3 <= x <= 4
	0,      otherwise
}
compute E[X^2]
= ∫^4_3 x^2 * (8 - 2x) dx
= ∫^4_3 (8x^2 - 2x^3) dx
= [8^3/3 - x^4/2]|_3-4
= 67/6
Var[X] = E[X^2] - E[X]^2
67/6 - (10/3)^2
= 1/18

example:
The continuous random variable X has the PDF given E[X] = 3/2 compute Var[X]
f(x) = {
	3/x^4, x >= 1,
	0,     otherwise
}
= ∫^∞_1 x^2 3/x^4 dx
= ∫^∞_1 3/x^2 dx
= [-3/x]|_1-∞
= (0) - (-3/1)
= 3
3 - (3/2)^2
= 12/4 - 9/4
= 3/4

example:
PDF given E[X] = 3/2 compute Var[X]
f(x) = {
	3x^2/8, 0 <= x <= 2
	0,      otherwise
}
= ∫^2_0 x^2 3x^2/8 dx
= ∫^2_0 3x^4/8 dx
= [3x^5/40]|_0-2
= (96/40) - (0)
= 48/20
= 24/10
= 12/5
12/5 - (3/2)^2
= 48/20 - 45/20
= 3/20

computing the variance of a continuous random variable given three-branch PDF
given E[X] ~= 1.79167 compute Var[X] round 2 decimals
f(x) = {
	1/4, 	   0 < x < 2
	x/2 - 3/4, 2 <= x < 3
	0,         otherwise
}
= ∫^3_0 x^2 f(x) dx
= ∫^2_0 x^2 1/4 dx + ∫^3_2 x^2 * (x/2 - 3/4) dx
= ∫^2_0 x^2/4 dx + ∫^3_2 (x^3/2 - 3x^2/4) dx
= [x^3/12]|_0-2 + [x^4/8 - x^3/4]|_2-3
~= 4.04167
~= 4.04167 - (1.79167)^2
~= 0.83

example:
PDF given E[X] ~= 2.2778 compute Var[X] round 3 decimals
f(x) = {
	x^2/12, 0 < x < 2
	7/9,    2 <= x < 3
	0,      otherwise
}
= ∫^2_0 x^2 x^2/12 dx + ∫^3_2 x^2 7/9 dx
= ∫^2_0 x^4/12 dx + ∫^3_2 7x^2/9 dx
= [x^5/60]|_0-2 + [7x^3/27]|_2-3
= (32/60) - (0) + (189/27) - (56/27)
= 16/30 + 133/27
= 8/15 + 133/27
= 72/135 + 665/135
= 737/135
~= 5.459259
~= 5.459259 - (2.2778)^2
~= 5.459259 - 5.18837
~= 0.271

example:
PDF given E[X] = 3 compute Var[X]
f(x) = {
	x/9, 		0 < x < 3
	1/9(6 - x), 3 <= x < 6
	0,  		otherwise
}
= ∫^3_0 x^2 x/9 dx + ∫^6_3 x^2 1/9(6 - x) dx
= ∫^3_0 x^3/9 dx + ∫^6_3 2x^2/3 - x^3/9 dx
= [x^4/36]|_0-3 + [2x^3/9 - x^4/36]|_3-6
= (81/36) - (0) + ((432/9) - (1296/36) - (54/9) - (81/36))
= (81/36) + ((1728/36) - (1296/36) - (54/9) - (81/36))
= (81/36) + ((432/36) - (216/36) - (81/36))
= (81/36) + ((432/36) - (135/36))
= (81/36) + (297/36)
= 378/36
= 189/18
= 63/6
= 21/2
= 21/2 - (3)^2
= 21/2 - 9
= 21/2 - 18/2
= 3/2

===================================================

The rule of the lazy statistion
suppose we have a discrete random variable X with probability mass function f_X and support S_X = {-1,0,1}
______________________
|x     | -1 | 0 | 1  |
|f_X(x)|0.25|0.5|0.25|
----------------------
lets define a new random variable Y
Y = (X + 1)^3
computing E[Y] is a lot of work if we only use the methods we've seen up to now, typically we would need to find the probability mass function f_Y of Y and then compute E[Y]
lets use the rule of the lazy statistician so we can compute E[Y] without having to find f_Y first, this is useful because E[Y] might be the only quantity we're interested in
the rule of the lazy statistician states that if Y = r(X) then
applying the rule of the lazy statistician with the function r(x) = (x + 1)^3 for our probability mass function (PMF) f_X
E[Y] = Σ_x∈S_X r(x)f_X(x)
= Σ_x∈S_X (x + 1)^3 f_X(x)
= (-1 + 1)^3 * 0.25 + (0 + 1)^3 * 0.5 + (1 + 1)^3 * 0.25
= 0^3 * 0.25 + 1^3 * 0.5 + 2^3 * 0.25
= 0 * 0.25 + 1 * 0.5 + 8 * 0.25
= 0 + 0.5 + 2
= 2.5
E[Y] = 2.5
The rason for the name "the rule of the lazy statistician" is that the rule is sometimes mistaken as a deinition when in fact it is a statement that requires rigorous proof
The rule is sometimes called the law of the uncconscious statisician (or LOTUS)
In general E[r(X)] != r(E[X])
A spinner has four sides, labeled 1-4, let random variable X equal the result when the spinner is spun once, let the function Y = r(X) denote the profit made when the spinner lands on X = x, to calc the expected profit on each spin we multiply each possible payout by the probability of that payout occuring and sum over all possible values of X

applying the rule of the lazy statistician for discrete random variables
A discrete random variable X has the probability mass function f(x) shown in the table, find E[5e^(1-X)]
______________
|x   | 1 | 2 |
|f(x)|0.2|0.8|
--------------
A discrete random variable X and a function r(x) the rule of the lazy statistician states that
E[r(X)] = Σ_x∈S r(x)f(x)
we have
r(x) = 5e^(1-x)
E[5e^(1-x)] = Σ_x∈S f(x)
= 5e^(1-1) * 0.2 + 5e^(1-2) * 0.8
= 5e^(0) * 0.2 + 5e^(-1) * 0.8
= 5 * 0.2 + 5e^(-1) * 0.8
= 1 + 4e^(-1)
= 1 + 4/e

example
PMF shown in table, find E[(X - 1)^2]
__________________
|x   | 1 | 2 | 3 |
|f(x)|0.1|0.3|0.6|
------------------
r(x) = (X - 1)^2
= (1-1)^2 * 0.1 + (2-1)^2 * 0.3 + (3-1)^2 * 0.6
= 0 + 0.3 + 2.4
= 2.7

example
PMF shown in table, find E[sin(X)]
____________________
|x   | 0 |pi/6|pi/2|
|f(x)|1/4| 1/4| 1/4|
--------------------
r(x) = sin(X)
= sin(0) * 1/4 + sin(pi/6) * 1/4 + sin(pi/2) * 1/4
= 0 + 1/2 * 1/4 + 1 * 1/4
= 1/8 + 2/8
= 3/8

The rule of the lazy statistician for continuous random variables
let X be a continuous random variable with the PDF f(x) and let Y = r(X)
E[Y] = E[r(X)] = ∫^∞_-∞ r(x)f(x) dx

applying the rule of the lazy statistician for continuous random variables
find E[X^2 - 3X + 3] with the PDF
f(x) = 2x, 0 < x < 1
in our case
r(X) = x^2 - 3x + 3
= ∫^1_0 (x^2 - 3x + 3) * 2x dx
= ∫^1_0 (2x^3 - 6x^2 + 6x) dx
= [x^4/2 - 2x^3 + 3x^2]|_0-1
= (1/2 - 2 + 3) - 0
= 3/2

example:
find E[X^3 - 2X] given the PDF
f(x) = x^2/9, 0 <= x <= 3
= ∫^3_0 (x^3 - 2x) * x^2/9 dx
= ∫^3_0 (x^5/9 - 2x^3/9) dx
= [x^6/54 - x^4/18]|_0-3
= (729/54) - (81/18)
= (729/54) - (243/54)
= (486/54)
= 9

example:
find E[e^x] given PDF
f(x) = {
	2e^(-2x), x >= 0
	0,        otherwise
}
∫^∞_0 2e^(-2x) * e^x dx
= 2 ∫^∞_0 e^(-x) dx
= -2[e^(-x)]|_0-∞
= -2(0 - 1)
= 2

proof of the linearity of expectation
although E[r(X)] != r(E[X]) in general equality is possible for some functions
for example we've seen that if a and b are real constants
E[aX + b] = aE[X] + b
we can prove this using the rule of the lazy statistician for a discrete random variable X, in this case r(X) = aX + b
E[aX + b] = Σ_x∈S (ax + b)f(x)
= Σ_x∈S ax f(x) + Σ_x∈S b f(x)
= a Σ_x∈S x f(x) + b Σ_x∈S f(x)
= aE[X] + b
a similar proof can be conducted for continuous random variables
for functions r(x) and h(x) and real coefficients a and b
E[a * r(X) + b * s(X)] = aE[r(X)] + bE[s(X)]

===================================================

The Bernoulli Distribution
A discrete random variable X follows a Bernoulli distribution if it has the following probability mass function
f(x) = {
	p,     x = 1
	1 - p, x = 0
}
the bernoulli distribution models the outcome of a Bernoulli trail an experiment with only two events "success" and "failure", we label "success" by X = 1 with a probability of p and "failure" by X = 0 with a probability of 1 - p
If a random variable X follows a Bernoulli distribution we write
X ~ Bernoulli(p)
for example suppose a fair die is rolled. Each roll of the die is a Bernoulli tail. Lets interpret a "success" as the event that we roll a 6, the outcome of a roll of the die X can be modeled as a Bernoulli random variable
X ~ Bernoulli(1/6)
probability of success (X = 1)
P(X = 1) = f(1) = 1/6
probability of failure (X = 0)
P(X = 0) = f(0) = 1 - 1/6 = 5/6

computing the probability at X=1
given that X ~ Bernoulli(0.55) compute P(X = 1)
if X ~ Bernoulli(p), then X has the PMF
f(x) = {
	p, 	   x = 1
	1 - p, x = 0
}
here X ~ Bernoulli(0.55), so the distribution of X
f(x) = {
	0.55, x = 1
	0.45, x = 0
}
P(X = 1) = f(1) = 0.55

computing the probability at X=0
if X ~ Bernoulli(p), then X has the PMF
given that X ~ Bernoulli(0.65) compute P(X = 0)
if X ~ Bernoulli(p), then X has the PMF
f(x) = {
	p, 	   x = 1
	1 - p, x = 0
}
X ~ Bernoulli(0.65) the distribution of X in this case
f(x) = {
	0.65, x = 1
	0.35, x = 0
}
P(X = 0) = f(0) = 0.35

computing the probability of an intersection
given that X ~ Bernoulli(0.7) and Y ~ Bernoulli(0.6) are independent, calculate P(X = 1 ∩ Y = 1) = P(X = 1) * P(Y = 1)
X ~ Bernoulli(0.7)
P(X = 1) = 0.7, P(X = 0) = 0.3
Y ~ Bernoulli(0.6)
P(Y = 1) = 0.6, P(Y = 0) = 0.4
therefore,
P(X = 1 ∩ Y = 1) = P(X = 1) * P(Y = 1)
= 0.7 * 0.6
= 0.42

example:
given X ~ Bernoulli(0.3) and Y ~ Bernoulli(0.4) are independent, calculate P(X = 1 ∩ Y = 0)
X ~ Bernoulli(0.3)
P(X = 1) = 0.3
Y ~ Bernoulli(0.4)
P(Y = 0) = 0.6
P(X = 1) * P(Y = 0) = (0.3)(0.6) = 0.18

example:
given X ~ Bernoulli(0.5) and Y ~ Bernoulli(0.3) are independent, calculate P(X = 0 ∩ Y = 0)
X ~ Bernoulli(0.5)
P(X = 1) = P(X = 0) = 0.5
Y ~ Bernoulli(0.3)
P(Y = 0) = 0.7
(0.5)(0.7) = 0.35

computing the probability of a union
given that X ~ Bernoulli(0.3) and Y ~ Bernoulli(0.7) are independent, calculate P(X = 0 ∪ Y = 0)
P(X = 0 ∪ Y = 0) = P(X = 0) + P(Y = 0) - P(X = 0 ∩ Y = 0)
using the multiplication law for independent events
P(X = 0 ∩ Y = 0) = P(X = 0) * P(Y = 0)
P(X = 0 ∪ Y = 0) = P(X = 0) + P(Y = 0) - P(X = 0) * P(Y = 0)
X ~ Bernoulli(0.3)
P(X = 1) = 0.3, P(X = 0) = 0.7
Y ~ Bernoulli(0.7)
P(Y = 1) = 0.7, P(Y = 0) = 0.3
P(X = 0 ∪ Y = 0) = P(X = 0) + P(Y = 0) - P(X = 0) * P(Y = 0)
= 0.7 + 0.3 - 0.7 * 0.3
= 1 - 0.21
= 0.79

example:
given that X ~ Bernoulli(0.6) and Y ~ Bernoulli(0.2) are independent, calculate P(X = 0 ∪ Y = 1)
P(X = 0 ∪ Y = 0) = P(X = 0) + P(Y = 1) - P(X = 0) * P(Y = 1)
0.4 + 0.2  - 0.4 * 0.2
= 0.6 - 0.08
= 0.52

example:
given that X ~ Bernoulli(0.5) and Y ~ Bernoulli(0.3) are independent, calculate P(X = 0 ∪ Y = 1)
0.5 + 0.3 - 0.5 * 0.3
= 0.8 - 0.15
= 0.65

===================================================

Mean and Variance of the binomial distribution
if X is a binomial random variable in which there are n experiments and p is the probability of success on each experiment, then X has the following PMF
(its hard to show by typing but here (n, x) should be denoted as a column refering to n choose x)
f(x) = (n, x)p^x(1 - p)^(n - x)
expected value, variance and SD of a binomial random variable X can be computed as so
E[X] = np
Var[X] = np(1 - p)
SD[X] = sqrt(np(1 - p))

finding the mean of a binomial random variable
A coin is biased so that the probability of it landing on heads is 0.4 the coin is tossed 7 times. What is the mean number of times the coin lands on heads?
there are n = 7 tosses
the probability of landing on heads for each toss is p = 0.4
X follows a binomial distribution where X ~ B(7, 0.4)
for a binomial random variable X, the mean is given by
E[X] = np
n is the number of experiments and p is the probability of success on each experiment
the mean of the number of times the coin lands on heads is
E[X] = np
= 7(0.4)
= 2.8

example:
A fair six-sided die is rolled 7 times, what is the mean of the number of times that the outcome of a single roll is a 2 or a 3?
p = 2/6 = 1/3
n = 7 rolls
X ~ B(7, 1/3)
E[X] = np
= 7(1/3)
= 7/3

example:
A football team has a 60% change of winning each game in a 12 game season what is the mean number of games that the team wins during a season?
p = 0.6
n = 12 games
X ~ B(12, 0.6)
E[X] = np
= 12(0.6)
= 7.2

variance of a binomial random variable
An archer practices shooting 12 arrows at a target. The archer has a 50% chance of hitting the bull's eye, what is the variance in the number of arrows that hit the bull's eye?
n = 12 shots
p = 50% = 0.5
X ~ B(12, 0.5)
for binomial random variable X the variance
Var[X] = np(1 - p)
= (12)(0.5)(1 - 0.5)
= (6)(0.5)
= 3

example:
A fair six-sided die is rolled 12 times, what is the variance of the number of times we get a number less than 3?
n = 12 times
p = 2/6 = 1/3
X ~ B(12, 1/3)
= (12)(1/3)(1 - 1/3)
= (12/3)(2/3)
= 24/9
= 8/3

example:
A cross-country running team has a 30% change of winning each run in a 20 race season. What is the variance of the number of runs that the running team wins during a season?
n = 20 races
p = 30% = 0.3
X ~ B(20, 0.3)
= (20)(0.3)(1 - 0.3)
= 6(0.7)
= 4.2

finding the standard deviation of a binomial random variable
A fair five-sided die is rolled 8 times. What is the SD of the number of times we get an even number, round to 3
n = 8 rolls
p = (2, 4) = 2/5
X ~ B(8, 2/5)
the SD is given by
SD[X] = sqrt(Var[X]) = sqrt(np(1 - p))
n is number of experiments, p is probability of success
SD to get an even number
= sqrt(8 * 2/5 * (1 - 2/5))
= sqrt(1.92)
= 1.386

example:
A fair six-sided die is rolled 10 times what is the SD of the number of times we get an even number? round 3 decimal
n = 10 rolls
p = 3/6 = 1/2
X ~ B(10, 1/2)
= (10)(1/2)(1 - 1/2)
= 5(1/2)
= sqrt(5/2)
~= 1.581 

example:
At a grocery store, it is found that 5% of eggs are cracked, if a customer purchases a carton of 12 eggs, what is the SD of the number of cracked eggs in the carton?
n = 12 eggs
p = 5% = 0.05
X ~ B(12, 0.05)
= (12)(0.05)(1 - 0.05)
= (0.6)(0.95)
= sqrt(0.57)
~= 0.755

===================================================

The discrete uniform distribution
A discrete random variable X follows a discrete uniform distribution over a set S if each value in S is equally likely.
The porbability mass function (PMF) for discrete uniform distribution on a set S
f(x) = {
	1/|S|, x ∈ S
	0,     otherwise
}
a random variable X follows a discrete uniform distribution over the set S = {s1,s2,...,sn}, we denote it as
X ~ U{s1,s2,...,sn}
we say that X is uniformly distributed over S

consider the random variable T ~ U{1,2,4} in this case S = {1,2,4}, |S| = 3 and PMF for T
f(t) = {
	1/3, t ∈ {1,2,4}
	0,   otherwise
}
cardinality of a set of consecutive numbers
|{a, a+1, a+2, ..., b}| = b - a + 1
the +1 is needed because we are including both expoints a and b in the count. Notice that |{4,5}| = 5 - 4 + 1 = 2

computing a probability at a single point
given that X ~ U{7,8,9,...,21} compute P(X = 18)
S = {7,8,9,...,21} has |S| 21 - 7 + 1 = 15 items
f(x) = {
	1/15, x = 7,8,9,...,21
	0,    otherwise
}
P(X = 18) = f(18) = 1/15

example:
given that X ~ U{3,4,5,...,22} compute P(X = 7)
S = {3,4,5,...,22} has |S| 22 - 3 + 1 = 20 items
f(x) = {
	1/20, x = 3,4,5,...,22
	0,    otherwise
}
P(X = 7) = f(7) = 1/20

example:
given that X ~ U{2,3,5,6,9} compute P(X = 3)
S = {2,3,5,6,9} has |S| 5 items
f(x) = {
	1/5, x = 2,3,5,6,9
	0,    otherwise
}
P(X = 3) = f(3) = 1/5

computing a probability on an unbounded interval
given X ~ U{5,6,7,...,14} compute P(X >= 9)
X ~ U{5,6,7,...,14} and set S = {5,6,7,...,14} has |S| = 14 - 5 + 1 = 10 items
f(x) = {
	1/10, x = 5,6,7,...,14
	0,    otherwise
}
In S the items that satisfy X >= 9 are 9,10,11,...,14 there are 14 - 9 + 1 = 6
P(X >= 9) = 6 * 1/10 = 3/5

example:
X ~ U{3,4,5,...,22} compute P(X <= 17)
S = {3,4,5,...,22} has |S| = 22 - 3 + 1 = 20 items
f(x) = {
	1/20, x = 3,4,5,...,22
	0,    otherwise
}
X <= 17 are 3,4,5,...,17 17 - 3 + 1 = 15 items
P(X <= 17) = 15 * 1/20 = 15/20 = 3/4

example:
X ~ U{2,3,5,6,9} compute P(X > 4)
S = {2,3,5,6,9} has |S| = 5 items
f(x) = {
	1/5, x = 2,3,5,6,9
	0,    otherwise
}
X > 4 = 3 items
P(X > 4) = 3 * 1/5 = 3/5

computing probability on a counded interval
X ~ U{7,8,9,...,16} compute P(8 <= X < 13)
S = {7,8,9,...,16} has |S| = 16 - 7 + 1 = 10 items
f(x) = {
	1/10, x = 7,8,9,...,16
	0,    otherwise
}
there are 5 items (8,9,10,11,12) that satisfy 8 <= X < 13
P(8 <= X < 13) = 5 * 1/10 = 1/2

example:
X ~ U{3,4,5,...,22} compute P(5 < X <= 10)
S = {3,4,5,...,22} |S| has 22 - 3 + 1 = 20 items
f(x) = {
	1/20, x = 3,4,5,...,22
	0,    otherwise
}
there are 5 items (6,7,8,9,10)
5 * 1/20 = 5/20 = 1/4

example:
X ~ U{1,3,5,6,8,10,12} compute P(3 < X < 8)
S = {1,3,5,6,8,10,12} |S| has 7 items
f(x) = {
	1/7, x = 1,3,5,6,8,10,12
	0,    otherwise
}
there are 2 items (5,6)
2 * 1/7 = 2/7

===================================================

Modeling with discrete uniform distributions
Occurs regularly in games of chance
for example, in a particular lottery, players must select a number from the set: {1,2,...,50}
A player randomly selects a number from this set such that each number has an equal probability of being selected if X represents the number the player chooses we can model this situation using a discrete uniform distribution
X ~ U{1,2,...,50}
since S = {1,2,...,50} has |S| = 50 items the probability mass function for the random variable X
f(x) = {
	1/50, x ∈ {1,2,...,50}
	0,    otherwise
}

determining when a discrete uniform distribution is appropriate
to apply a discrete uniform probability model to a situation in context
The support S of our random variable should be finite
Each x ∈ S must be equally likely
some common pitfalls
We cannot use a discrete uniform distribution to model continuous measurements
(for example weight, time and distance)
We must be wary of discrete models requiring infinitely large support
(X could be any positive integer if it represents something like number of suicides in a given month. X is hopefully 0 or a very small number but a variable with inifinite support is more appropriate)

Determining situations that can be modeled as a discrete uniform distributino
which of the following can be modeled as a discrete uniform distribution
1. The amount of time, in seconds, needed to download a game
2. the number obtained when rolling a fair 4-sided die whose sides are labeled 5,6,7,8
3. the number of rainy days in a randomly selected summer month in NY
1) cannot be modeled as a discrete uniform distribution, time is a real number and the support is not finite.
2) 2 can be modeled as a discrete uniform distribution theres a finite number of sides and the die is fair
3) 3 cannot be modeled, each possible number of rainy days is not equally likely, for example its more likely that there will be 5 rainy days than 31 rainy days
2 only works here

a situation that I thought would model but doesn't:
the number of correct answers in a multiple choice test of 10 questions where each answer is either "true" or "false" and each answer is guess randomly
why? Each number of correct answers is not equally likely for example, it is more likely to score 50% than 100%

modeling a situation in context with the discrete uniform distribution
A fair die has 9 sides labeled with numbers 1-9 whats the probability of getting a number greater than or equal to 6 on the next throw of the die
X ~ U{1,2,...,9} compute P(X >= 6)
S = {1,2,...,9} has |S| = 9 - 1 + 1 = 9
f(x) = {
	1/9, x = 1,2,...,9
	0,   otherwise
}
X >=6 are 6,7,8,9 4 elements in total
P(X >= 6) = 4 * 1/9 = 4/9

example:
In a game show, a contestant can choose a prize from any box. The boxes are labeled with number 5-15 if the contestant choosess a box at random, what is the probability that the box number they pick is less than 10?
X ~ U{5,6,7,...,15} compute P(X < 10)
S = {5,6,7,...,15} has |S| = 15 - 5 + 1 = 11 items
f(x) = {
	1/11, x = 5,6,7,...,15
	0,   otherwise
}
X < 10 are 5,6,7,8,9 5 elements in total
P(X < 10) = 5 * 1/11 = 5/11

example:
in a videogame, the number of obstacles that show up during a game can vary between 100 and 155 inclusive. Assuming that each number of the obstacles has the same probability to occur, what is the probability of having less than 130 obstacles?
X ~ U{100,101,102,...,155} P(X < 130)
S = {100,101,102,...,155} has |S| = 155 - 100 + 1 = 56 items
f(x) = {
	1/56, x = 100,101,102,...,155
	0,   otherwise
}
X < 130 is 30 elements in total (starting at 100 makes it easy)
P(X < 130) = 30 * 30/56 = 15/28

===================================================

mean and variance of discrete uniform distributions
X ~ {a,a+1,a+2,...,b}
a and b are integers, we're concerned with the special case where the support S of X contains every integer between a and b inclusive
in this case the mean value of X
E[X] = (a + b)/2
also
Var[X] = (|S|^2 - 1)/12

computing mean values in context
A fair die has 12 sides labeled with numbers 1-12 what is the expected value of a random roll of this die?
X ~ U{1,2,...,12} we want to compute E[X]
E[X] = 1+12/2 = 6.5

example:
in a game show, a contestant can choose a prize from any box the boxes are labeled 3-19 if the contestant chooses a box at random what is the expected value of the chosen number?
X ~ U{3,4,...,19}
E[X] = 3+19/2 = 22/2 = 11

example:
Grandpa is playing with his phone without his glasses and opens an app at random, the apps working on that phone are labeled with numbers 4-21 what is the expected number of the chosen app
X ~ U{4,5,...,21}
E[X] = 4+21/2 = 25/2 = 12.5

computing variances in context
A fair die has sides labeled with numbers 4-15, what is the variance of the numbers obtained when rolling this die?
X ~ U{4,5,...,15} we want Var[X]
in general if X ~ U{a,a+1,...,b}
Var[X] = (|S|^2 - 1)/12
|S| = b - a + 1
= 15 - 4 + 1 = 12
Var[X] = (12^2 - 1)/12
= 143/12

example:
in a game show a contestant can choose a prize from any box, the boxes are labeled with numbers 2-8. If contestants choose a box at random, what is the variance of the chosen numbers?
X ~ U{2,3,...,8}
|S| = 8 - 2 + 1 = 7
Var[X] = (7^2 - 1)/12
48/12 = 4

example:
in winter the max daily temp of a certain mountain location varies between -5C and 10C, inclusive, assuming that for a given winter day, each max temp value between -5C and 10C has an equal probability of occuring, what is the temp standard deviation
X ~ U{-5,-4,...,10}
|S| = 10 - (-5) + 1 = 16
Var[X] = (16^2 - 1)/12
= 255/12
= sqrt(21.25)
= 4.61

proving formula for mean
E[X] = (a + b/2)
PMF:
f(x) = {
	1/|S|, x ∈ S
	0,     otherwise
}
definition of expected value for discrete random variables
E[X] = Σ_x∈S x f(x)
= Σ_x∈S x * 1/|S|
= 1/|S| Σ_x∈S x
= 1/|S| [a + (a+1) + (a+2) + ... + b]
the term in the square brackets is an arithmetic series with the first term a and last term b, using the formula for the sum of an arithmetic series
E[X] = 1/|S| [a + (a+1) + (a+2) + ... + b]
= 1/|S| * |S|/2 * (a + b)
= 1/2 * (a + b)
= (a + b)/2

formula for variance
Var[X] = |S|^2 - 1/12
we will assume without loss of generality that the distribution of our random variable X
X ~ {1,2,3,...,|S|}
in general the variance (i.e. spread) of a random variable is unaffected by translations
Var[X + c] = Var[X]
E[X^2] = Σ_x∈S x^2 f(x)
= Σ_x∈S x^2 1/|S|
= 1/|S| * Σ_x∈S x^2
= 1/|S| *  [1^2 + 2^2 + ... + |S|^2]
make use of the following result
Σ^n_i=1 i^2 = 1/6(n)(n + 1)(2n + 1)
E[X^2] = 1/|S| * [1^2 + 2^2 + ... + |S|^2]
= 1/|S| * 1/6 * |S| * (|S| + 1)(2|S| + 1)
(|S| cancel out)
= ((|S| + 1)(2|S| + 1)/6)
next we use the results:
Var[X] = E[X^2] - (E[X])^2,  E[X] = (1 + |S|/2)
therefore
= ((|S| + 1)(2|S| + 1)/6) - (|S| + 1/2)^2
= ((|S| + 1)(2|S| + 1)/6) - (|S| + 1)^2/4
= (2((|S| + 1)(2|S| + 1))/12) - 3(|S| + 1)^2/12
= (|S| + 1/12)(2(2|S| + 1)) - 3(|S| + 1)
= (|S| + 1/12)(4|S| + 2 - 3|S| - 3)
= (|S| + 1/12)(|S| - 1)
= ((|S| + 1)(|S| - 1)/12)
= (|S|^2 - 1/12)

===================================================

A discrete random variable X follows a Poisson distribution if it has the following probability mass function
f(x) = (λ^x * e^(-λ)/x!) x = 0,1,2...
the rate parameter λ is a positive real number and e ~= 2.71828 is Euler's number
if a random variable X follows a Poisson distribution we write
X ~ Po(λ)
The Poisson distribution models the number of independent events X that occur within a fixed interval of space or time if events occur independently and at a constant average rate. The parameter λ represents the mean number of events that occur in an interval.

For example, suppose that an average of λ = 2 buses stop at a particular bus shelter every 10 min, the number of buses that stop at the shelter in a randomly selected 10-minute period can be modeled by a Poisson random variable
X ~ Po(2)
the probability that X = 3 buses will stop at the shelter in the next 10 minutes is
P(X = 3) = f(3)
= (2^3 * e^(-2)/3!)
~= 0.1804

Computing a probability at a point
X ~ Po(4), compute P(X = 2)
if X ~ Po(λ) then X has the following probability mass function
f(x) = (λ^x * e^(-λ)/x!)
X ~ Po(4) so the distribution of X in this case is
f(x) = (4^x * e^(-4)/x!)
P(X = 2) = f(2)
= (4^2 * e^(-4)/2!)
~= 0.1465

example:
X ~ Po(3) compute P(X = 5)
= (3^5 * e^(-3)/5!)
~= 0.1008

example:
X ~ Po(2) compute P(X = 3)
= (2^3 * e^(-2)/3!)
~= 0.1804

computing a probability over a bounded interval: Lower bound is zero
X ~ Po(5) compute P(X < 3)
f(x) = (5^x * e^(-5)/x!)
P(X < 3) = P(X ∈ {0,1,2})
= P(X = 0) + P(X = 1) + P(X = 2)
= f(0) + f(1) + f(2)
= (5^0 * e^(-5)/0!) + (5^1 * e^(-5)/1!) + (5^2 * e^(-5)/2!)
~= 0.1247

example:
X ~ Po(3) compute P(X < 4)
f(x) = (3^x * e^(-3)/x!)
= (3^0 * e^(-3)/0!) + (3^1 * e^(-3)/1!) + (3^2 * e^(-3)/2!) + (3^3 * e^(-3)/3!)
= 0.049787 + 0.14936 + 0.22404 + 0.22404
~= 0.6472

example:
X ~ Po(2) compute P(X <= 2)
f(x) = (2^x * e^(-2)/x!)
= (2^0 * e^(-2)/0!) + (2^1 * e^(-2)/1!) + (2^2 * e^(-2)/2!)
= 0.1353 + 0.2707 + 0.2707
~= 0.6767

computing a probability over a bounded interval: lower bound is not zero
X ~ Po(3) compute P(2 < X < 5)
f(x) = (3^x * e^(-3)/x!)
therefore
P(2 < X < 5) = P(X ∈ {3,4})
= P(X = 3) + P(X = 4)
= f(3) + f(4)
= (3^3 * e^(-3)/3!) + (3^4 * e^(-3)/4!)
~= 0.3921

example:
X ~ Po(2) compute P(5 <= X < 7)
f(x) = (2^x * e^(-2)/x!)
= (2^5 * e^(-2)/5!) + (2^6 * e^(-2)/6!)
= 0.0361 + 0.0120
~= 0.0481

example:
X ~ Po(4) compute P(1 <= X <= 3)
f(x) = (4^x * e^(-4)/x!)
= (4^1 * e^(-4)/1!) + (4^2 * e^(-4)/2!) + (4^3 * e^(-4)/3!)
= 0.0733 + 0.1465 + 0.1954
~= 0.4152

computing a probability over an unbounded interval using the complement
X ~ Po(3) compute P(X > 2)
f(x) = (3^x * e^(-3)/x!)
there are infinitely many values of X such that X > 2 we can simplify the computation by using the complement instead
P(X > 2) = 1 - P(X <= 2)
computing the compliment
P(X <= 2) = P(X ∈ {0,1,2})
= P(X = 0) + P(X = 1) + P(X = 2)
= f(0) + f(1) + f(2)
= (3^0 * e^(-3)/0!) + (3^1 * e^(-3)/1!) + (3^2 * e^(-3)/2!)
~= 0.4232
P(X > 2) = 1 - P(X <= 2)
~= 1 - 0.4232
= 0.5768

example:
X ~ Po(2) compute P(X > 3)
f(x) = (2^x * e^(-2)/x!)
P(X > 3) = 1 - P(X <= 3)
P(X <= 3) = P(X ∈ {0,1,2,3})
= (2^0 * e^(-2)/0!) + (2^1 * e^(-2)/1!) + (2^2 * e^(-2)/2!) + (2^3 * e^(-2)/3!)
= 0.1353 + 0.2707 + 0.2707 + 0.1804
= 1 - 0.8571
= 0.1429

example:
X ~ Po(5) compute P(X >= 2)
f(x) = (5^x * e^(-5)/x!)
P(X >= 2) = 1 - P(X < 2)
P(X < 2) = P(X ∈ {0,1})
= (5^0 * e^(-5)/0!) + (5^1 * e^(-5)/1!)
= 0.0067 + 0.3369
= 1 - 0.0404
~= 0.9596

Justification that the poisson distribution forms a probability distribution
for a function f(x) with support S to be a valid probability mass function for a discrete random variable X it must satisfy
0 <= f(x) <= 1 for all x in S
Σ_x∈S f(x) = 1
pdf of a Poisson random variable
f(x) = (λ^x * e^(-λ)/x!) x = 0,1,2,3...
first using the maclaurin series for the exponential function
Σ^∞_x=0 (λ^x * e^(-λ)/x!)
= e^(-λ) Σ^∞_x=0 λ^x/x!
= e^(-λ) * e^λ
= e^0
= 1
f(x) = (λ^x * e^(-λ)/x!) >= 0
for all x because λ^x, e^(-λ) and x! are all positive. Furthermore f(x) is non-negative and the sum of all values is 1 we must have f(x) <= 1 for all x therefore x <= f(x) <= 1
f(x) is a valid probability mass function

===================================================

Modeling with the poisson distribution
The shape of the graph of a poisson distribution depends on the value of λ, when λ is small the probability is concentrated at small values of x when λ is larger the probability is concentrated at larger values of x

Identifying random variables that follow a poisson distribution
which of the following is a poisson distribution?
1. The number of calls received at a telephone center over a 10 min period
2. The number of giraffes in a zoo
3. The number of pairs of shoes produced with defects in a factory in a single day.
1 and 3 both follow a poisson distribution both independent events with an interval of time
2 there are no events that occur at a constant average rate, and there is no interval of time or space.

The average tempature in a city over the course of a year is not a poisson distribution model, its not an event that occurs at a constant average rate.

The number of misspellings found in a book can be a Poisson distribution because the events occur in an interval of space (the pages of the book)

Computing the probability of a poisson random variable at some value
In a telephone excahnge an average 3 prank calls are received per day, what is the probability of receiving 5 prank calls tomorrow?
X ~ Po(3)
f(x) = (3^x * e^(-3)/x!)
the probability of receiving 5 prank calls
P(X = 5) = f(5)
= (3^5 * e^(-3)/5!)
~= 0.101

example:
The janitor at a soda bottle factory notices that on average every batch of soda bottles has 5 bottles on which the label is upside down. What is the probability that the next batch of soda bottles will have 9 bottles on which the label is upside down?
X ~ Po(5)
f(x) = (5^x * e^(-5)/x!)
= (5^9 * e^(-5)/9!)
~= 0.036

example:
Paul a friend of mine was an author of many books, he let me read a first draft of his new book, on average there have been 4 typos per page. What is the probability of finding 2 typos on the next page?
X ~ Po(4)
f(x) = (4^x * e^(-4)/x!)
= (4^2 * e^(-4)/2!)
~= 0.147

computing the probability of a poisson random variable on a bounded interval
On a particular freeway, on average 4 traffic accidents occur per week. What is the probability that there will be fewer than 3 accidents next week?
X ~ Po(4)
f(x) = (4^x * e^(-4)/x!)
P(X < 3) = P(X ∈ {0,1,2})
= (4^0 * e^(-4)/0!) + (4^1 * e^(-4)/1!) + (4^2 * e^(-4)/2!)
= e^(-4)((4^0/0!) + (4^1/1!) + (4^2/2!))
~= 0.238

example:
The janitor at a soda bottle company notices that on average every batch of 1000 soda bottles has 3 soda bottles which there is a rat inside. What is the probability that the next bach of soda bottles will have no more than 1 bottle in which a rat is found?
X ~ Po(3)
f(x) = (3^x * e^(-3)/x!)
P(X <= 1) = P(X ∈ {0,1})
= e^(-3)((3^0/0!) + (3^1/1!))
~= 0.199

example:
Maggie a friend of mine let me read her first draft of her new book, on average there have been 4 typos per page. What is the probability of finding fewer than 4 typos on the next page?
X ~ Po(4)
f(x) = (4^x * e^(-4)/x!)
P(X < 4) = P(X ∈ {0,1,2,3})
= e^(-4)((4^0/0!) + (4^1/1!) + (4^2/2!) + (4^3/3!))
~= 0.433

computing the probability of a poisson random variable on an unbounded interval
In a factory 3 machines on average suffer some failure during the course of a week, what is the probability that at least 2 machine register a fault next week?
X ~ Po(3)
f(x) = (3^x * e^(-3)/x!)
compliment of P(X >= 2)
P(X >= 2) = 1 - P(X < 2)
= P(X ∈ {0,1})
= (3^0 * e^(-3)/0!) + (3^1 * e^(-3)/1!)
~= 0.199
1 - P(X < 2)
~= 1 - 0.199
~= 0.801

example:
A janitor at a soda bottle factory notices on average every batch of 1000 soda bottles has 7 soda bottles with a rat inside. what is the probability that the next batch of soda bottles will have more than 2 bottles where a rat is found?
X ~ Po(7)
f(x) = (7^x * e^(-7)/x!)
P(X > 2) = 1 - P(X <= 2)
= P(X ∈ {0,1,2})
= e^(-7)((7^0/0!) + (7^1/1!) + (7^2/2!))
~= 0.030
1 - P(X <= 2)
~= 1 - 0.030
~= 0.970

example:
King Mathias is writing his first draft of a book, on average there have been 2 typos per page, what is the probability of finding at least 4 typos on the next page?
X ~ Po(2)
f(x) = (2^x * e^(-2)/x!)
= P(X >= 4) = 1 - P(X < 4)
= P(X ∈ {0,1,2,3})
= e^(-2)((2^0/0!) + (2^1/1!) + (2^2/2!) + (2^3/3!))
~= 0.857
~= 1 - 0.857
~= 0.143

===================================================

The continuous uniform distribution
A continuous random variable X follows a continuous uniform distribution on the interval [a,b] if all values of X are equally likely on [a,b]
the familiar statement "choose a random number between a and b" can be translated as "observe a value from the continuous uniform distribution on [a,b]"
The probability density function for the continuous uniform distribution on an interval [a,b] takes the following form
f(x) = {
	1/b - a, a <= x <= b
	0, 		 otherwise
}
The PDF of the continuous uniform distribution on [a,b] is just 1 divided by the length of the interval. This ensures that our PDF has the same value everywhere in the interval and integrates to 1
∫^b_a (1/b - a) dx = [x/b - a]|_a-b
= (b - a/b - a)
= 1
when X follows a continuous uniform distribution on the interval [a,b] we often write
X ~ U[a,b]
"X is unfiromly distributed on the interval [a,b]"

computing a "less than" probability for a uniformly distributed continuous random variable.
X is uniformly distributed on the interval [0,10] compute P(X <= 7)
f(x) = {
	1/10, 0 <= x <= 10
	0,    otherwise
}
P(X <= 7) = P(0 <= X <= 7)
= ∫^7_0 f(x) dx
= ∫^7_0 1/10 dx
= 7 - 0/10
= 7/10

example:
X is uniformly distributed on the interval [-4,3] compute P(X < -2)
f(x) = {
	1/7, -4 <= x <= 3
	0,    otherwise
}
P(X < -2) = P(-4 <= x < -2)
= ∫^-2_-4 f(x) dx
= ∫^-2_-4 1/7 dx
= -2 - (-4)/7
= 2/7

example:
X is uniformly distributed on the interval [-3,10] compute P(X <= 0)
f(x) = {
	1/13, -3 <= x <= 10
	0,    otherwise
}
P(X <= 0) = P(-3 <= x <= 0)
= ∫^0_-3 f(x) dx
= ∫^0_-3 1/13 dx
= 0 - (-3)/13
= 3/13

Computing the probability of a uniformly distributed random variable over an interval
X is uniformly distributed on the interval [-1,4] compute P(0 < X < 3)
f(x) = {
	1/5, -1 <= x <= 4
	0,   otherwise
}
P(0 < X < 3) 
= ∫^3_0 f(x) dx
= ∫^3_0 1/5 dx
= 3 - 0/5
= 3/5

example:
X is uniformly distributed on the interval [-2, 6] compute P(1 < X < 2)
f(x) = {
	1/8, 1 < x < 2
	0,   otherwise
}
= ∫^2_1 f(x) dx
= ∫^2_1 1/8 dx
= 2 - 1/8
= 1/8

example:
W is uniformly distributed on the interval [-3,4] compute P(-2 <= W <= 3)
f(w) = {
	1/7, -2 <= w <= 3
	0,   otherwise
}
= ∫^3_-2 f(w) dw
= ∫^3_-2 1/7 dw
= 3 - (-2)/7
= 5/7

computing a "greater than" probability for unfiromly distributed continuous random variable
Y is uniformly distributed on the interval [1,16] compute P(Y >= 10)
f(y) = {
	1/15, 1 <= y <= 16
	0,   otherwise
}
P(Y >= 10) = P(10 <= Y <= 16)
= ∫^16_10 f(y) dy
= ∫^16_10 1/15 dy
= 16 - 10/15
= 6/15
= 2/5

example:
Y is uniformly distributed on the interval [-3,7] compute P(Y > 3)
f(y) = {
	1/10, -3 <= y <= 7
	0,   otherwise
}
P(Y > 3) = (3 < Y <= 7)
= ∫^7_3 f(y) dy
= ∫^7_3 1/10 dy
= 7 - 3/10
= 4/10
= 2/5

example:
Y is unfiormly distributed on interval [-6,-1] compute P(Y > -5)
f(y) = {
	1/5, -6 <= y <= -1
	0,   otherwise
}
P(Y > -5) = P(-5 < Y <= -1)
= ∫^-1_-5 f(y) dy
= ∫^-1_-5 1/5 dy
= -1 - (-5)/5
= 4/5

===================================================

Mean and variance of continuous uniform distributions
the expected expected value, and variance of X
E[X] = a+b/2
Var[X] = (b-a)^2/12
the result for E[X] is simply the midpoint of the interval [a,b]

Computing the expected value of a uniformly distributed random variable
X is a random variable such that X ~ U[2,3] find E[X]
a = 2, b = 8
E[X] = 2+8/2 = 5

example:
X ~ U[-9,-3] find E[X]
= -9 + (-3) = -12
= -12/2 = -6

example:
X ~ U[-1,9] find E[X]
= 8/2 = 4

Computing the variance of a uniformly distributed random variable
X ~ U[2,10] find Var[X]
Var[X] = (10 - 2)^2/12
= 64/12 = 16/3

example:
X ~ U[-1,9] find Var[X]
Var[X] = (9 - (-1))^2/12
= 100/12
= 50/6
= 25/3

example:
X ~ U[-12,-8] find Var[X]
Var[X] = (-8 - (-12))^2/12
= 16/12 = 4/3

computing the standard deviation of a uniformly distributed random variable
X is a random variable such that X ~ U[-9,-3] find SD[X]
a = -9, b = -3
Var[X] = (-3 - (-9))^2/12
= 36/12 = 3
SD[X] = sqrt(3)
~= 1.732

example:
X ~ U[-5,4] find SD[X]
Var[X] = (4 - (-5))^2/12
= 81/12
SD[X] = sqrt(81/12)
~= 2.598

example:
X ~ U[2,15] find SD[X]
Var[X] = (15 - 2)^2/12
= 169/12
SD[X] = sqrt(169/12)
~= 3.753

deriving the formula for the mean
X ~ U[a,b] PMF
f(x) = {
	1/a-b, a <= x <= b
	0,     otherwise
}
E[X] = ∫^b_a x f(x) dx
= ∫^b_a x/b-a dx
= 1/b-a ∫^b_a x dx
= 1/b-a * [x^2/2]|_a-b
= 1/(2(b-a)) * (b^2 - a^2)
= 1/(2(b-a)) * (b - a)(b + a)
(a b-a cancels out)
= (a + b)/2

deriving the formula for variance
X ~ U[a,b] PMF
f(x) = {
	1/a-b, a <= x <= b
	0,     otherwise
}
Var[X] = E[X^2] - (E[X])^2
E[X^2] = ∫^b_a x^2 f(x) dx
= ∫^b_a x^2/b-a dx
= 1/b-a ∫^b_a x^2 dx
= 1/b-a * [x^2/3]|_a-b
= 1/(3(b-a)) * (b^3 - a^3)
= 1/(3(b-a)) * (b - a)(b^2 + ab + a^2)
(cancel out a b-a becomes)
= 1/3(b^2 + ab + a^2)
since E[X] = (b + a)/2
(E[X])^2 = 1/4(b^2 + 2ab + a^2)
Var[X] = E[X^2] - (E[X])^2
= 1/3(b^2 + ab + a^2) - 1/4(b^2 + 2ab + a^2)
= 4/12(b^2 + ab + a^2) - 3/12(b^2 + 2ab + a^2)
= 1/12(4b^2 + 4ab + 4a^2) - 1/12(3b^2 + 6ab + 3a^2)
= 1/12(b^2 - 2ab + a^2)
= (b - a)^2/12
