Our crawling logic/step:
===========================================================
from urllib.parse import urlparse
from loguru import logger
from tqdm import tqdm
from typing_extensions import Annotated
from zenml import get_step_context, step
from llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher
from llm_engineering.domain.documents import UserDocument

@step
def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], "crawled_links"]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(f"Starting to crawl {len(links)} link(s).")
    metadata = {}
    successfull_crawls = 0
    for link in tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
        step_context = get_step_context()
    step_context.add_output_metadata(output_name="crawled_links", metadata=metadata)
    logger.info(f"Successfully crawled {successfull_crawls} / {len(links)} links.")
    return links

def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    try:
        crawler.extract(link=link, user=user)
        return (True, crawler_domain)
    except Exception as e:
        logger.error(f"An error occurred while crawling: {e!s}")
        return (False, crawler_domain)

def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:
	if domain not in metadata:
	    metadata[domain] = {}
	metadata[domain]["successful"] = metadata.get(domain, {}).get("successful", 0) + successfull_crawl
	metadata[domain]["total"] = metadata.get(domain, {}).get("total", 0) + 1
	return metadata
===========================================================
CrawlerDispatcher:
===========================================================
import re
from urllib.parse import urlparse
from loguru import logger
from .base import BaseCrawler
from .custom_article import CustomArticleCrawler
from .github import GithubCrawler
from .linkedin import LinkedInCrawler
from .medium import MediumCrawler

class CrawlerDispatcher:
    def __init__(self) -> None:
        self._crawlers = {}

    @classmethod
    def build(cls) -> "CrawlerDispatcher":
        dispatcher = cls()
        return dispatcher

    # CrawlerDispatcher.build().register_linkedin().register_medium()
    def register_medium(self) -> "CrawlerDispatcher":
        self.register("https://medium.com", MediumCrawler)
        return self
    def register_linkedin(self) -> "CrawlerDispatcher":
        self.register("https://linkedin.com", LinkedInCrawler)
        return self
    def register_github(self) -> "CrawlerDispatcher":
        self.register("https://github.com", GithubCrawler)
        return self


    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[r"https://(www\.)?{}/*".format(re.escape(domain))] = crawler


    def get_crawler(self, url: str) -> BaseCrawler:
        for pattern, crawler in self._crawlers.items():
            if re.match(pattern, url):
                return crawler()
        else:
            logger.warning(f"No crawler found for {url}. Defaulting to CustomArticleCrawler.")
            return CustomArticleCrawler()
===========================================================
BaseCrawler
===========================================================
from abc import ABC, abstractmethod
class BaseCrawler(ABC):
    model: type[NoSQLBaseDocument]
    @abstractmethod
    def extract(self, link: str, **kwargs) -> None: ...
===========================================================
BaseSeleniumCrawler
===========================================================
import time
from tempfile import mkdtemp
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from llm_engineering.domain.documents import NoSQLBaseDocument
# Check if the current version of chromedriver exists
# and if it doesn't exist, download it automatically,
# then add chromedriver to path
chromedriver_autoinstaller.install()

class BaseSeleniumCrawler(BaseCrawler, ABC):
    def __init__(self, scroll_limit: int = 5) -> None:
        options = webdriver.ChromeOptions()
       
        options.add_argument("--no-sandbox")
        options.add_argument("--headless=new")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-background-networking")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument(f"--user-data-dir={mkdtemp()}")
        options.add_argument(f"--data-path={mkdtemp()}")
        options.add_argument(f"--disk-cache-dir={mkdtemp()}")
        options.add_argument("--remote-debugging-port=9226")

        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )

    # placeholders
    def set_extra_driver_options(self, options: Options) -> None:
        pass
    def login(self) -> None:
        pass

    def scroll_page(self) -> None:
        """Scroll through the LinkedIn page based on the scroll limit."""
        current_scroll = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):
                break
            last_height = new_height
            current_scroll += 1

===========================================================
next we can look at the concrete crawlers, in our case theres three
GitHubCrawler(BaseCrawler)
CustomArticleCrawler(BaseCrawler)
MediumCrawler(BaseSeleniumCrawler)

We don't have to log in to GitHub through the browser, as we can leverage Git clone functionality. this means no Selenium functionality. 


GithubCrawler

===========================================================
class GithubCrawler(BaseCrawler):
    model = RepositoryDocument
    def __init__(self, ignore=(".git", ".toml", ".lock", ".png")) -> None:
        super().__init__()
        self._ignore = ignore

    def extract(self, link: str, **kwargs) -> None:
    	old_model = self.model.find(link=link)
    	if old_model is not None:
        	logger.info(f"Repository already exists in the database: {link}")
        	return
        logger.info(f"Starting scrapping GitHub repository: {link}")
    	repo_name = link.rstrip("/").split("/")[-1]
    	local_temp = tempfile.mkdtemp()

	    try:
        	os.chdir(local_temp)
        	subprocess.run(["git", "clone", link])

        	# walks dir tree, skipping any ignore patterns
        	# removes spaces

        	repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])
	        tree = {}
	        for root, _, files in os.walk(repo_path):
	            dir = root.replace(repo_path, "").lstrip("/")
	            if dir.startswith(self._ignore):
	                continue
	            for file in files:
	                if file.endswith(self._ignore):
	                    continue
	                file_path = os.path.join(dir, file)
	                with open(os.path.join(root, file), "r", errors="ignore") as f:
	                    tree[file_path] = f.read().replace(" ", "")
	        # creates a new instance of RepositoryDocument model
	        # then saved to MongoDB
	        user = kwargs["user"]
	        instance = self.model(
	            content=tree,
	            name=repo_name,
	            link=link,
	            platform="github",
	            author_id=user.id,
	            author_full_name=user.full_name,
	        )
	        instance.save()
	    # clean up the temp directory
	    except Exception:
	        raise
	    finally:
	        shutil.rmtree(local_temp)
	    logger.info(f"Finished scrapping GitHub repository: {link}")
===========================================================
CustomArticleCrawler
This will use AsyncHtmlLoader and Html2TextTransformer
both from the langchain community

some developers avoid using LangChain in production

===========================================================
from urllib.parse import urlparse
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers.html2text import Html2TextTransformer
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseCrawler

class CustomArticleCrawler(BaseCrawler):
    model = ArticleDocument
    def extract(self, link: str, **kwargs) -> None:
        old_model = self.model.find(link=link)
        if old_model is not None:
            logger.info(f"Article already exists in the database: {link}")
            return
        # here we use both
        # AsyncHtmlLoader, and Html2TextTransformer
        # we are not in control is extracted and parsed
        # fallback where we donâ€™t have anything custom implemented

	    logger.info(f"Starting scrapping article: {link}")
	    loader = AsyncHtmlLoader([link])
	    docs = loader.load()
	    html2text = Html2TextTransformer()
	    docs_transformed = html2text.transform_documents(docs)
	    doc_transformed = docs_transformed[0]
	    # page content and meta data
        content = {
            "Title": doc_transformed.metadata.get("title"),
            "Subtitle": doc_transformed.metadata.get("description"),
            "Content": doc_transformed.page_content,
            "language": doc_transformed.metadata.get("language"),
        }

        # This parses URL to determine the platform (domain) 
        # from which article was scrapped

        parsed_url = urlparse(link)
        platform = parsed_url.netloc

        # Next save everything to MongoDB

        user = kwargs["user"]
        instance = self.model(
            content=content,
            link=link,
            platform=platform,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Finished scrapping custom article: {link}")       
===========================================================
MediumCrawler

NOTE: since we use extract and a conditional at the beginnning
for each of these crawlers this should be abstracted out as well.
===========================================================
from bs4 import BeautifulSoup
from loguru import logger
from llm_engineering.domain.documents import ArticleDocument
from .base import BaseSeleniumCrawler
class MediumCrawler(BaseSeleniumCrawler):
    model = ArticleDocument

    def set_extra_driver_options(self, options) -> None:
    	options.add_argument(r"--profile-directory=Profile 2")

    def extract(self, link: str, **kwargs) -> None:
	    old_model = self.model.find(link=link)
	    if old_model is not None:
	        logger.info(f"Article already exists in the database: {link}")
	        return
	    logger.info(f"Starting scrapping Medium article: {link}")
	    self.driver.get(link)
	    self.scroll_page()
	    # The only crawler using BeautifulSoup
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        title = soup.find_all("h1", class_="pw-post-title")
        subtitle = soup.find_all("h2", class_="pw-subtitle-paragraph")
        data = {
            "Title": title[0].string if title else None,
            "Subtitle": subtitle[0].string if subtitle else None,
            "Content": soup.get_text(),
        }

        # save to DB

        self.driver.close()
        user = kwargs["user"]
        instance = self.model(
            platform="medium",
            content=data,
            link=link,
            author_id=user.id,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(f"Successfully scraped and saved article: {link}")

Linked in is a bit longer then the other two scrapers as it uses Beautiful soup
does a bulk_insert for all posts, logins() the user, find elements based on classes which in fact may change over time but the same idea applies for scrapping setting up model and saving

Since LLM core features is scrapping there are two other popular scrapping data:
Scrapy: https://github.com/scrapy/scrapy
Crawl4Ai: https://github.com/unclecode/crawl4ai
ORMs:
FastAPI SQLModel: https://github.com/fastapi/sqlmodel
SQLAlchemy: https://www.sqlalchemy.org/

It is best practice to structure data in classes instead of dictionaries, each item is more verbose, reducing run errors. 

ODM pattern is similar to ORM but instead it simply works with NoSQL

===========================================================
NoSQLBaseDocument

Nothing special in these methods just a wrapper for
DB management, the class methods here are self explaintory
similar to mongoengine
===========================================================
import uuid
from abc import ABC
from typing import Generic, Type, TypeVar
from loguru import logger
from pydantic import UUID4, BaseModel, Field
from pymongo import errors
from llm_engineering.domain.exceptions import ImproperlyConfigured
from llm_engineering.infrastructure.db.mongo import connection
from llm_engineering.settings import settings

_database = connection.get_database(settings.DATABASE_NAME)
T = TypeVar("T", bound="NoSQLBaseDocument")

# id field is defined as a UUID4, with a default factory generating a unique UUID
class NoSQLBaseDocument(BaseModel, Generic[T], ABC):
	id: UUID4 = Field(default_factory=uuid.uuid4)
	def __eq__(self, value: object) -> bool:
	    if not isinstance(value, self.__class__):
	        return False
	    return self.id == value.id
	def __hash__(self) -> int:
	    return hash(self.id)

	# The from_mongo() and to_mongo()
	# are what it says mongoDB->class, dictionary->mongoDB
	@classmethod
	def from_mongo(cls: Type[T], data: dict) -> T:
	    if not data:
	        raise ValueError("Data is empty.")
	    id = data.pop("_id")
	    return cls(**dict(data, id=id))
	def to_mongo(self: T, **kwargs) -> dict:
	    exclude_unset = kwargs.pop("exclude_unset", False)
	    by_alias = kwargs.pop("by_alias", True)
	    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
	    if "_id" not in parsed and "id" in parsed:
	        parsed["_id"] = str(parsed.pop("id"))
	    for key, value in parsed.items():
	        if isinstance(value, uuid.UUID):
	            parsed[key] = str(value)
	    return parsed

	def save(self: T, **kwargs) -> T | None:
	    collection = _database[self.get_collection_name()]
	    try:
	        collection.insert_one(self.to_mongo(**kwargs))
	        return self
	    except errors.WriteError:
	        logger.exception("Failed to insert document.")
	        return None

	@classmethod
	def get_or_create(cls: Type[T], **filter_options) -> T:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        new_instance = cls(**filter_options)
	        new_instance = new_instance.save()
	        return new_instance
	    except errors.OperationFailure:
	        logger.exception(f"Failed to retrieve document with filter options: {filter_options}")
	        raise

	@classmethod
	def bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:
	    collection = _database[cls.get_collection_name()]
	    try:
	        collection.insert_many([doc.to_mongo(**kwargs) for doc in documents])
	        return True
	    except (errors.WriteError, errors.BulkWriteError):
	logger.error(f"Failed to insert documents of type {cls.__name__}")
	        return False

	@classmethod
	def find(cls: Type[T], **filter_options) -> T | None:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instance = collection.find_one(filter_options)
	        if instance:
	            return cls.from_mongo(instance)
	        return None
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return None

	@classmethod
	def bulk_find(cls: Type[T], **filter_options) -> list[T]:
	    collection = _database[cls.get_collection_name()]
	    try:
	        instances = collection.find(filter_options)
	        return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]
	    except errors.OperationFailure:
	        logger.error("Failed to retrieve document.")
	        return []

	@classmethod
	def get_collection_name(cls: Type[T]) -> str:
	    if not hasattr(cls, "Settings") or not hasattr(cls.Settings, "name"):
	        raise ImproperlyConfigured(
	            "Document should define an Settings configuration class with the name of the collection."
	        )
	    return cls.Settings.name

=========================================================
Lastly concrete classes that define our data categories
=========================================================
from abc import ABC
from typing import Optional
from pydantic import UUID4, Field
from .base import NoSQLBaseDocument
from .types import DataCategory
from enum import StrEnum

class DataCategory(StrEnum):
    PROMPT = "prompt"
    QUERIES = "queries"
    INSTRUCT_DATASET_SAMPLES = "instruct_dataset_samples"
    INSTRUCT_DATASET = "instruct_dataset"
    PREFERENCE_DATASET_SAMPLES = "preference_dataset_samples"
    PREFERENCE_DATASET = "preference_dataset"
    POSTS = "posts"
    ARTICLES = "articles"
	REPOSITORIES = "repositories"

class Document(NoSQLBaseDocument, ABC):
    content: dict
    platform: str
    author_id: UUID4 = Field(alias="author_id")
    author_full_name: str = Field(alias="author_full_name")

class RepositoryDocument(Document):
    name: str
    link: str
    class Settings:
        name = DataCategory.REPOSITORIES
class PostDocument(Document):
    image: Optional[str] = None
    link: str | None = None
    class Settings:
        name = DataCategory.POSTS
class ArticleDocument(Document):
    link: str
    class Settings:
        name = DataCategory.ARTICLES

class UserDocument(NoSQLBaseDocument):
    first_name: str
    last_name: str
    class Settings:
        name = "users"
    @property
    def full_name(self):
        return f"{self.first_name} {self.last_name}"
---------------------------------------------------------------------------
Ingestion code:
====================================================
from zenml import pipeline
from llm_engineering.interfaces.orchestrator.steps import feature_engineering as fe_steps

@pipeline
def feature_engineering(author_full_names: list[str]) -> None:
    raw_documents = fe_steps.query_data_warehouse(author_full_names)
    cleaned_documents = fe_steps.clean_documents(raw_documents)
     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)
    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)
    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)
    return [last_step_1.invocation_id, last_step_2.invocation_id]
====================================================
We can run feature_engineering from the command line, its easier to do this with a yaml file:

parameters:
  author_full_names:
    - Mathias
    - Nate
    - Hope

feature_engineering.with_options(config_path="â€¦/feature_engineering.yaml")()

query_data_warehouse()
====================================================
â€¦ # other imports
from zenml import get_step_context, step

@step
def query_data_warehouse(
    author_full_names: list[str],
) -> Annotated[list, "raw_documents"]:
    documents = []
    authors = []
    for author_full_name in author_full_names:
        logger.info(f"Querying data warehouse for user: {author_full_name}")
        first_name, last_name = utils.split_user_full_name(author_full_name)
        logger.info(f"First name: {first_name}, Last name: {last_name}")
        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
        authors.append(user)
        results = fetch_all_data(user)
        user_documents = [doc for query_result in results.values() for doc in query_result]
        documents.extend(user_documents)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="raw_documents", metadata=_get_metadata(documents))
    return documents
====================================================

Multi-threaded fetch_all_data() each data source will have its own GIL and run in parallel

====================================================
def fetch_all_data(user: UserDocument) -> dict[str, list[NoSQLBaseDocument]]:
    user_id = str(user.id)
    with ThreadPoolExecutor() as executor:
        future_to_query = {
            executor.submit(__fetch_articles, user_id): "articles",
            executor.submit(__fetch_posts, user_id): "posts",
            executor.submit(__fetch_repositories, user_id): "repositories",
        }
        results = {}
        for future in as_completed(future_to_query):
            query_name = future_to_query[future]
            try:
                results[query_name] = future.result()
            except Exception:
                logger.exception(f"'{query_name}' request failed.")
                results[query_name] = []
    return results
====================================================

_get_metadata():
Counts the number of documents related to each author relative to category
useful for monitoring/debugging
====================================================
def _get_metadata(documents: list[Document]) -> dict:
    metadata = {
        "num_documents": len(documents),
    }
    for document in documents:
        collection = document.get_collection_name()
        if collection not in metadata:
            metadata[collection] = {}
        if "authors" not in metadata[collection]:
            metadata[collection]["authors"] = list()
        metadata[collection]["num_documents"] = metadata[collection].get("num_documents", 0) + 1
        metadata[collection]["authors"].append(document.author_full_name)
    for value in metadata.values():
        if isinstance(value, dict) and "authors" in value:
            value["authors"] = list(set(value["authors"]))
    return metadata
====================================================

clean_documents()
CleaningDispatcher will know how to clean each data source
====================================================
@step
def clean_documents(
    documents: Annotated[list, "raw_documents"],
) -> Annotated[list, "cleaned_documents"]:
    cleaned_documents = []
    for document in documents:
        cleaned_document = CleaningDispatcher.dispatch(document)
        cleaned_documents.append(cleaned_document)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="cleaned_documents", metadata=_get_metadata(cleaned_documents))
    return cleaned_documents
====================================================

chunk_and_embed()
ChunkingDispatcher/EmbeddingDispatcher knows how to handle each source
====================================================
@step
def chunk_and_embed(
    cleaned_documents: Annotated[list, "cleaned_documents"],
) -> Annotated[list, "embedded_documents"]:
    metadata = {"chunking": {}, "embedding": {}, "num_documents": len(cleaned_documents)}
    embedded_chunks = []
    for document in cleaned_documents:
        chunks = ChunkingDispatcher.dispatch(document)
        metadata["chunking"] = _add_chunks_metadata(chunks, metadata["chunking"])
        for batched_chunks in utils.misc.batch(chunks, 10):
            batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks)
            embedded_chunks.extend(batched_embedded_chunks)
    metadata["embedding"] = _add_embeddings_metadata(embedded_chunks, metadata["embedding"])
    metadata["num_chunks"] = len(embedded_chunks)
    metadata["num_embedded_chunks"] = len(embedded_chunks)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name="embedded_documents", metadata=metadata)
    return embedded_chunks
====================================================

Metadata can save so much time when debugging.
load_to_vector_db()
====================================================
@step
def load_to_vector_db(
    documents: Annotated[list, "documents"],
) -> None:
    logger.info(f"Loading {len(documents)} documents into the vector database.")
    grouped_documents = VectorBaseDocument.group_by_class(documents)
    for document_class, documents in grouped_documents.items():
        logger.info(f"Loading documents into {document_class.get_collection_name()}")
        for documents_batch in utils.misc.batch(documents, size=4):
            try:
                document_class.bulk_insert(documents_batch)
            except Exception:
                return False
    return True
====================================================

domain-driven design (DDD) state that domain entities are the core of your application.

Pydantic for type validation at runtime makes your system more robust

We use abstract classes in case we need to easily extend to other data sources
====================================================
class CleanedDocument(VectorBaseDocument, ABC):
    content: str
    platform: str
    author_id: UUID4
    author_full_name: str
class CleanedPostDocument(CleanedDocument):
    image: Optional[str] = None
    class Config:
        name = "cleaned_posts"
        category = DataCategory.POSTS
        use_vector_index = False
class CleanedArticleDocument(CleanedDocument):
    link: str
    class Config:
        name = "cleaned_articles"
        category = DataCategory.ARTICLES
        use_vector_index = False
class CleanedRepositoryDocument(CleanedDocument):
    name: str
    link: str
    class Config:
        name = "cleaned_repositories"
        category = DataCategory.REPOSITORIES
        use_vector_index = False
====================================================
class Chunk(VectorBaseDocument, ABC):
    content: str
    platform: str
    document_id: UUID4
    author_id: UUID4
    author_full_name: str
    metadata: dict = Field(default_factory=dict)
â€¦ # PostChunk, ArticleChunk, RepositoryChunk
class EmbeddedChunk(VectorBaseDocument, ABC):
    content: str
    embedding: list[float] | None
    platform: str
    document_id: UUID4
    author_id: UUID4
    author_full_name: str
    metadata: dict = Field(default_factory=dict)
â€¦ # EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk
====================================================
class DataCategory(StrEnum):
    POSTS = "posts"
    ARTICLES = "articles"
    REPOSITORIES = "repositories"
====================================================

OVM is inspired by ORM but instead of SQL and structured data we workd with embedding and vector DBs

VectorBaseDocument supports CRUD operations ontop of Qdrant
====================================================
from pydantic import UUID4, BaseModel
from typing import Generic
from llm_engineering.infrastructure.db.qdrant import connection

T = TypeVar("T", bound="VectorBaseDocument")
class VectorBaseDocument(BaseModel, Generic[T], ABC):
    id: UUID4 = Field(default_factory=uuid.uuid4)
    @classmethod
    def from_record(cls: Type[T], point: Record) -> T:
        _id = UUID(point.id, version=4)
        payload = point.payload or {}
        attributes = {
            "id": _id,
            **payload,
        }
        if cls._has_class_attribute("embedding"):
            payload["embedding"] = point.vector or None
        return cls(**attributes)
    def to_point(self: T, **kwargs) -> PointStruct:
        exclude_unset = kwargs.pop("exclude_unset", False)
        by_alias = kwargs.pop("by_alias", True)
        payload = self.dict(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
        _id = str(payload.pop("id"))
        vector = payload.pop("embedding", {})
        if vector and isinstance(vector, np.ndarray):
            vector = vector.tolist()
        return PointStruct(id=_id, vector=vector, payload=payload)

    @classmethod
    def bulk_insert(cls: Type[T], documents: list["VectorBaseDocument"]) -> bool:
        try:
            cls._bulk_insert(documents)
        except exceptions.UnexpectedResponse:
            logger.info(
                f"Collection '{cls.get_collection_name()}' does not exist. Trying to create the collection and reinsert the documents."
            )
            cls.create_collection()
            try:
                cls._bulk_insert(documents)
            except exceptions.UnexpectedResponse:
                logger.error(f"Failed to insert documents in '{cls.get_collection_name()}'.")
                return False
        return True

    @classmethod
    def _bulk_insert(cls: Type[T], documents: list["VectorBaseDocument"]) -> None:
        points = [doc.to_point() for doc in documents]
        connection.upsert(collection_name=cls.get_collection_name(), points=points)
    
    @classmethod
    def get_collection_name(cls: Type[T]) -> str:
        if not hasattr(cls, "Config") or not hasattr(cls.Config, "name"):
            raise ImproperlyConfigured(
                "The class should define a Config class with" "the 'name' property that reflects the collection's name."
            )
        return cls.Config.name

    @classmethod
    def bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:
        try:
            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)
        except exceptions.UnexpectedResponse:
            logger.error(f"Failed to search documents in '{cls.get_collection_name()}'.")
            documents, next_offset = [], None
        return documents, next_offset

    @classmethod
    def _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:
        collection_name = cls.get_collection_name()
        offset = kwargs.pop("offset", None)
        offset = str(offset) if offset else None
        records, next_offset = connection.scroll(
            collection_name=collection_name,
            limit=limit,
            with_payload=kwargs.pop("with_payload", True),
            with_vectors=kwargs.pop("with_vectors", False),
            offset=offset,
            **kwargs,
        )
        documents = [cls.from_record(record) for record in records]
        if next_offset is not None:
            next_offset = UUID(next_offset, version=4)
        return documents, next_offset

    @classmethod
    def search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:
        try:
            documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)
        except exceptions.UnexpectedResponse:
            logger.error(f"Failed to search documents in '{cls.get_collection_name()}'.")
            documents = []
        return documents

    @classmethod
    def _search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:
        collection_name = cls.get_collection_name()
        records = connection.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            with_payload=kwargs.pop("with_payload", True),
            with_vectors=kwargs.pop("with_vectors", False),
            **kwargs,
        )
        documents = [cls.from_record(record) for record in records]
        return documents
====================================================

The dispatcher layer
CleaningDispatcher
(ChunkingDispatcher/EmbeddingDispatcher follow the same pattern)

====================================================
class CleaningDispatcher:
    cleaning_factory = CleaningHandlerFactory()
    @classmethod
    def dispatch(cls, data_model: NoSQLBaseDocument) -> VectorBaseDocument:
        data_category = DataCategory(data_model.get_collection_name())
        handler = cls.cleaning_factory.create_handler(data_category)
        clean_model = handler.clean(data_model)
        logger.info(
            "Data cleaned successfully.",
            data_category=data_category,
            cleaned_content_len=len(clean_model.content),
        )
        return clean_model
====================================================
# a match case would work here just fine

class CleaningHandlerFactory:
    @staticmethod
    def create_handler(data_category: DataCategory) -> CleaningDataHandler:
        if data_category == DataCategory.POSTS:
            return PostCleaningHandler()
        elif data_category == DataCategory.ARTICLES:
            return ArticleCleaningHandler()
        elif data_category == DataCategory.REPOSITORIES:
            return RepositoryCleaningHandler()
        else:
            raise ValueError("Unsupported data type")

====================================================

CleaningDataHandler

====================================================
# Other imports.
from typing import Generic, TypeVar

DocumentT = TypeVar("DocumentT", bound=Document)
CleanedDocumentT = TypeVar("CleanedDocumentT", bound=CleanedDocument)
class CleaningDataHandler(ABC, Generic[DocumentT, CleanedDocumentT]):
    @abstractmethod
    def clean(self, data_model: DocumentT) -> CleanedDocumentT:
        pass

====================================================
class PostCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: PostDocument) -> CleanedPostDocument:
        return CleanedPostDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(data_model.content.values())),
            # Copy the rest of the parameters from the data_model object.
        )
class ArticleCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: ArticleDocument) -> CleanedArticleDocument:
        valid_content = [content for content in data_model.content.values() if content]
        return CleanedArticleDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(valid_content)),
            platform=data_model.platform,
            link=data_model.link,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
        )
class RepositoryCleaningHandler(CleaningDataHandler):
    def clean(self, data_model: RepositoryDocument) -> CleanedRepositoryDocument:
        return CleanedRepositoryDocument(
            id=data_model.id,
            content=clean_text(" #### ".join(data_model.content.values())),
            # Copy the rest of the parameters from the data_model object.
        )
====================================================

ChunkingDataHandler

====================================================
# Other imports.
from typing import Generic, TypeVar

CleanedDocumentT = TypeVar("CleanedDocumentT", bound=CleanedDocument)
ChunkT = TypeVar("ChunkT", bound=Chunk)
 class ChunkingDataHandler(ABC, Generic[CleanedDocumentT, ChunkT]):
    @property
    def metadata(self) -> dict:
        return {
            "chunk_size": 500,
            "chunk_overlap": 50,
        }
    @abstractmethod
    def chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:
        pass
====================================================
class ArticleChunkingHandler(ChunkingDataHandler):
    @property
    def metadata(self) -> dict:
        return {
            "min_length": 1000,
            "max_length": 1000,
        }
    def chunk(self, data_model: CleanedArticleDocument) -> list[ArticleChunk]:
        data_models_list = []
        cleaned_content = data_model.content
        chunks = chunk_article(
            cleaned_content, min_length=self.metadata["min_length"], max_length=self.metadata["max_length"]
        )
        for chunk in chunks:
            chunk_id = hashlib.md5(chunk.encode()).hexdigest()
            model = ArticleChunk(
                id=UUID(chunk_id, version=4),
                content=chunk,
                platform=data_model.platform,
                link=data_model.link,
                document_id=data_model.id,
                author_id=data_model.author_id,
                author_full_name=data_model.author_full_name,
                metadata=self.metadata,
            )
            data_models_list.append(model)
        return data_models_list
====================================================

regex removes punctuation, abbrevations, until max_length is reached

====================================================
def chunk_article(text: str, min_length: int, max_length: int) -> list[str]:
    sentences = re.split(r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s", text)
    extracts = []
    current_chunk = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += sentence + " "
        else:
            if len(current_chunk) >= min_length:
                extracts.append(current_chunk.strip())
            current_chunk = sentence + " "
    if len(current_chunk) >= min_length:
        extracts.append(current_chunk.strip())
    return extracts
====================================================

The other chunking handlers use chunk_text()

RecursiveCharacterTextSplitter() from LangChain split the text
based on a given seperator or chunk size, if too long it cuts

SenteceTransformersTokenTextSplitter() considers the max input length
====================================================

# Other imports.
from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter
from llm_engineering.application.networks import EmbeddingModelSingleton

def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list[str]:
    character_splitter = RecursiveCharacterTextSplitter(separators=["\n\n"], chunk_size=chunk_size, chunk_overlap=0)
    text_split_by_characters = character_splitter.split_text(text)
    token_splitter = SentenceTransformersTokenTextSplitter(
        chunk_overlap=chunk_overlap,
        tokens_per_chunk=embedding_model.max_input_length,
        model_name=embedding_model.model_id,
    )
    chunks_by_tokens = []
    for section in text_split_by_characters:
        chunks_by_tokens.extend(token_splitter.split_text(section))
    return chunks_by_tokens
====================================================

when calling the embedding model, we want to batch as many samples as possible to optimize the inference process. GPU can parallize this process

EmbeddingDataHandler()
====================================================

# Other imports.
from typing import Generic, TypeVar, cast
from llm_engineering.application.networks import EmbeddingModelSingleton

ChunkT = TypeVar("ChunkT", bound=Chunk)
EmbeddedChunkT = TypeVar("EmbeddedChunkT", bound=EmbeddedChunk)
embedding_model = EmbeddingModelSingleton()
class EmbeddingDataHandler(ABC, Generic[ChunkT, EmbeddedChunkT]):
    """
    Abstract class for all embedding data handlers.
    All data transformations logic for the embedding step is done here
    """
    def embed(self, data_model: ChunkT) -> EmbeddedChunkT:
        return self.embed_batch([data_model])[0]
    def embed_batch(self, data_model: list[ChunkT]) -> list[EmbeddedChunkT]:
        embedding_model_input = [data_model.content for data_model in data_model]
        embeddings = embedding_model(embedding_model_input, to_list=True)
        embedded_chunk = [
            self.map_model(data_model, cast(list[float], embedding))
            for data_model, embedding in zip(data_model, embeddings, strict=False)
        ]
        return embedded_chunk
    @abstractmethod
    def map_model(self, data_model: ChunkT, embedding: list[float]) -> EmbeddedChunkT:
        pass
====================================================
class ArticleEmbeddingHandler(EmbeddingDataHandler):
    def map_model(self, data_model: ArticleChunk, embedding: list[float]) -> EmbeddedArticleChunk:
        return EmbeddedArticleChunk(
            id=data_model.id,
            content=data_model.content,
            embedding=embedding,
            platform=data_model.platform,
            link=data_model.link,
            document_id=data_model.document_id,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
            metadata={
                "embedding_model_id": embedding_model.model_id,
                "embedding_size": embedding_model.embedding_size,
                "max_input_length": embedding_model.max_input_length,
            },
        )
====================================================

Writing a wrapper over external packages is often good practice.

from sentence_transformers.SentenceTransformer import SentenceTransformer
from llm_engineering.settings import settings
from .base import SingletonMeta

class EmbeddingModelSingleton(metaclass=SingletonMeta):
    def __init__(
        self,
        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,
        device: str = settings.RAG_MODEL_DEVICE,
        cache_dir: Optional[Path] = None,
    ) -> None:
        self._model_id = model_id
        self._device = device
        self._model = SentenceTransformer(
            self._model_id,
            device=self._device,
            cache_folder=str(cache_dir) if cache_dir else None,
        )
        self._model.eval()
    @property
    def model_id(self) -> str:
        return self._model_id
    @cached_property
    def embedding_size(self) -> int:
        dummy_embedding = self._model.encode("")
        return dummy_embedding.shape[0]
    @property
    def max_input_length(self) -> int:
        return self._model.max_seq_length
    @property
    def tokenizer(self) -> AutoTokenizer:
        return self._model.tokenizer
    def __call__(
        self, input_text: str | list[str], to_list: bool = True
    ) -> NDArray[np.float32] | list[float] | list[list[float]]:
        try:
            embeddings = self._model.encode(input_text)
        except Exception:
            logger.error(f"Error generating embeddings for {self._model_id=} and {input_text=}")
            return [] if to_list else np.array([])
        if to_list:
            embeddings = embeddings.tolist()
        return embeddings
---------------------------------------------------------------------------
synthetic data pipeline
====================================================
openai==1.37.1 (interact with model)
datasets==2.20.0 (format it into a Hugging Face compatible)
tqdm==4.66.4 (visualize)
====================================================
import concurrent.futures
import json
import random
import re
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple
from datasets import Dataset
from openai import OpenAI
from pydantic import BaseModel, Field
from tqdm.auto import tqdm

def load_articles_from_json(file_path: str) -> Dataset:
    with open(file_path, "r") as file:
        data = json.load(file)
    return Dataset.from_dict(
        {
            "id": [item["id"] for item in data["artifact_data"]],
            "content": [item["content"] for item in data["artifact_data"]],
            "platform": [item["platform"] for item in data["artifact_data"]],
            "author_id": [item["author_id"] for item in data["artifact_data"]],
            "author_full_name": [item["author_full_name"] for item in data["artifact_data"]],
            "link": [item["link"] for item in data["artifact_data"]],
        }
    )

def clean_text(text):
    text = re.sub(r"[^\w\s.,!?']", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:
    extracts = []
    sentence_pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
    for article in dataset["content"]:
        cleaned_article = clean_text(article)
        sentences = re.split(sentence_pattern, cleaned_article)
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence + " "
            else:
                if len(current_chunk) >= min_length:
                    extracts.append(current_chunk.strip())
                current_chunk = sentence + " "
        if len(current_chunk) >= min_length:
            extracts.append(current_chunk.strip())
    return extracts
====================================================
instruction-answer pairs

InstructionAnswerSet
====================================================
class InstructionAnswerSet:
    def __init__(self, pairs: List[Tuple[str, str]]):
        self.pairs = pairs

    @classmethod
    def from_json(cls, json_str: str) -> 'InstructionAnswerSet':
        data = json.loads(json_str)
        pairs = [(pair['instruction'], pair['answer'])
                 for pair in data['instruction_answer_pairs']]
        return cls(pairs)

    def __iter__(self):
        return iter(self.pairs)
====================================================

generate five instruction and answer pairs for each extract

====================================================
def generate_instruction_answer_pairs(
    extract: str, client: OpenAI
) -> List[Tuple[str, str]]:
    prompt = f"""Based on the following extract, generate five instruction-answer pairs. Each instruction \
must ask to write about a specific topic contained in the context. each answer \
must provide a relevant paragraph based on the information found in the \
context. Only use concepts from the context to generate the instructions. \
Instructions must never explicitly mention a context, a system, a course, or an extract. \
Instructions must be self-contained and general. \
Answers must imitate the writing style of the context. \
Example instruction: Explain the concept of an LLM Twin. \
Example answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \
It's designed to write just like you by incorporating these elements into a language model. \
The idea is to create a digital replica of your writing habits using advanced AI techniques. \
Provide your response in JSON format with the following structure:
{{
    "instruction_answer_pairs": [
        {{"instruction": "...", "answer": "..."}},
        ...
    ]
}}
Extract:
{extract}
"""
====================================================

System prompt

parsed using the InstructionAnswerSet class
====================================================
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
        	"role": "system",
        	"content": "You are a helpful assistant who \
        	generates instruction-answer pairs based on the given context. \
        	Provide your response in JSON format.",
        },
        {
        	"role": "user",
        	"content": prompt
    	},
    ],
    response_format={"type": "json_object"},
    max_tokens=1200,
    temperature=0.7,
)
# Parse the structured output
result = InstructionAnswerSet.from_json(completion.choices[0].message.content)
# Convert to list of tuples
return result.pairs
====================================================

Creating create_instruction_dataset() to automate the process!
extracts substrings from the input dataset

====================================================
def create_instruction_dataset(
    dataset: Dataset, client: OpenAI, num_workers: int = 4
) -> Dataset:
    extracts = extract_substrings(dataset)
    instruction_answer_pairs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(generate_instruction_answer_pairs, extract, client)
            for extract in extracts
        ]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)
        ):
            instruction_answer_pairs.extend(future.result())
    instructions, answers = zip(*instruction_answer_pairs)
    return Dataset.from_dict(
        {"instruction": list(instructions), "output": list(answers)}
    )
====================================================

orchestrate the pipeline, loads raw data, creates instruction
dataset, splits into training and testing sets pushes to Hugging Face Hub

====================================================
def main(dataset_id: str) -> Dataset:
    client = OpenAI()
    # 1. Load the raw data
    raw_dataset = load_articles_from_json("cleaned_documents.json")
    print("Raw dataset:")
    print(raw_dataset.to_pandas())
    # 2. Create instructiondataset
instruction_dataset = create_instruction_dataset(raw_dataset, client)
    print("Instruction dataset:")
    print(instruction_dataset.to_pandas())
    # 3. Train/test split and export
    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)
    filtered_dataset.push_to_hub("memoryoverflow/llmtwin")
    return filtered_dataset
Dataset({
    features: ['instruction', 'output'],
    num_rows: 3335
})

---------------------------------------------------------------------------
generation pipeline:
this time we need triples (instruction, answer1, answer2)
load_articles_from_json() and clean_text() are the same as before
====================================================
import concurrent.futures
import json
import re
from typing import List, Tuple
from datasets import Dataset
from openai import OpenAI
from tqdm.auto import tqdm

class PreferenceSet:
    def __init__(self, triples: List[Tuple[str, str, str]]):
        self.triples = triples

    @classmethod
    def from_json(cls, json_str: str) -> 'PreferenceSet':
        data = json.loads(json_str)
        triples = [(triple['instruction'], triple['generated_answer'], triple['extracted_answer'])
                   for triple in data['preference_triples']]
        return cls(triples)

    def __iter__(self):
        return iter(self.triples)

	def load_articles_from_json(file_path: str) -> Dataset:
	    with open(file_path, "r") as file:
	        data = json.load(file)
	    return Dataset.from_dict(
	        {
	            "id": [item["id"] for item in data["artifact_data"]],
	            "content": [item["content"] for item in data["artifact_data"]],
	            "platform": [item["platform"] for item in data["artifact_data"]],
	            "author_id": [item["author_id"] for item in data["artifact_data"]],
	            "author_full_name": [item["author_full_name"] for item in data["artifact_data"]],
	            "link": [item["link"] for item in data["artifact_data"]],
	        }
	    )

	def clean_text(text: str) -> str:
		text = re.sub(r"[^\w\s.,!?']", " ", text)
		text = re.sub(r"\s+", " ", text)
		return text.strip()

	def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:
	    extracts = []
	    sentence_pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
	    for article in dataset["content"]:
	        cleaned_article = clean_text(article)
	        sentences = re.split(sentence_pattern, cleaned_article)
	        current_chunk = ""
	        for sentence in sentences:
	            sentence = sentence.strip()
	            if not sentence:
	                continue
	            if len(current_chunk) + len(sentence) <= max_length:
	                current_chunk += sentence + " "
	            else:
	                if len(current_chunk) >= min_length:
	                    extracts.append(current_chunk.strip())
	                current_chunk = sentence + " "
	        if len(current_chunk) >= min_length:
	            extracts.append(current_chunk.strip())
	    return extracts

	def generate_preference_triples(extract: str, client: OpenAI) -> List[Tuple[str, str, str]]:
	    prompt = f"""Based on the following extract, generate five instruction-answer triples. Each triple should consist of:
		1. An instruction asking about a specific topic in the context.
		2. A generated answer that attempts to answer the instruction based on the context.
		3. An extracted answer that is a relevant excerpt directly from the given context.
		Instructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.
		Important:
		- Ensure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.
		- Do not add any ellipsis (...) or [...]  to indicate skipped text in the extracted answer.
		- If the relevant text is not continuous, use two separate sentences from the context instead of skipping text.
		Provide your response in JSON format with the following structure:
		{{
		    "preference_triples": [
		        {{
		            "instruction": "...",
		            "generated_answer": "...",
		            "extracted_answer": "..."
		        }},
		        ...
		    ]
		}}
		    Extract:
		    {extract}
		"""
		completion = client.chat.completions.create(
	        model="gpt-4o-mini",
	        messages=[
	            {
	                "role": "system",
	                "content": "You are a helpful assistant who generates instruction-answer triples based on the given context. Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format.",
	            },
	            {"role": "user", "content": prompt},
	        ],
	        response_format={"type": "json_object"},
	        max_tokens=2000,
	        temperature=0.7,
	    )
	    result = PreferenceSet.from_json(completion.choices[0].message.content)
	    return result.triples

		def filter_short_answers(dataset: Dataset, min_length: int = 100) -> Dataset:
		    def is_long_enough(example):
		        return len(example['chosen']) >= min_length
		    return dataset.filter(is_long_enough)

		def filter_answer_format(dataset: Dataset) -> Dataset:
		    def is_valid_format(example):
		        chosen = example['chosen']
		        return (len(chosen) > 0 and
		                chosen[0].isupper() and
		                chosen[-1] in ('.', '!', '?'))
		    return dataset.filter(is_valid_format)

		def create_preference_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -> Dataset:
		    extracts = extract_substrings(dataset)
		    preference_triples = []
		    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
		        futures = [
		            executor.submit(generate_preference_triples, extract, client)
		            for extract in extracts
		        ]
		        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
		            preference_triples.extend(future.result())
		    instructions, generated_answers, extracted_answers = zip(*preference_triples)
		    return Dataset.from_dict(
		        {
		            "prompt": list(instructions),
		            "rejected": list(generated_answers),
		            "chosen": list(extracted_answers)
		        }
		    )

		def main(dataset_id: str) -> Dataset:
		    client = OpenAI()
		    # 1. Load the raw data
		    raw_dataset = load_articles_from_json("cleaned_documents.json")
		    print("Raw dataset:")
		    print(raw_dataset.to_pandas())
		    # 2. Create preference dataset
		    dataset = create_preference_dataset(raw_dataset, client)
		    print("Preference dataset:")
		    print(dataset.to_pandas())
		    # 3. Filter out samples with short answers
		    dataset = filter_short_answers(dataset)
		    # 4. Filter answers based on format
		    dataset = filter_answer_format(dataset)
		    # 5. Export
		    dataset.push_to_hub(dataset_id)
		    return dataset
---------------------------------------------------------------------------
Generating answers

from vllm import LLM, SamplingParams
from datasets import load_dataset
from tqdm.auto import tqdm
import gc

def generate_answers(model_id, dataset_name):
    dataset = load_dataset(dataset_name, split="test")

def format(sample):
        return "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Response:\n".format(sample["instruction"])
    dataset = dataset.map(lambda sample: {"prompt": format(sample)})

llm = LLM(model=model_id, max_model_len=4096)
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, min_p=0.05, max_tokens=4096)
outputs = llm.generate(dataset["prompt"], sampling_params)
# log the answers and review them later:
answers = [output.outputs[0].text for output in outputs]
dataset = dataset.add_column("answers", answers)

print(f"Uploading results for {model_id}")
dataset.push_to_hub(f"memoryoverflow/{model_id.split('/')[-1]}-results")
gc.collect()
return dataset

model_ids = [
    'mlabonne/TwinLlama-3.1-8B',
    'mlabonne/TwinLlama-3.1-8B-DPO',
    'meta-llama/Meta-Llama-3.1-8B-Instruct'
]
for model_id in model_ids:
    generate_answers(model_id, "mlabonne/llmtwin")
====================================================
Evaluate the answers

import json
from typing import List
from datasets import Dataset, load_dataset
from openai import OpenAI
from tqdm.auto import tqdm
import concurrent.futures

def evaluate_answer(
    instruction: str, answer: str, client: OpenAI
) -> dict:
    prompt = f"""You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria:
1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.
2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or academic language.

===================
Accuracy scale:
1 (Poor): Contains factual errors or misleading information
2 (Good): Mostly accurate with minor errors or omissions
3 (Excellent): Highly accurate and comprehensive
Style scale:
1 (Poor): Too formal, uses some overly complex words
2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions
3 (Excellent): Perfectly accessible language for blog/social media, uses simple but precise technical terms when necessa
====================
Example of bad style: The Llama2 7B model constitutes a noteworthy progression in the field of artificial intelligence, serving as the successor to its predecessor, the original Llama architecture.
Example of excellent style: Llama2 7B outperforms the original Llama model across multiple benchmarks.
Instruction: {instruction}
Answer: {answer}
Provide your evaluation in JSON format with the following structure:
{{
    "accuracy": {{
        "analysis": "...",
        "score": 0
    }},
    "style": {{
        "analysis": "...",
        "score": 0
    }}
}}
"""
====================
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.",
        },
        {"role": "user", "content": prompt},
    ],
    response_format={"type": "json_object"},
    max_tokens=1000,
    temperature=0.8,
)

def evaluate_batch(batch, start_index):
    client = OpenAI(api_key=OPENAI_KEY)
    return [
        (i, evaluate_answer(instr, ans, client))
        for i, (instr, ans) in enumerate(batch, start=start_index)
    ]

def evaluate_answers(model_id: str, num_threads: int = 10, batch_size: int = 5) -> Dataset:
    dataset = load_dataset(f"mlabonne/{model_id.split('/')[-1]}-results", split="all")
	batches = [
	    (i, list(zip(dataset["instruction"][i:i+batch_size], dataset["answers"][i:i+batch_size])))
	    for i in range(0, len(dataset), batch_size)
	]
    evaluations = [None] * len(dataset)
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(evaluate_batch, batch, start_index) for start_index, batch in batches]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
            for index, evaluation in future.result():
                evaluations[index] = evaluation

    if 'evaluation' in dataset.column_names:
        dataset = dataset.remove_columns(['evaluation'])
    dataset = dataset.add_column("evaluation", evaluations)

    accuracy_scores = []
    style_scores = []
    for evaluation in dataset['evaluation']:
        try:
            eval_dict = json.loads(evaluation) if isinstance(evaluation, str) else evaluation
            accuracy_score = eval_dict['accuracy']['score']
            style_score = eval_dict['style']['score']
            accuracy_scores.append(accuracy_score)
            style_scores.append(style_score)
        except (json.JSONDecodeError, KeyError, TypeError):
            accuracy_scores.append(None)
            style_scores.append(None)

    if 'accuracy' in dataset.column_names:
        dataset = dataset.remove_columns(['accuracy'])
    dataset = dataset.add_column('accuracy', accuracy_scores)

    if 'style' in dataset.column_names:
        dataset = dataset.remove_columns(['style'])
    dataset = dataset.add_column('style', style_scores)

    dataset.push_to_hub(f"memoryoverflow/{model_id.split('/')[-1]}-results")
    return dataset

model_ids = [
    'mlabonne/TwinLlama-3.1-8B',
    'mlabonne/TwinLlama-3.1-8B-DPO',
    'meta-llama/Meta-Llama-3.1-8B-Instruct'
]
for model_id in model_ids:
    evaluate_answers(model_id)
---------------------------------------------------------------------------
from abc import ABC, abstractmethod
from langchain.prompts import PromptTemplate
from pydantic import BaseModel

class PromptTemplateFactory(ABC, BaseModel):
    @abstractmethod
    def create_template(self) -> PromptTemplate:
        pass

from typing import Any
from llm_engineering.domain.queries import Query

class RAGStep(ABC):
    def __init__(self, mock: bool = False) -> None:
        self._mock = mock
    @abstractmethod
    def generate(self, query: Query, *args, **kwargs) -> Any:
        pass

from pydantic import UUID4, Field
from llm_engineering.domain.base import VectorBaseDocument
from llm_engineering.domain.types import DataCategory

class Query(VectorBaseDocument):
    content: str
    author_id: UUID4 | None = None
    author_full_name: str | None = None
    metadata: dict = Field(default_factory=dict)
class Config:
        category = DataCategory.QUERIES

    @classmethod
    def from_str(cls, query: str) -> "Query":
        return Query(content=query.strip("\n "))

    def replace_content(self, new_content: str) -> "Query":
        return Query(
            id=self.id,
            content=new_content,
            author_id=self.author_id,
            author_full_name=self.author_full_name,
            metadata=self.metadata,
        )

class EmbeddedQuery(Query):
    embedding: list[float]
    class Config:
        category = DataCategory.QUERIES

from langchain_openai import ChatOpenAI
from llm_engineering.domain.queries import Query
from llm_engineering.settings import settings
from .base import RAGStep
from .prompt_templates import QueryExpansionTemplate

class QueryExpansion(RAGStep):
    def generate(self, query: Query, expand_to_n: int) -> list[Query]:
        assert expand_to_n > 0, f"'expand_to_n' should be greater than 0. Got {expand_to_n}."
        if self._mock:
            return [query for _ in range(expand_to_n)]
        query_expansion_template = QueryExpansionTemplate()
        prompt = query_expansion_template.create_template(expand_to_n - 1)
        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=0)

        chain = prompt | model
        response = chain.invoke({"question": query})
        result = response.content

        queries_content = result.strip().split(
        	query_expansion_template.separator
    	)
        queries = [query]
        queries += [
            query.replace_content(stripped_content)
            for content in queries_content
            if (stripped_content := content.strip())
        ]
        return queries		

from langchain.prompts import PromptTemplate
from .base import PromptTemplateFactory

class QueryExpansionTemplate(PromptTemplateFactory):
    prompt: str = """You are an AI language model assistant. Your task is to generate {expand_to_n}
    different versions of the given user question to retrieve relevant documents from a vector
    database. By generating multiple perspectives on the user question, your goal is to help
    the user overcome some of the limitations of the distance-based similarity search.
    Provide these alternative questions separated by '{separator}'.
    Original question: {question}"""
    @property
    def separator(self) -> str:
        return "#next-question#"
    def create_template(self, expand_to_n: int) -> PromptTemplate:
        return PromptTemplate(
            template=self.prompt,
            input_variables=["question"],
            partial_variables={
                "separator": self.separator,
                "expand_to_n": expand_to_n,
            },
        )

query = Query.from_str("Write an article about the best types of advanced RAG methods.")
    query_expander = QueryExpansion()
    expanded_queries = query_expander.generate(query, expand_to_n=3)
    for expanded_query in expanded_queries:
        logger.info(expanded_query.content)

---------------------------------------------------------------------------
Self querying:
---------------------------------------------------------------------------
from langchain_openai import ChatOpenAI
from llm_engineering.application import utils
from llm_engineering.domain.documents import UserDocument
from llm_engineering.domain.queries import Query
from llm_engineering.settings import settings
from .base import RAGStep
from .prompt_templates import SelfQueryTemplate

class SelfQuery(RAGStep):
    def generate(self, query: Query) -> Query:
        if self._mock:
            return query
        prompt = SelfQueryTemplate().create_template()
        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=0)
        chain = prompt | model
        response = chain.invoke({"question": query})
        user_full_name = response.content.strip("\n ")
        if user_full_name == "none":
            return query
        first_name, last_name = utils.split_user_full_name(user_full_name)
        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
        query.author_id = user.id
        query.author_full_name = user.full_name
        return query

from langchain.prompts import PromptTemplate
from .base import PromptTemplateFactory

class SelfQueryTemplate(PromptTemplateFactory):
    prompt: str = """You are an AI language model assistant. Your task is to extract information from a user question.
    The required information that needs to be extracted is the user name or user id.
    Your response should consist of only the extracted user name (e.g., John Doe) or id (e.g. 1345256), nothing else.
    If the user question does not contain any user name or id, you should return the following token: none.
   
    For example:
    QUESTION 1:
    My name is Paul Iusztin and I want a post about...
    RESPONSE 1:
    Paul Iusztin
   
    QUESTION 2:
    I want to write a post about...
    RESPONSE 2:
    none
   
    QUESTION 3:
    My user id is 1345256 and I want to write a post about...
    RESPONSE 3:
    1345256
   
    User question: {question}"""
    def create_template(self) -> PromptTemplate:
        return PromptTemplate(template=self.prompt, input_variables=["question"])

query = Query.from_str("I am Mathias. Write an article about Javascript and WebGPU.")
self_query = SelfQuery()
query = self_query.generate(query)
logger.info(f"Extracted author_id: {query.author_id}")
logger.info(f"Extracted author_full_name: {query.author_full_name}")
---------------------------------------------------------------------------
Rerank process:
---------------------------------------------------------------------------
from llm_engineering.application.networks import CrossEncoderModelSingleton
from llm_engineering.domain.embedded_chunks import EmbeddedChunk
from llm_engineering.domain.queries import Query
from .base import RAGStep

class Reranker(RAGStep):
    def __init__(self, mock: bool = False) -> None:
        super().__init__(mock=mock)
        self._model = CrossEncoderModelSingleton()

    def generate(self, query: Query, chunks: list[EmbeddedChunk], keep_top_k: int) -> list[EmbeddedChunk]:
        if self._mock:
            return chunks
        # creates pairs of the query content and each chunks content
        query_doc_tuples = [(query.content, chunk.content) for chunk in chunks]
        # cross encoder model to score each pair, assessing how well the chunk matches the query
        scores = self._model(query_doc_tuples)
        # zips scores with corresponding chunks to create a scored list of tuples
        scored_query_doc_tuples = list(zip(scores, chunks, strict=False))
        # sorts list in descending order based on the scores
        scored_query_doc_tuples.sort(key=lambda x: x[0], reverse=True)
        # selects the top keep_top_k chunks
        reranked_documents = scored_query_doc_tuples[:keep_top_k]
        # extracts the chunks from the tuples
        reranked_documents = [doc for _, doc in reranked_documents]
        return reranked_documents

from sentence_transformers.cross_encoder import CrossEncoder
from .base import SingletonMeta

class CrossEncoderModelSingleton(metaclass=SingletonMeta):
    def __init__(
        self,
        model_id: str = settings.RERANKING_CROSS_ENCODER_MODEL_ID,
        device: str = settings.RAG_MODEL_DEVICE,
    ) -> None:
        """
        A singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.

    def __call__(self, pairs: list[tuple[str, str]], to_list: bool = True) -> NDArray[np.float32] | list[float]:
        scores = self._model.predict(pairs)
        if to_list:
            scores = scores.tolist()
        return scores