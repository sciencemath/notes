LAB 1
This will show the advantages of cumulative data, and how we can create queries/CTEs to high performant operations, and to easily/powerful way to aggregate the data.
--------------------------------------------

We need to dedup the table so the differences between rows are the season stats/year so we will make a new type (ignoring age):
SELECT * FROM player_seasons;
CREATE TYPE season_stats AS (
                                season INTEGER,
                                gp INTEGER,
                                pts REAL,
                                reb REAL,
                                ast REAL
                            );

now we can create a new table that should represent the data with our new type
CREATE TABLE players (
    player_name TEXT,
    height TEXT,
    college TEXT,
    country TEXT,
    draft_year TEXT,
    draft_round TEXT,
    draft_number TEXT,
    season_stats season_stats[],
    current_season INTEGER,
    PRIMARY KEY(player_name, current_season)
)
next we can create a CTE (common table expression) to combine the two tables where the player_name matches:

WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 1996
    )
SELECT * FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name


This becomes our "seed query", since our minimum season is 1996, so this is really NULL, but this won't be the case as we accumulate more and more.
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),

We can COALESCE if the values don't change
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name

After Coalescing we can cocatenate todays with yesterdays season stats:
CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULLL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        COALESCE(t.season, y.current_season + 1) as current_season

|| is concatenation
:: we can cast it to the season_stats type

WHEN t.season IS NOT NULLL THEN y.season_stats ||
this is so we don't keep adding data in case the player has been retired

ELSE y.season_stats
so we dont add a bunch of NULLs

use current season otherwise use yesterdays + 1
COALESCE(t.season, y.current_season + 1)

We can INSERT our new data into our players table:
INSERT INTO players
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 1996
    )
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.country, y.country) AS country,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number,
        CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        COALESCE(t.season, y.current_season + 1) as current_season
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name;

We run this query and the main column to focus on is the season_stats:
first stat from A.C. Green:
{(1996,83,7.2,7.9,0.8)}
Next we increase WHERE current_season = 1995, and WHERE season = 1996 by one WHERE current_season = 1996, and WHERE season = 1997, and re-run query:
SELECT * FROM players WHERE current_season = 1997;
{(1996,83,7.2,7.9,0.8),(1997,82,7.3,8.1,1.5)}
We do this a few times until and current_season = 2000 season = 2001

SELECT * FROM players WHERE current_season = 2001
(example from A.C. Green)
{(1996,83,7.2,7.9,0.8),(1997,82,7.3,8.1,1.5),(1998,50,4.9,4.6,0.5),(1999,82,5,5.9,1),(2000,82,4.5,3.8,0.5)}

Theres a gap in Michael Jordan:
{(1996,82,29.6,5.9,4.3),(1997,82,28.7,5.8,3.5),(2001,60,22.9,5.7,5.2)}

You can do some powerful stuff with the new table we can view these as 3 seperate records for Michael Jordan (UNNEST):
SELECT player_name,
	UNNEST(season_stats)::season_stats AS season_stats
FROM players
WHERE current_season = 2001
AND player_name = 'Michael Jordan'

With UNNEST the sorting will stay intack

we can wrap this in a CTE and perform actions on the CTE
WITH unnested AS (
	... (above query)
)
This uses the unnested and puts them in there corresponding columns
SELECT player_name
	(season_stats::season_stats).*
FROM unnested

This allows us to go back and forth between viewing the data compact and unraveling and we don't need to use so many joins we can use the CTE we've created.

Next we create a scoring system:
CREATE TYPE scoring_class AS ENUM ('star', 'good', 'average', 'bad');
DROP players, and re-create it with two new fields:
scoring_class scoring_class,
years_since_last_season INTEGER,
We can add to our cumlative query:

Scoring (ELSE y.scoring_class it just pulls from previous year if none this year):
CASE
    WHEN t.season IS NOT NULL THEN
        CASE WHEN t.pts > 20 THEN 'star'
            WHEN t.pts > 15 THEN 'good'
            WHEN t.pts > 10 THEN 'average'
        ELSE 'bad'
    END::scoring_class
    ELSE y.scoring_class
END,

last season (how many years off a player has had like Michael Jordan in 2000)
CASE WHEN t.season IS NOT NULL THEN 0
	ELSE y.years_since_last_season + 1
END AS years_since_last_season

We can aggregrate some data say we want to know whose made the biggest improvement in pts from their first season to their last season:
SELECT
    player_name,
    (season_stats[1]::season_stats).pts AS first_season,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts AS latest_season
FROM players
WHERE current_season = 2001

SELECT
    player_name,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts/ 
    CASE WHEN (season_stats[1]::season_stats).pts = 0 THEN 1 ELSE (season_stats[1]::season_stats).pts END,
    latest_season
FROM players
WHERE current_season = 2001

COMPLETE DUMP of the queries ran (commented out some just to save in case I needed to run them again)

-- SELECT * FROM player_seasons;
-- CREATE TYPE season_stats AS (
--                                 season INTEGER,
--                                 gp INTEGER,
--                                 pts REAL,
--                                 reb REAL,
--                                 ast REAL
--                             )
-- CREATE TYPE scoring_class AS ENUM ('star', 'good', 'average', 'bad');
-- DROP TABLE players;
-- CREATE TABLE players (
--     player_name TEXT,
--     height TEXT,
--     college TEXT,
--     country TEXT,
--     draft_year TEXT,
--     draft_round TEXT,
--     draft_number TEXT,
--     season_stats season_stats[],
--     scoring_class scoring_class,
--     years_since_last_season INTEGER,
--     current_season INTEGER,
--     PRIMARY KEY(player_name, current_season)
-- );
-- SELECT MIN(season) FROM player_seasons;
INSERT INTO players
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 2000
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 2001
    )
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.country, y.country) AS country,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number,
        CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        CASE
            WHEN t.season IS NOT NULL THEN
                CASE WHEN t.pts > 20 THEN 'star'
                    WHEN t.pts > 15 THEN 'good'
                    WHEN t.pts > 10 THEN 'average'
                ELSE 'bad'
            END::scoring_class
            ELSE y.scoring_class
        END,
        CASE WHEN t.season IS NOT NULL THEN 0
            ELSE y.years_since_last_season + 1
        END AS years_since_last_season,
        COALESCE(t.season, y.current_season + 1) as current_season
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name;

-- SELECT player_name,
--        UNNEST(season_stats)::season_stats AS season_stats
-- FROM players
-- WHERE current_season = 2001
--   AND player_name = 'Michael Jordan'

-- SELECT * FROM players WHERE current_season = 2001;

SELECT
    player_name,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts/
    CASE WHEN (season_stats[1]::season_stats).pts = 0 THEN 1 ELSE (season_stats[1]::season_stats).pts END
FROM players
WHERE current_season = 2001
AND scoring_class = 'star'
-- ORDER BY 2 DESC

The final output to continue to Lab 2:


CREATE TABLE players (
    player_name TEXT,
    height TEXT,
    college TEXT,
    country TEXT,
    draft_year TEXT,
    draft_round TEXT,
    draft_number TEXT,
    season_stats season_stats[],
    scoring_class scoring_class,
    years_since_last_active INTEGER,
    current_season INTEGER,
    is_active BOOLEAN,
    PRIMARY KEY (player_name, current_season)
);

INSERT INTO players
WITH years AS (
    SELECT *
    from generate_series(1996, 2022) as season
), p AS (
    SELECT player_name , MIN(season) as first_season
    FROM player_seasons
    GROUP BY player_name
 ), players_and_seasons as (
         SELECT *
         FROM p
            JOIN years y ON p.first_season <= y.season
), windowed AS (
         SELECT
             ps.player_name, ps.season,
             array_remove(
                             array_agg(CASE
                                           WHEN p1.season is not null then
                                               cast(row(p1.season, p1.gp, p1.pts, p1.reb, p1.ast) AS season_stats)
                         end
                                      )
                             OVER (PARTITION by ps.player_name ORDER BY COALESCE(p1.season, ps.season))
                 ,null
             )
                 as seasons
         FROM players_and_seasons ps
                  LEFT JOIN player_seasons p1
                            ON ps.player_name = p1.player_name AND ps.season = p1.season
         ORDER BY ps.player_name, ps.season
     )
        ,static AS (
    SELECT player_name,
           max(height) AS height,
           max(college) AS college,
           max(country) AS country,
           max(draft_year) AS draft_year,
           max(draft_round) AS draft_round,
           max(draft_number) AS draft_number
    FROM player_seasons ps
    GROUP BY player_name
)

SELECT
    w.player_name,
    s.height,
    s.college,
    s.country,
    s.draft_year,
    s.draft_number,
    s.draft_round,
    seasons as season_stats
        ,CASE
             WHEN (seasons[cardinality(seasons)]).pts > 20 THEN 'star'
             WHEN (seasons[cardinality(seasons)]).pts > 15 THEN 'good'
             WHEN (seasons[cardinality(seasons)]).pts > 10 THEN 'average'
             else 'bad'
        end :: scoring_class as scorring_class
        ,w.season - (seasons[cardinality(seasons)]).season as years_since_last_season
        ,w.season as current_season
        ,(seasons[cardinality(seasons)]).season = w.season as is_active
FROM windowed w
         join static s
              on w.player_name = s.player_name;


LAB 2
Building SCD Type 2 (purely idempotent)
--------------------------------------------
we're going to be tracking in our scd table:
scoring_class, and scoring_class
current_season is last (you can think of it as a date partition)

CREATE TABLE players_scd (
    player_name TEXT,
    scoring_class scoring_class,
    is_active BOOLEAN,
    start_season INTEGER,
    end_season INTEGER,
    current_season INTEGER,
    PRIMARY KEY(player_name, start_season)
);

LAG() is a window function that lets us use the previous row
AS with_previous (
	SELECT
	    player_name,
	    current_season,
	    scoring_class,
	    is_active,
	    LAG(scoring_class, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_scoring_class,
	    LAG(is_active, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_is_active
	FROM players
)


This is a good way to look at data and compare players based on previous seasons (<> does not equal)
SELECT *,
        CASE
            WHEN scoring_class <> previous_scoring_class THEN 1
            ELSE 0
        END AS scoring_class_change_indicator,
        CASE
            WHEN is_active <> previous_is_active THEN 1
            ELSE 0
        END AS is_active_change_indicator
FROM with_previous

We can combine the two change columns from the above query into one to analyse the values better
with_indicators AS (
    SELECT *,
            CASE
                WHEN scoring_class <> previous_scoring_class THEN 1
                WHEN is_active <> previous_is_active THEN 1
                ELSE 0
            END AS change_indicator
    FROM with_previous
)

Next we can sum the rows with the change_indicator, so we can aggregate the data a little better like finding min/max on a season on a streak_identifier.
 with_streaks AS (
    SELECT *,
           SUM(change_indicator)
           OVER (PARTITION BY player_name ORDER BY current_season) AS streak_identifier
    FROM with_indicators
)

Now we can easily see changes in players over time (although some common practices don't include the streak_identifier in the SELECT clause, and have an ORDER BY player_name):
SELECT player_name,
       streak_identifier,
       is_active,
       scoring_class,
       MIN(current_season) AS start_season,
       MAX(current_season) AS end_season
FROM with_streaks
GROUP BY player_name, streak_identifier, is_active, scoring_class

then we can just wrap everything with an INSERT INTO player_scd, this is powerful cause we have 2 window functions and an aggregate over the whole data set. Most of the time this is enough, but can be prone to out of memory exceptions.

Complete Postegres dump from Lab 2:

-- CREATE TABLE players_scd (
--     player_name TEXT,
--     scoring_class scoring_class,
--     is_active BOOLEAN,
--     start_season INTEGER,
--     end_season INTEGER,
--     current_season INTEGER,
--     PRIMARY KEY(player_name, start_season)
-- );
--
-- SELECT player_name, scoring_class, is_active
-- FROM players
-- WHERE current_season = 2022;
-- DROP TABLE players_scd;
-- INSERT INTO players_scd
-- WITH with_previous AS (
--     SELECT
--         player_name,
--         current_season,
--         scoring_class,
--         is_active,
--         LAG(scoring_class, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_scoring_class,
--         LAG(is_active, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_is_active
--     FROM players
--     WHERE current_season <= 2021
-- ),
-- with_indicators AS (SELECT *,
--                            CASE
--                                WHEN scoring_class <> previous_scoring_class THEN 1
--                                WHEN is_active <> previous_is_active THEN 1
--                                ELSE 0
--                                END AS change_indicator
--                     FROM with_previous
-- ),with_streaks AS (
--     SELECT *,
--            SUM(change_indicator)
--            OVER (PARTITION BY player_name ORDER BY current_season) AS streak_identifier
--     FROM with_indicators
-- )
--
-- SELECT player_name,
--        scoring_class,
--        is_active,
--        MIN(current_season) AS start_season,
--        MAX(current_season) AS end_season,
--        2021 AS current_season
-- FROM with_streaks
-- GROUP BY player_name, streak_identifier, is_active, scoring_class
-- ORDER BY player_name, streak_identifier
-- SELECT * FROM players_scd;
--
-- CREATE TYPE scd_type AS (
--     scoring_class scoring_class,
--     is_active BOOLEAN,
--     start_season INTEGER,
--     end_season INTEGER
-- );
WITH last_season_scd AS (
    SELECT * FROM players_scd
    WHERE current_season = 2021
    AND end_season = 2021
), historical_scd AS (
    SELECT
        player_name,
        scoring_class,
        is_active,
        start_season,
        end_season
        FROM players_scd
    WHERE current_season = 2021
    AND end_season < 2021
), this_season_data AS (SELECT *
                        FROM players
                        WHERE current_season = 2022
-- SELECT * FROM last_season_scd;
), unchanged_records AS (
    SELECT
        ts.player_name,
        ts.scoring_class,
        ts.is_active,
        ls.start_season,
        ts.current_season as end_season
    FROM this_season_data ts
        JOIN last_season_scd ls
        ON ls.player_name = ts.player_name
            WHERE ts.scoring_class = ls.scoring_class
            AND ts.is_active = ls.is_active
), changed_records AS (
    SELECT
        ts.player_name,
        UNNEST(ARRAY[
            ROW(
                ls.scoring_class,
                ls.is_active,
                ls.start_season,
                ls.end_season
                )::scd_type,
            ROW(
                ts.scoring_class,
                ts.is_active,
                ts.current_season,
                ts.current_season
            )::scd_type
        ]) AS records
    FROM this_season_data ts
             LEFT JOIN last_season_scd ls
                  ON ls.player_name = ts.player_name
    WHERE (ts.scoring_class <> ls.scoring_class
        OR ts.is_active <> ls.is_active)
), unnested_changed_records AS (
    SELECT player_name,
           (records::scd_type).scoring_class,
           (records::scd_type).is_active,
           (records::scd_type).start_season,
           (records::scd_type).end_season
           FROM changed_records
), new_records AS (
    SELECT
        ts.player_name,
        ts.scoring_class,
        ts.is_active,
        ts.current_season AS start_season,
        ts.current_season AS end_season
    FROM this_season_data ts
    LEFT JOIN last_season_scd ls
        ON ts.player_name = ls.player_name
    WHERE ls.player_name IS NULL
)

SELECT * FROM historical_scd

UNION ALL

SELECT * FROM unchanged_records

UNION ALL

SELECT * FROM unnested_changed_records

UNION ALL

SELECT * FROM new_records

LAB 3
Network Graph
--------------------------------------------
This will be from the same data we've been using (basketball)
the vertices, and edges table will almost always be the same schema.

CREATE TYPE vertex_type AS ENUM('player', 'team', 'game');

CREATE TABLE vertices (
    identifier TEXT,
    type vertex_type,
    properties JSON,
    PRIMARY KEY (identifier, type)
)

CREATE TYPE edge_type AS ENUM('plays_against', 'shares_team', 'plays_in', 'plays_on')

CREATE TABLE edges (
    subject_identifier TEXT,
    subject_type vertex_type,
    object_identifier TEXT,
    object_type vertex_type,
    edge_type edge_type,
    properties JSON,
    PRIMARY KEY (subject_identifier,
                subject_identifier,
                object_identifier,
                object_type,
                edge_type)
)
Sometimes theres an edge_id in the edges table it acts as:
	A global identifier for the edge (idenpendent of primary key)
	Facilitates easier querying and referencing
	referential integrity across related tables
	enables better graph traversal, optimization, and maintenance

Creating a vertex specific to games
INSERT INTO vertices
SELECT
    game_id AS identifier,
    'game'::vertex_type AS type,
    json_build_object(
        'pts_home', pts_home,
        'pts_away', pts_away,
        'winning_team', CASE WHEN home_team_wins = 1 THEN home_team_id ELSE visitor_team_id END
        ) as properties
FROM games;

ARRAY_AGG will collapse rows down by putting a multiple values in a single row usually with a GROUP BY
this gives us a list of teams a player is associated with

SELECT
    player_id AS identifier,
    MAX(player_name) AS player_name,
    COUNT(1) AS number_of_games,
    SUM(pts) AS total_points,
    ARRAY_AGG(DISTINCT team_id) AS teams
FROM game_details
GROUP BY player_id

A nice way to visualize everything, this might be how data is structured to form a JSON response:

INSERT INTO vertices
WITH players_agg AS (
    SELECT
        player_id AS identifier,
        MAX(player_name) AS player_name,
        COUNT(1) AS number_of_games,
        SUM(pts) AS total_points,
        ARRAY_AGG(DISTINCT team_id) AS teams
    FROM game_details
    GROUP BY player_id
)
SELECT
    identifier,
    'player'::vertex_type,
    json_build_object(
        'player_name', player_name,
        'number_of_games', number_of_games,
        'total_points', total_points,
        'teams', teams
    )
FROM players_agg 

Next we can create our next vertex for teams
INSERT INTO vertices
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams;

Since we have duplicated data in the teams, we need to make sure its cleaned up by de-duping the data:
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT * FROM teams_deduped WHERE row_num = 1;

so the above team insert becomes:
INSERT INTO vertices
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams_deduped
WHERE row_num = 1

We can view all of our verticies:
SELECT type, COUNT(1)
FROM vertices
GROUP BY 1

Next we can build our edges (here we de-dupe the player_id and game_id)
INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
)
SELECT
    player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    game_id AS object_identifier,
    'game'::vertex_type AS object_type,
    'plays_in'::edge_type AS edge_type,
    json_build_object(
        'start_position', start_position,
        'pts', pts,
        'team_id', team_id,
        'team_abbreviation', team_abbreviation
    ) as properties
FROM deduped
WHERE row_num = 1;

MAX(CAST(e.properties->>'pts' AS INTEGER)) casts this as an integer and gets the property out:
SELECT
    v.properties->>'player_name',
    MAX(e.properties->>'pts')
    FROM vertices v JOIN edges e
    ON e.subject_identifier = v.identifier
    AND e.subject_type = v.type
GROUP BY 1 

Creating a double edge on our vertex (a join on itself), this will show us 2 players playing with/against each other if on same team (with) or different (against):
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
), filtered AS (
    SELECT * FROM deduped
    WHERE row_num = 1
)
SELECT
    f1.player_name,
    f2.player_name,
    f1.team_abbreviation,
    f2.team_abbreviation
    FROM filtered f1
    JOIN filtered f2
    ON f1.game_id = f2.game_id
    AND f1.player_name <> f2.player_name

if we want to easily see teams with/against we can just use our enum and build a CASE clause:
CASE WHEN f1.team_abbreviation = f2.team_abbreviation
    THEN 'shares_team'::edge_type
    ELSE 'plays_against'::edge_type
END

We can now do some aggreations on the data:
COUNT(1) AS num_games,
SUM(f1.pts) AS left_points,
SUM(f2.pts) AS right_points

When creating network graphs we can end up with double edges (sometimes this is needed other times not) in our dataset we are comparing whos playing against who and we don't need to know if tim plays against bob AND bob plays against tim this is data that is redundant:
WHERE f1.player_id > f2.player_id

altogether in a CTE form:
, aggregated AS (
    SELECT
        f1.player_id,
        f1.player_name,
        f2.player_id,
        f2.player_name,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END,
        COUNT(1) AS num_games,
        SUM(f1.pts) AS left_points,
        SUM(f2.pts) AS right_points
    FROM filtered f1
             JOIN filtered f2
                  ON f1.game_id = f2.game_id
                      AND f1.player_name <> f2.player_name
    WHERE f1.player_id > f2.player_id
    GROUP BY
        f1.player_id,
        f1.player_name,
        f2.player_id,
        f2.player_name,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END
)

We have to be careful with data sometimes we can get a duplicate error when joining or aggregating, for instance a player can have the same id but different name (a player can change there name):
MAX(f1.player_name) AS subject_player_name
MAX(f2.player_name) AS object_player_name

Total numbers of points divided by games, we can get all different types of connections, and aggregate the data at a much higher level!
SELECT
    v.properties->>'player_name',
    e.object_identifier,
    CAST(v.properties->>'number_of_games' AS REAL)/
    CASE WHEN CAST(v.properties->>'total_points' AS REAL) = 0 THEN 1
            ELSE CAST(v.properties->>'total_points' AS REAL)
    END,
    e.properties->>'subject_points',
    e.properties->>'num_games'
    FROM vertices v JOIN edges e
    ON v.identifier = e.subject_identifier
    AND v.type = e.subject_type
WHERE e.object_type = 'player'::vertex_type


Complete Postegres dump from Lab 3:

CONSOLE 1:

CREATE TYPE vertex_type AS ENUM('player', 'team', 'game');

CREATE TABLE vertices (
    identifier TEXT,
    type vertex_type,
    properties JSON,
    PRIMARY KEY (identifier, type)
)

CREATE TYPE edge_type AS ENUM('plays_against', 'shares_team', 'plays_in', 'plays_on')

CREATE TABLE edges (
    subject_identifier TEXT,
    subject_type vertex_type,
    object_identifier TEXT,
    object_type vertex_type,
    edge_type edge_type,
    properties JSON,
    PRIMARY KEY (subject_identifier,
                subject_type,
                object_identifier,
                object_type,
                edge_type)
)

CONSOLE 2:

INSERT INTO vertices
SELECT
    game_id AS identifier,
    'game'::vertex_type AS type,
    json_build_object(
        'pts_home', pts_home,
        'pts_away', pts_away,
        'winning_team', CASE WHEN home_team_wins = 1 THEN home_team_id ELSE visitor_team_id END
        ) as properties
FROM games;

INSERT INTO vertices
WITH players_agg AS (
    SELECT
        player_id AS identifier,
        MAX(player_name) AS player_name,
        COUNT(1) AS number_of_games,
        SUM(pts) AS total_points,
        ARRAY_AGG(DISTINCT team_id) AS teams
    FROM game_details
    GROUP BY player_id
)
SELECT
    identifier,
    'player'::vertex_type,
    json_build_object(
        'player_name', player_name,
        'number_of_games', number_of_games,
        'total_points', total_points,
        'teams', teams
    )
FROM players_agg;

SELECT * FROM teams;

INSERT INTO vertices
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams_deduped
WHERE row_num = 1

CONSOLE 3:

SELECT
    v.properties->>'player_name',
    e.object_identifier,
    CAST(v.properties->>'number_of_games' AS REAL)/
    CASE WHEN CAST(v.properties->>'total_points' AS REAL) = 0 THEN 1
            ELSE CAST(v.properties->>'total_points' AS REAL)
    END,
    e.properties->>'subject_points',
    e.properties->>'num_games'
    FROM vertices v JOIN edges e
    ON v.identifier = e.subject_identifier
    AND v.type = e.subject_type
WHERE e.object_type = 'player'::vertex_type

SELECT
    v.properties->>'player_name',
    MAX(CAST(e.properties->>'pts' AS INTEGER))
    FROM vertices v JOIN edges e
    ON e.subject_identifier = v.identifier
    AND e.subject_type = v.type
GROUP BY 1
ORDER BY 2 DESC

INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
), filtered AS (
    SELECT * FROM deduped
    WHERE row_num = 1
), aggregated AS (
    SELECT
        f1.player_id AS subject_player_id,
        f2.player_id AS object_player_id,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
        END AS edge_type,
        MAX(f1.player_name) AS subject_player_name,
        MAX(f2.player_name) AS object_player_name,
        COUNT(1) AS num_games,
        SUM(f1.pts) AS subject_points,
        SUM(f2.pts) AS object_points
    FROM filtered f1
             JOIN filtered f2
                  ON f1.game_id = f2.game_id
                      AND f1.player_name <> f2.player_name
    WHERE f1.player_id > f2.player_id
    GROUP BY
        f1.player_id,
        f2.player_id,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END
)
SELECT
    subject_player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    object_player_id AS object_identifier,
    'player'::vertex_type AS object_type,
    edge_type AS edge_type,
    json_build_object(
        'num_games', num_games,
        'subject_points', subject_points,
        'object_points', object_points
    )
FROM aggregated

INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
)
SELECT
    player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    game_id AS object_identifier,
    'game'::vertex_type AS object_type,
    'plays_in'::edge_type AS edge_type,
    json_build_object(
        'start_position', start_position,
        'pts', pts,
        'team_id', team_id,
        'team_abbreviation', team_abbreviation
    ) as properties
FROM deduped
WHERE row_num = 1;

SELECT type, COUNT(1)
FROM vertices
GROUP BY 1


LAB 4
Fundamentals of Fact Data
--------------------------------------------

Lets explore the data from game_details:
SELECT
    game_id, team_id, player_id, COUNT(1)
FROM game_details
GROUP BY 1,2,3
HAVING COUNT(1) > 1
This will show us that there are duplicates between the three columns

We can easily dedupe the data:
WITH deduped AS (
    SELECT
        *, ROW_NUMBER() OVER(PARTITION BY game_id, team_id, player_id) as row_num
    FROM game_details
)
SELECT *
FROM deduped
WHERE row_num = 1
The problem with this data is that there is no "when"

We can get the "where" from our games table and join by game_id:
WITH deduped AS (
    SELECT
        g.game_date_est,
        gd.*,
        ROW_NUMBER() OVER(PARTITION BY gd.game_id, team_id, player_id ORDER BY g.game_date_est) as row_num
    FROM game_details gd
        JOIN games g on gd.game_id = g.game_id
)
SELECT * FROM deduped
WHERE row_num = 1

Some columns like comments contian familiar phrases like DNP (did not play) this can be useful for later analysis, we can make this more readable by creating a new dimension with COALESCE and POSITION:
COALESCE(POSITION('DNP' in comment), 0) > 0 AS dim_did_not_play,
COALESCE(POSITION('DND' in comment), 0) > 0 AS dim_did_not_dress,
COALESCE(POSITION('NWT' in comment), 0) > 0 AS dim_not_with_team,
now column comment can be removed from selecting it because we extracted everything we need.

You have to always question the columns you are creating useful, for instance we have a string column for min formated as 00:00 we can transform this into something more useful:
SPLIT_PART(min, ':', 1) AS minutes,
SPLIT_PART(min, ':', 2) AS seconds,
We can do better and have a combined column:
CAST(SPLIT_PART(min, ':', 1) AS REAL)
    + CAST(SPLIT_PART(min, ':', 2) AS REAL)/60
    AS minutes,

Sometimes a naming convention is to prefix dimensions with an m_ standing for measurable, we can perform operations on these such as (aggregate):
m_fga INTEGER
(measurable field goal attempt)
dim_ columns that we can filter by and group by on:
dim_player_id

The whole point of fact building data sets is that we can answer questions which would normally be a pain to figure out in a query:
SELECT dim_player_name,
       COUNT(1) AS num_games,
       COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS bailed_num,
       CAST(COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS REAL)/COUNT(1) AS bail_pct
FROM fct_game_details
    GROUP BY 1
ORDER BY 4 DESC
This shows how many times a player didn't show up, and gives a percentage based on how many games they've played
A great way to aggregate data!

Can see who has not shown up to games when played at home:
SELECT dim_player_name,
       dim_is_playing_at_home,
       COUNT(1) AS num_games,
       SUM(m_pts) AS total_points,
       COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS bailed_num,
       CAST(COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS REAL)/COUNT(1) AS bail_pct
FROM fct_game_details
    GROUP BY 1,2
ORDER BY 6 DESC

COMPLETE LAB 4 dump:
-- DROP TABLE fct_game_details
INSERT INTO fct_game_details
WITH deduped AS (
    SELECT
        g.game_date_est,
        g.season,
        g.home_team_id,
        gd.*,
        ROW_NUMBER() OVER(PARTITION BY gd.game_id, team_id, player_id ORDER BY g.game_date_est) as row_num
    FROM game_details gd
        JOIN games g on gd.game_id = g.game_id
)
SELECT
    game_date_est AS dim_game_date,
    season AS dim_season,
    team_id AS dim_team_id,
    player_id AS dim_player_id,
    player_name AS dim_player_name,
    start_position AS dim_start_position,
    team_id = home_team_id AS dim_is_playing_at_home,
    COALESCE(POSITION('DNP' in comment), 0) > 0 AS dim_did_not_play,
    COALESCE(POSITION('DND' in comment), 0) > 0 AS dim_did_not_dress,
    COALESCE(POSITION('NWT' in comment), 0) > 0 AS dim_not_with_team,
    CAST(SPLIT_PART(min, ':', 1) AS REAL)
        + CAST(SPLIT_PART(min, ':', 2) AS REAL)/60
        AS m_minutes,
    fgm AS m_fgm,
    fga AS m_fga,
    fg3m AS m_fg3m,
    fg3a AS m_fg3a,
    ftm AS m_ftm,
    fta AS m_fta,
    oreb AS m_oreb,
    dreb AS m_dreb,
    reb AS m_reb,
    ast AS m_ast,
    stl AS m_stl,
    blk AS m_blk,
    "TO" AS m_turnovers,
    pf AS m_pf,
    pts AS m_pts,
    plus_minus AS m_plus_minus
FROM deduped
WHERE row_num = 1

CREATE TABLE fct_game_details (
    dim_game_date DATE,
    dim_season INTEGER,
    dim_team_id INTEGER,
    dim_player_id INTEGER,
    dim_player_name TEXT,
    dim_start_position TEXT,
    dim_is_playing_at_home BOOLEAN,
    dim_did_not_play BOOLEAN,
    dim_did_not_dress BOOLEAN,
    dim_not_with_team BOOLEAN,
    m_minutes REAL,
    m_fgm INTEGER,
    m_fga INTEGER,
    m_fg3m INTEGER,
    m_fg3a INTEGER,
    m_ftm INTEGER,
    m_fta INTEGER,
    m_oreb INTEGER,
    m_dreb INTEGER,
    m_reb INTEGER,
    m_ast INTEGER,
    m_stl INTEGER,
    m_blk INTEGER,
    m_turnovers INTEGER,
    m_pf INTEGER,
    m_pts INTEGER,
    m_plus_minus INTEGER,
    PRIMARY KEY (dim_game_date, dim_team_id, dim_player_id)
)

-- SELECT * FROM fct_game_details;
-- SELECT
--     t.*,
--     gd.*
-- FROM fct_game_details gd
-- JOIN teams t ON t.team_id = gd.dim_team_id
-- SELECT dim_player_name,
--        COUNT(1) AS num_games,
--        COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS bailed_num,
--        CAST(COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS REAL)/COUNT(1) AS bail_pct
-- FROM fct_game_details
--     GROUP BY 1
-- ORDER BY 4 DESC
SELECT dim_player_name,
       dim_is_playing_at_home,
       COUNT(1) AS num_games,
       SUM(m_pts) AS total_points,
       COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS bailed_num,
       CAST(COUNT(CASE WHEN dim_not_with_team THEN 1 END) AS REAL)/COUNT(1) AS bail_pct
FROM fct_game_details
    GROUP BY 1,2
ORDER BY 6 DESC


LAB 5
Datalist date type
--------------------------------------------
We first want to setup a cumulative table for our dates
CREATE TABLE users_cumulated (
    user_id BIGINT,
    dates_active DATE[],
    date DATE,
    PRIMARY KEY (user_id, date)
)
dates_active will be past dates indicating whether a user was active that day, date is current date

We have a t as today and y as yesterday, we can retrieve one day by:
COALESCE(t.date_active, y.date + INTERVAL '1 day') AS date

To accumulate dates:
CASE WHEN y.dates_active IS NULL
    THEN ARRAY[t.date_active]
    ELSE ARRAY[t.date_active] || y.dates_active
END AS dates_active,

but we add:
WHEN t.date_active IS NULL THEN y.dates_active
after the THEN because we don't want to keep accumulating a NULL list

Here is the accumlative dates final, if theres already an array in place concat that with yesterdays dates_active
CASE WHEN y.dates_active IS NULL
    THEN ARRAY[t.date_active]
    WHEN t.date_active IS NULL THEN y.dates_active
    ELSE ARRAY[t.date_active] || y.dates_active
END AS dates_active,

we keep running INSERT INTO and change yesterdays from users_cumulated date by 1 day:
WHERE date = DATE('2023-01-01')
and we must change today as well from events:
DATE(CAST(event_time AS TIMESTAMP)) = DATE('2023-01-02')
then we can look at the dates_active for these two dates (yesterday and todays):
SELECT * FROM users_cumulated
WHERE date = DATE('2023-01-02')
if there are two days in days_active that user was active 2 days, same for one day.

Do this 30 times by changing the dates then we can see all activity for the whole month:
SELECT * FROM users_cumulated
WHERE date = DATE('2023-01-31') 

@> is contains
SELECT '{"name": "Mathias", "age": 30}'::jsonb @> '{"name": "Mathias"}'::jsonb;
will return true
SELECT '{1,2,3,4}'::int[] @> '{2,3}'::int[];
will return true

When we turn the dates into bits the left most bit is the most recent.

We can generate series with a builtin function:
SELECT * FROM generate_series(DATE('2023-01-01'), DATE('2023-01-31'), INTERVAL '1 day')

CROSS JOIN
will make every combination for what your joining simple example:
|id|color|             |id| size |     |color|size  |
| 1| red |  CROSS JOIN | 1| small|     |red  |small |
| 2| blue|			   | 2|medium|  =  |red  |medium|
| 3|green|			   | 3|large |     |red  |large |
----------             -----------     |blue |small |
                                       |blue |medium|
                                       |blue |large |
                                       |green|small |
                                       |green|medium|
                                       |green|large |
                                       --------------
In our case we CROSS JOIN with our generated series (looking at just one user for clarity):
SELECT *
FROM users CROSS JOIN series
WHERE user_id = '137925124111668560'
Notice every row has been duplicated but the series_date is different as it pertains all 31 days.

We can now put the @> operator to use, now we can see which active_dates are in the series date:
SELECT dates_active @> ARRAY [DATE(series_date)]

get the days between the current date from the 31st and the series date 1st:
date - DATE(series_date)

Convert days that were active into a power of 2's (binary format):
POW(2, 32 - (date - DATE(series_date)))
This way when we sum them up we can get the history of the ones and zeros

Then this can be CASTed as BIT(32) and CAST the POW as BIGINT:
CASE WHEN
    dates_active @> ARRAY [DATE(series_date)]
THEN CAST(POW(2, 32 - (date - DATE(series_date))) AS BIGINT)
    ELSE 0
END AS placeholder_int_value,

We can now use the BIT(32) on our placeholder_int_value:
CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))
with this we can see that the 1's correspond to dates they were active and 0 not active

How many days active from our bit column:
BIT_COUNT(CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32)))

See if a user was active in the last 7 days we can & gate our values (both values have to be 1's to output a 1 otherwise 0)
CAST('11111110000000000000000000000000' AS BIT(32)) &
	CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))

Then we can output more readable values by wrapping it
BIT_COUNT(CAST('11111110000000000000000000000000' AS BIT(32)) &
	CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))) > 0 
		AS dim_is_weekly_active
You can also do some other operations with this to further your understanding of the data such as daily active:
Get our business questions answered by using binary!
daily active:
BIT_COUNT(CAST('10000000000000000000000000000000' AS BIT(32)) &
          CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))) > 0
    AS dim_is_daily_active

SQL DUMP console 1

-- DROP TABLE users_cumulated
INSERT INTO users_cumulated
WITH yesterday AS (
    SELECT
        *
    FROM users_cumulated
    WHERE date = DATE('2023-01-30')
), today AS (
    SELECT
        CAST(user_id AS TEXT) AS user_id,
        DATE(CAST(event_time AS TIMESTAMP)) AS date_active
    FROM events
    WHERE
        DATE(CAST(event_time AS TIMESTAMP)) = DATE('2023-01-31')
        AND user_id IS NOT NULL
        GROUP BY user_id, DATE(CAST(event_time AS TIMESTAMP))
)

SELECT
    COALESCE(t.user_id, y.user_id) AS user_id,
    CASE WHEN y.dates_active IS NULL
        THEN ARRAY[t.date_active]
        WHEN t.date_active IS NULL THEN y.dates_active
        ELSE ARRAY[t.date_active] || y.dates_active
    END AS dates_active,
    COALESCE(t.date_active, y.date + INTERVAL '1 day') AS date
    FROM today t
    FULL OUTER JOIN yesterday y
ON t.user_id = y.user_id

SELECT * FROM generate_series(DATE('2023-01-01'), DATE('2023-01-31'), INTERVAL '1 day')

SELECT * FROM users_cumulated
WHERE date = DATE('2023-01-31')

-- CREATE TABLE users_cumulated (
--     user_id TEXT,
--     dates_active DATE[],
--     date DATE,
--     PRIMARY KEY (user_id, date)
-- )

SQL DUMP console 2

WITH users AS (
    SELECT * FROM users_cumulated
    WHERE date = DATE('2023-01-31')
), series AS (
    SELECT *
    FROM generate_series(DATE('2023-01-01'), DATE('2023-01-31'), INTERVAL '1 day') AS series_date
), place_holder_ints AS (
    SELECT
        CASE WHEN
            dates_active @> ARRAY [DATE(series_date)]
        THEN CAST(POW(2, 32 - (date - DATE(series_date))) AS BIGINT)
            ELSE 0
        END AS placeholder_int_value,
        *
    FROM users CROSS JOIN series
)
SELECT
    user_id,
    CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32)),
    BIT_COUNT(CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))) > 0
        AS dim_is_monthly_active,
    BIT_COUNT(CAST('11111110000000000000000000000000' AS BIT(32)) &
              CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))) > 0
        AS dim_is_weekly_active,
    BIT_COUNT(CAST('10000000000000000000000000000000' AS BIT(32)) &
              CAST(CAST(SUM(placeholder_int_value) AS BIGINT) AS BIT(32))) > 0
        AS dim_is_daily_active
FROM place_holder_ints
GROUP BY user_id

-- SELECT * FROM users_cumulated
-- WHERE date = DATE('2023-01-31')


LAB 6
Building reduced facts
--------------------------------------------
Creating our metrics
CREATE TABLE array_metrics (
    user_id NUMERIC,
    month_start DATE,
    metric_name TEXT,
    metric_array REAL[],
    PRIMARY KEY (user_id, month_start, metric_name)
)

Get our daily aggregate (lets use jan 1st for now)
WITH daily_aggregate AS (
    SELECT
        user_id,
        DATE(event_time) AS date,
        COUNT(1) AS num_site_hits
    FROM events
    WHERE DATE(event_time) = DATE('2023-01-01')
    AND user_id IS NOT NULL
    GROUP BY user_id, DATE(event_time)
)
SELECT * FROM daily_aggregate

we can build yesterdays data:
, yesterday_array AS (
    SELECT * FROM array_metrics
    WHERE month_start = DATE('2023-01-01')
)

Theres a built in function to just use the first of the month:
DATE_TRUNC('month', da.date)
if da.date = '2024-11-15' this would return '2024-11-01'

if we need to insert new rows into a table, but if a conflict occurs (i.e., the row already exists based on certain unique constraints), it can update the existing row instead.
ON CONFLICT (user_id, month_start, metric_name)
DO
    UPDATE SET metric_array = EXCLUDED.metric_array;
if a conflict occurs (a row with existing values)
the metric_array of that row will be updated to the value from the insert attempt, updating the metrics for that user and month.

Creating our metric array:
CASE WHEN ya.metric_array IS NOT NULL THEN
    ya.metric_array || ARRAY[COALESCE(da.num_site_hits, 0)]
WHEN ya.metric_array IS NULL THEN ARRAY_FILL(0, ARRAY[date - DATE(DATE_TRUNC('month', date))]) || ARRAY[COALESCE(da.num_site_hits, 0)]
END AS metric_array

if a user signs up on the 3rd day of the month we need a way to fill in our metric array with 0's at the beginning because the start of the array will denote the 1st of month 2nd item in array the 2nd day of the month etc.
ARRAY_FILL(0, ARRAY[COALESCE(date - DATE(DATE_TRUNC('month', date)), 0)])

We need to wrap this in a date because DATE_TRUNC returns a timestamp:
DATE(DATE_TRUNC('month', date)

If we run an insert twice by changing:
WHERE DATE(event_time) = DATE('2023-01-02')
we will have two days of data and if a user didn't sign up till the 2nd we will have the metric array start with a 0

We can prove that all users have at least 2 items (from running our insert twice on 2 days) in their metric_array by running a query:
SELECT cardinality(metric_array), COUNT(1)
    FROM array_metrics
GROUP BY 1

Get total sum for all days (in this case we ran the day query above 3 times)
SELECT
    metric_name,
    month_start,
    ARRAY[SUM(metric_array[1]),
          SUM(metric_array[2]),
          SUM(metric_array[3])
        ] AS summed_array
    FROM array_metrics
GROUP BY metric_name, month_start 

We only explode the array after everything has been aggregated
Monthly -> Daily aggregates
(index - 1 postgres likes to index starting at 1)
this will put our aggregated data (array) into the array into individual rows calculating the individual day total
SELECT
    metric_name,
    month_start + CAST(CAST(index - 1 AS TEXT) || ' day' AS INTERVAL),
    elem AS value
    FROM agg
    CROSS JOIN UNNEST(agg.summed_array)
        WITH ORDINALITY AS a(elem, index)

This will help us bring in other data if needed.

Complete LAB 6 DUMP:
Console 1:
-- CREATE TABLE array_metrics (
--     user_id NUMERIC,
--     month_start DATE,
--     metric_name TEXT,
--     metric_array REAL[],
--     PRIMARY KEY (user_id, month_start, metric_name)
-- )

-- DELETE FROM array_metrics
-- SELECT * FROM array_metrics;
-- SELECT cardinality(metric_array), COUNT(1)
--     FROM array_metrics
-- GROUP BY 1

INSERT INTO array_metrics
WITH daily_aggregate AS (
    SELECT
        user_id,
        DATE(event_time) AS date,
        COUNT(1) AS num_site_hits
    FROM events
    WHERE DATE(event_time) = DATE('2023-01-03')
    AND user_id IS NOT NULL
    GROUP BY user_id, DATE(event_time)
), yesterday_array AS (
    SELECT * FROM array_metrics
    WHERE month_start = DATE('2023-01-01')
)
SELECT
    COALESCE(da.user_id, ya.user_id) AS user_id,
    COALESCE(ya.month_start, DATE_TRUNC('month', da.date)) AS month_start,
    'site_hits' AS metric_name,
    CASE WHEN ya.metric_array IS NOT NULL THEN
        ya.metric_array || ARRAY[COALESCE(da.num_site_hits, 0)]
    WHEN ya.metric_array IS NULL THEN ARRAY_FILL(0, ARRAY[date - DATE(DATE_TRUNC('month', date))]) || ARRAY[COALESCE(da.num_site_hits, 0)]
    END AS metric_array
FROM daily_aggregate da
    FULL OUTER JOIN yesterday_array ya ON
da.user_id = ya.user_id
ON CONFLICT (user_id, month_start, metric_name)
DO
    UPDATE SET metric_array = EXCLUDED.metric_array;

Console 2:
WITH agg AS (
    SELECT
        metric_name,
        month_start,
        ARRAY[SUM(metric_array[1]),
            SUM(metric_array[2]),
            SUM(metric_array[3])
            ] AS summed_array
    FROM array_metrics
    GROUP BY metric_name, month_start
)
SELECT
    metric_name,
    month_start + CAST(CAST(index - 1 AS TEXT) || ' day' AS INTERVAL),
    elem AS value
    FROM agg
    CROSS JOIN UNNEST(agg.summed_array)
        WITH ORDINALITY AS a(elem, index)


LAB 7
Spark + Iceburg
--------------------------------------------
Reason why this is getOrCreate (camelcase instead of snakecase) has
to do with this running on the JVM and calling methods on a python wrapper.

lit here means literal value so when we do:
df.join(df, lit(1) == lit(1)) it will crash because its a cross join with all the data
this is why we never really want to run collect(), but instead .take(), show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, col, lit

# Build a session
spark = SparkSession.builder.appName("Jupyter").getOrCreate()

df = spark.read.option("header", "true") \
		.csv("/home/iceberg/data/events.csv") \
		.withColumn("event_date", expr("DATE_TRUNC('day', event_time)"))
# df.show()

# .collect() can cause out of memory errors depending on how much data
# df.join(df, lit(1) == lit(1)).collect()

# use take() or show() or some form of data that aggregated already avoid using collect()
# df.join(df, lit(1) == lit(1)).take(5)

split up partitions by event_date: repartition(10, col("event_date")
.sortWithinPartitions() and .sort() is different and very different at scale.
.sort() sorts on a global scale where as .sortWithinPartitions() will sort within our 10 local partitions, global sort is slow

To get a better at the two different sorts we can use .explain()
IMPORTANT: from the plan we look at the most indented event first and work upwards (explain)

Whenever we see exchange in a query plan think of shuffle. (more exchanges the slower the query, especially over 20GB)

sorted = df.repartition(10, col("event_date")) \
			.sortWithinPartitions(col("event_date"), col("host"), col("browser_family")) \
			.withColumn("event_time", col("event_time").cast("timestamp"))

sorted_global = df.repartition(10, col("event_date")) \
			.sort(col("event_date"), col("host"), col("browser_family")) \
			.withColumn("event_time", col("event_time").cast("timestamp"))

sorted.explain()
sorted_global.explain()

project = select when viewing an explain()
exchange in this case is the repartition

when you do a global sort you have to pass the data through 1 executor, almost never use .sort()

when your sorting data you want to write out your lowest cardinality first for instance:
.sortWithinPartitions(col("event_date"), col("browser_family"), col("host"))
event_date has ~100 records

Comparing table sizes
SELECT SUM(file_size_in_bytes) AS size, COUNT(1) AS num_files
FROM table_name.files

UNION ALL

SELECT SUM(file_size_in_bytes) AS size, COUNT(1) AS num_files
FROM table_name.files

.files is meta information about your tables
you can get better compression with the run length encoding and shrink your data down.


LAB 8
DataFrame, Dataset, UDFs, Caching
--------------------------------------------
If you are using case class's a way to handle NULL is to use Option example:
case class Event {
	user_id: Option[Integer],
	device_id: Option[Integer],
	referrer: Option[String],
	host: String,
	url: String,
	event_time: String,
}
in this case we guarantee host, url, event_time is non-Nullable, if their NULL the pipeline will fail

example of loading in a CSV file:
val events: Dataset[Event] = sparkSession.read.option("header", "true")
                        .option("inferSchema", "true")
                        .csv("/home/iceberg/data/events.csv")
                        .as[Event]

filter out certain data:
val filteredViaDataset = events.filter(event => event.user_id.isDefined && event.device_id.isDefined)
val filteredViaDataFrame = events.toDF().where($"user_id".isNotNull && $"device_id".isNotNull)
val filteredViaSparkSql = sparkSession.sql("SELECT * FROM events WHERE user_id IS NOT NULL AND device_id IS NOT NULL")
THESE THREE STATEMENTS ARE THE SAME!

Combine tables via SQL, dataFrame, and datasets on device_id:

SQL:
filteredViaSparkSql.createOrReplaceTempView("filtered_events")
val combinedViaSparkSQL = spark.sql(f"""
    SELECT 
        fe.user_id,
        d.device_id,
        d.browser_type,
        d.os_type,
        d.device_type,
        fe. referrer,
        fe.host,
        fe.url,
        fe.event_time
    FROM filtered_events fe 
    JOIN devices d ON fe.device_id = d.device_id
""")

DataFrame:
val combinedViaDataFrames = filteredViaDataFrame.as("e")
            //Make sure to use triple equals when using data frames
            .join(devices.as("d"), $"e.device_id" === $"d.device_id", "inner")
            .select(
              $"e.user_id",
              $"d.device_id",
              $"d.browser_type",
              $"d.os_type",
              $"d.device_type",
              $"e.referrer",
              $"e.host",
              $"e.url",
              $"e.event_time"
            )
This option is more flexible due to the fact you can chain these operations

Dataset:
val combinedViaDatasets = filteredViaDataset
    .joinWith(devices, events("device_id") === devices("device_id"), "inner")
    .map{ case (event: Event, device: Device) => EventWithDeviceInfo(
                  user_id=event.user_id.get,
                  device_id=device.device_id,
                  browser_type=device.browser_type,
                  os_type=device.os_type,
                  device_type=device.device_type,
                  referrer=event.referrer,
                  host=event.host,
                  url=event.url,
                  event_time=event.event_time
              ) }
you get use user_id=event.user_id.getOrElse(defaultid) to provide a defualt for null, but we can simply use .get because we are using the filtered data
we map this dataset to a new schema called: EventWithDeviceInfo which got defined earlier:
case class EventWithDeviceInfo (
   user_id: Integer,
    device_id: Integer,
    browser_type: String,
    os_type: String,
    device_type: String,
    referrer: String,
    host: String,
    url: String,
    event_time: String
)

(Scala code)
If we want to modify the data say uppercase the browser_type:
def toUpperCase(s: String): String {
	return s.toUpperCase()
}

then just chain map
.map( case (row: EventWithDeviceInfo) => {
    row.browser_type = toUpperCase(row.browser_type)
    return row
})

To use a UDF in a dataframe (we can still use the uppercase example):
val toUpperCaseUdf = udf(toUpperCase _)
then in the select we need to wrap the field:
.select(
  toUpperCaseUdf($"d.browser_type").as("browser_type"),
)
(prefer the dataset/pure scala way)

You can create mock data easily if needed (following the Event class case):
mockData = List(
	Event(user_id=1, device_id=2, referrer="linkedin", host="eczachly.com", url="/signup", event_time="2023-01-01"),
	Event(user_id=3, device_id=7, referrer="twitter", host="eczachly.com", url="/signup", event_time="2023-01-01")
)

You can use diffchecker.com to compare plans, for instance if we want to see what .cache() does:
val eventsAggregated = spark.sql(f"""
  SELECT user_id, 
          device_id, 
        COUNT(1) as event_counts, 
        COLLECT_LIST(DISTINCT host) as host_array
  FROM events
  GROUP BY 1,2
""").cache() <----- compare plans with and without this

val usersAndDevices = users
  .join(eventsAggregated, eventsAggregated("user_id") === users("user_id"))
  .groupBy(users("user_id"))
  .agg(
    users("user_id"),
    max(eventsAggregated("event_counts")).as("total_hits"),
    collect_list(eventsAggregated("device_id")).as("devices")
  )

val devicesOnEvents = devices
      .join(eventsAggregated, devices("device_id") === eventsAggregated("device_id"))
      .groupBy(devices("device_id"), devices("device_type"))
      .agg(
        devices("device_id"),
        devices("device_type"),
         collect_list(eventsAggregated("user_id")).as("users")
      )

devicesOnEvents.explain() <-- compare plans
usersAndDevices.explain() <-- compare plans
Benfits only come with the second time we use .cache()

to write data to disk only 
import org.apache.spark.storage.StorageLevel
.persist(StorageLevel.DISK_ONLY)
but its better to write it to a table then to write to disk:
eventsAggregated.write.mode("overwrite").saveAsTable("bootcamp.events_aggregated_staging")

Disables broadcast join
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
then we can compare bucketing joins and broadcast joins (no shuffle from buckets)
(sort, merge, join within the bucket not the partition)
