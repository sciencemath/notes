Parametric Inference:

Point Estimation
Maximum likelihood
Hypothesis testing
Confidence Intervals

Note:
anything with Φ(z) denotes a z-table lookup

Note:
using the cumulative distribution function (CDF) is always by convention to format probabilities as P(Z < z) so if we end up with P(z > Z) or P(Z > z) we need to minus one (mirror distribution) (1 - P(Z < z))

--------------------------------------------

In statistics we often want ot use sample data to deduce or infer the values of various population parameters, the two most common population parameters
the population mean μ
the population variance σ^2
The population mean is the mean of the entire population, the population variance is the variance of the entire population we should consider these values as fixed unknowns
for example we want to find the mean number of pages per book at barnes and noble we wish to find the population mean μ, but it is impractactical to count teh pages in every book instead we migh approximate μ using the sampling procedure
1. Randomly choose 5 books off the shelf
2. count the number of pages in each book
3. compute the mean of the 5 observations
sample size n = 5
suppose we get the following observations
201, 506, 58, 104, 99
its convenient to denote the individual observations as xi
x1 = 201, x2 = 506, ..., x5 = 99
we can compute the mean x̄ of our sample
x̄ = 1/n*Σ^n_i=1(xi)
= 1/5(201 + 506 + 58 + 104 + 99)
= 1/5 * 968
= 193.6
this is called an estimate of the population mean μ, we sometimes write μ (hat) = 193.6 to denote that the value 193.6 is an estimate of μ
its important to realize that our estimate x̄ depends entirely on this particular sample we'd get a different estimate for μ if we took a different sample

The sample mean
it is helpful to think of the observations of a particular sample
x1,x2, ..., xn
as a particular instance of data generated by a sequence of random variables
X1,X2, ..., Xn
Xi is a random variable that denotes the ith observation in a random sample its value changes when we conduct a new sample
xi is the value of the ith observation in a particular sample
Our sequence of random variables X1,X2,...,Xn typically has these properties
they are independent and identically distributed (I.I.D) meaning they are indpendent and have the same probability distribution they have the same distribution as the population
their mean is equal to the population mean
E(Xi) = μ, 1 <= i <= n
their variance is equal to the population variance
Var(Xi) = σ^2, 1 <= i <= n
the sequence of random variables X1,X2,...Xn with these properties as a random sample size n
in general if X1,X2,...,Xn is a random sample of size n then the sample mean is given by
X̄ = 1/n*Σ^n_i=1(Xi)
the sample mean X̄ is an estimator of the population mean μ
X1,X2,...Xn are random variables so is X̄. The probability distribution of X̄ is called the sampling distribution

Estimating a population mean from the sample mean
data is sampled from a population, compute an estimate for the population mean
6, 13, 7, 5, 14, 11, 7
X1,X2,...Xn is a sequence of random variables representing a random sample of size n drawn from a population mean μ the sample mean X̄
X̄ = 1/n*Σ^n_i=1(Xi)
X̄ is the mean value of X1,X2,...,Xn x̄ is the mean of a specific sample x1,x2...,xn
the sample mean X̄ is an estimator of the population mean μ
the value of μ(hat) = x̄ is an estimate of the population mean μ
= 1/7(6 + 13 + 7 + 5 + 14 + 11 + 7)
= 63/7
= 9

example:
compute an estimate for the population mean
3, 8, 10, 5, 7
1/5(3 + 8 + 10 + 5 + 7)
= 33/5 = 6.6

example:
compute an estimate for the population mean
-2, 5, -3, 10
1/4(-2 + 5 - 3 + 10)
10/4 = 5/2 = 2.5

The sample mean as an unbiased estimator
One important feature of X̄ is that its an unbiased estimator of μ the expected value of X̄ = μ
E[X̄] = μ
we can show X̄ is an unbiased estimator using properties of expectation
E[X̄] = E[1/n*Σ^n_i=1(Xi)]
= 1/n * E[Σ^n_i=1(Xi)]
= 1/n * E[X1 + X2 + ... + Xn]
= 1/n * (E[X1] + E[X2] + ... + E[Xn])
= 1/n * (μ + μ + ... + μ)
= 1/n * nμ
= μ
any estimate of x̄ of the population mean μ found by computing the mean of a sample is called an unbiased estimate of μ

The expected value of the sample mean
A sample X1,X2 of two metallic rings is drawn from a population, the population has rings of two different diameters 16.5mm and 18mm distributed in the ratio 5:1 calculate E[X̄]
to compute the population mean we can consider a population consisting of 5+1 = 6 rings only with the following diameters
16.5mm, 16.5mm, 16.5mm, 16.5mm, 16.5mm, 18mm
the population mean is
1/6(5*16.5 + 18)
= 1/6(82.5 + 18)
= 100.5/6
= 16.75mm
E[X̄] = μ = 16.75mm

example:
if X1,X2,...,X50 is a sample size n = 50 drawn from a population with mean μ = 30 and variance σ^2 = 9 what is E[X̄]?
E[X̄] = μ
E[X̄] = 30

example:
A sample X1,X2 of two nails is drawn from a population, the population has nails of two different lengths 6cm,12cm distributed in the ratio 3:1 calculate E[X̄]
1/4(3*6 + 12)
= 1/4(18 + 12)
= 1/4(30)
= 7.5

===================================================

The sample mean is an example of a statistic. A statistic is a random variable computed using values from a sample. Statistics are often used to estimate population parameters, although they also have other uses
The following statistic could be used as an estimator for the population variance σ^2
1/n*Σ^n_i=1(Xi - X̄)^2
an estimator for the population maximum
max(X1,X2,...,Xn)
an estimator for the population range
max(X1,X2,...,Xn) - min(X1,X2,...,Xn)
an estimator for the population median
median(X1,X2,...,Xn)
statistics cannot include any unknown population parameters
if the population mean μ in unknown then this is not a statistic
1/n*Σ^n_i=1(Xi - μ)^2
however if the population parameters μ and σ are both known then this is a statistic
Σ^n_i=1((Xi - μ)/σ)^2
statistics are described using observations from a random sample, they are random variables and have a probability distribution. The probability distribution of a statistic is called the sampling distribution of the statistic.

Calculating statistics
The weights in stones of five randomly selected 24 year olds
12.5, 11.7, 13.2, 12.8, 12.1
calculate the value of the statistic Σ^n_i=1(Xi^2) for this particular sample
Σ^n_i=1(xi^2) = (12.5)^2 + (11.7)^2 + (13.2)^2 + (12.8)^2 + (12.1)^2
= 156.25 + 136.89 + 174.24 + 163.84 + 146.41
= 777.63

example:
25.2, 24.8, 28.9, 27.5, 28.9
calculate the value of the statistic max(X1,X2,...,Xn) - min(X1,X2,...,Xn)
28.9 - 24.8 = 4.1

example:
1.34, 1.24, 1.42, 1.28, 1.39, 1.44
Σ^n_i=1(Xi)
1.34 + 1.24 + 1.42 + 1.28 + 1.39 + 1.44
= 8.11

===================================================

Lets now find the variance of the sampling distribution of X̄ using the properties of variance for sums of independent variables
Var[X̄] = Var[1/n*Σ^n_i=1(Xi)]
= (1/n)^2 * Var[Σ^n_i=1(Xi)]
= 1/(n^2) * Var[X1 + X2 + ... + Xn]
= 1/(n^2) * (Var[X1] + Var[X2] + ... + Var[Xn])
= 1/(n^2) * (σ^2 + σ^2 + ... + σ^2)
= 1/(n^2) * nσ^2
= σ^2/n
Var[X̄] = σ^2/n
we dont yet know the sampling distribution of X̄. What we do know is that whatever the sampling distribution of X̄ is it has mean μ and variance σ^2/n

finding the variance of a sample mean
A sample X1,X2 of two pills is drawn from a population the population has pills of two different active ingredient concentrations 30mg/ml and 15mg/ml distributed in the ratio 1:2 calculate Var[X̄]
E[X̄] = μ, Var[X̄] = σ^2/n
to compute the population mean and variance, we can consider a population consisting of 1 + 2 = 3 pill concentrations
30mg/ml + 15mg/ml + 15mg/ml
μ = 1/3(30 + 2*15)
= 60/3
= 20mg/ml
variance
σ^2 = 1/3*Σ_i(xi^2 - μ^2)
= 1/3(30^2 + 2*15^2) - 20^2
= 1/3(900 + 450) - 400
= 1/3(1350) - 400
= 450 - 400
= 50(mg/ml)^2
sample size n = 2
Var[X̄] = σ^2/n = 50/2 = 25(mg/ml)^2

example:
if X1,X2,...,X100 is a sample size n = 100 mean μ = 30 and variance σ^2 = 9 calculate the value of Var[X̄]
9/100 = 0.09

example:
a sample X1,X2 of two nails is drawn from a population, the population has nails of two different lengths 6cm and 12cm distributed in the ratio 3:1 calculate Var[X̄]
E[X̄] = 1/4(6^2*3 + 12^2) - 7.5^2
= 1/4(108 + 144) - 7.5^2
= 63 - 56.25
= 6.75/2
= 3.375cm^2

The standard error
the standard deviation of the sample mean is called the standard error of the sample mean and is denoted SE[X̄]
SE[X̄] = sqrt(Var[X̄])
if we have a random sample of size n drawn from a population with mean μ and variance σ^2 then the standard error is
SE[X̄] = sqrt(σ^2/n) = σ/sqrt(n)
the population standard deviation σ is often unknown whenever we draw a random sample from a population, we can usually estimate its value by replacing σ with a suitable estimate say s
SE[X̄] = s/sqrt(n)

finding the standard error
if X1,X2,...,X200 is a sample size n = 200 drawn from a population with mean μ = 23 and variance σ^2 = 4 calculate the standard error of X̄
Var[X̄] = 4/200
= 0.02
SE[X̄] = sqrt(0.02) ~= 0.14

example:
X1,X2,...,X100 sample of size n = 100 with mean μ = 30 and variance σ^2 = 9 calculate standard error of X̄
9/100 = sqrt(0.09)
SE[X̄] = 0.3

example:
X1,X2,X3 of three nails is drawn from a population, the population has nails of two different lengths, 5cm and 10cm distributed in the ratio 4:1 calculate the standard error of X̄
E[X̄] = 1/5(4*5 + 10)
= 1/5(30)
= 6
σ^2 = 1/5(4*5^2 + 10^2) - 6^2
= 40 - 36
= 4cm^2
Var[X̄] = σ^2/n = 4/3cm^2
SE[X̄] = sqrt(4/3) ~= 1.15cm

The sample mean and large samples
As the sample size n becomes larger and larger the variance and standard error of X̄ become smaller and smaller it is easy to see that n -> ∞
Var[X̄] = σ^2/n -> 0
SE[X̄] = σ/sqrt(n) -> 0
when the sample size n is large, there is less variance in the sampling distribution of X̄ the larger the sample the more likely it is that X̄ will give good estimates for μ the individual sample elements do not have this property since Var[Xi] = σ^2
the sample mean X̄ becomes a better estimator for the corresponding population mean μ as the sample size n increases

calculating an appropiate sample size
X1,X2,...,Xn of n bananas is drawn from a population the population has bananas of two different weights 1.5lbs and 2lb distributed 2:3 determine the smallest sample size required to give a SE fo less than 0.02lb
E[X̄] = μ
Var[X̄] = σ^2/n
SE[X̄] = sqrt(Var[X̄]) = σ/sqrt(n)
population consisting of 2+3 = 5
1.5lb, 1.5lb, 2lb, 2lb, 2lb
population mean
= 1/5(2*1.5 + 3*2)
= 1/5(3+6)
= 9/5
= 1.8lb
variance
= 1/5(2*1.5^2 + 3*2^2) - 1.8^2
= 1/5(16.5) - 3.24
= 3.3 - 3.24
= 0.06lb^2
SD = σ = sqrt(0.06)
SE[X̄] = sqrt(0.06/n)
we require that the standard error is less than 0.02lb
sqrt(0.06/n) < 0.02
sqrt(0.06)/sqrt(n) < 0.02
sqrt(n) > sqrt(0.06)/0.02
sqrt(n) > sqrt(0.06/0.02^2)
sqrt(n) > sqrt(0.06/0.0004)
sqrt(n) > sqrt(150)
n > 150
smallest sample size is n = 151

example:
X1,X2,...,Xn is a sample size n drawn from a population with mean μ = 12 and variance σ^2 = 16 determine the smallest sample size required so that the SE of the mean is less than 0.1
SD = σ = sqrt(16) = 4
4/sqrt(n) < 0.1
sqrt(n) > 4/0.1
sqrt(n) > 40
n > 40^2
n > 1600
smallest sample size is n = 1601

example:
X1,X2,...,Xn population of bartenders with two different working hours 38 h/week and 40h/week distributed 3:1 determine the smallest sample size required so that the standard error of the mean is less than 0.5 h/week
population mean μ
= 1/4(38*3 + 40)
= 1/4(154)
= 38.5
variance
= 1/4(38^2*3 + 40^2) - 38.5^2
= 1/4(5932) - 38.5^2
= 1483 - 1482.25
= 0.75
sqrt(0.75/n) < 0.5
sqrt(0.75)/sqrt(n) < 0.5
sqrt(n) > sqrt(0.75)/0.5
sqrt(n) > sqrt(0.75/0.5^2)
sqrt(n) > sqrt(0.75/0.25)
sqrt(n) > sqrt(3)
n > 3
smallest sample size n = 4

===================================================

From the definition of the population variance you might expect
1/n*Σ^n_i=1(Xi - X̄)^2
is an unbiased estimator for σ^2, it can be shown that it is a biased estimator of σ^2
E[1/n*Σ^n_i=1(Xi - X̄)^2] != σ^2
To compute an unbiased estimate of σ^2 we use the sample variance
S^2 = 1/n-1*Σ^n_i=1(Xi - X̄)^2
Instead of dividing by n (as with our original biased estimator) we instead divide by n - 1 this is referred to as Bessel's correction it corrects the bias of the original estimator
we should think of S^2 as a random variable whose value varies according to the particular sample under consideration
since S^2 is a random variable it has a sampling distribution (probility distribution)
An alternative formula for a sample variance thats often more convenient in practice:
S^2 = n/n-1[X̄^2 - (X̄)^2]

Computing an estimate of the population variance
A random sample of size n = 81 is conducted from a population
Σ^81_i=1(xi - x̄)^2 = 180
each xi is a sample observation, compute an unbiased estimate for the population standard deviation
X1,X2,...Xn is a sequence of random variables representing a random sample of size n drawn from a population with a population mean μ and population variance σ^2 the sample variance S^2
S^2 = 1/n-1 Σ^n_i=1(Xi - X̄)^2 = n/n-1[X̄^2 - (X̄)^2]
X̄ is the sample mean
the sample variance S^2 is an unbiased esitmator of the population variance σ^2
σ^2 = s^2 denotes an unbiased estimate of σ^2 thats computed from the sample x1,x2,...,xn
Σ^81_i=1(xi - x̄)^2 = 180 and n = 81
= 1/n-1 Σ^n_i=1(xi - x̄)^2
= (1/81 - 1) * 180
= (1/80) * 180
= 2.25
an unbiased estimate of the population variance is σ^2 = 2.25 we take the square root to compute an unbiased estimate of the standard deviation
σ = sqrt(2.25) = 1.5

example:
sample size n = 50
Σ^50_i=1(xi - x̄)^2 = 1176
compute an unbiased estimate of the variance
= (1/49) * 1176
unbiased estimate = 24

compute an unbiased estimate of the standard deviation
unbiased 
sample size n = 100
Σ^100_i=1(xi - x̄)^2 = 1584
= 1/99 * 1584
σ^2 = 16
σ = sqrt(16) = 4

Computing an estimate of the population variance using the alternative formula
x1,x2,...,x9 is a random sample size n = 9 from a population x̄^2 - (x̄)^2 = 4
compute an unbiased estimate of the population variance
= n/n - 1[x̄^2 - (x̄)^2]
= 9/9-1 * 4
= 36/8
= 4.5

example:
x1,x2,...,x11 sample size 11 x̄^2 - (x̄)^2 = 7
compute an unbiased estimate of the population variance
= 11/10 * 7
= 77/10
= 7.7

example:
x1,x2,...,x6 sample size n = 6
x̄^2 - (x̄)^2 = 3
compute an unbiased esitmate of the population variance
= 6/5 * 3
= 18/5
= 3.6

Estimating the population variance from sample data
compute an unbiased estimate of the population variance
5, 3, 6, 4, 7
x̄ = 1/5(5 + 3 + 6 + 4 + 7)
= 25/5
= 5
x̄^2 = 1/5(5^2 + 3^2 + 6^2 + 4^2 + 7^2)
= 135/5
= 27
x̄^2 - (x̄)^2 = 27 - 5^2
= 2
σ^2 = s^2
= 5/5-1 * 2
= 2.5
unbiased estimate of population variance 2.5

example:
compute an unbiased estimate of the population variance
4, 8, 10, 6, 7
x̄ = 1/5(4 + 8 + 10 + 6 + 7)
= 1/5(35)
= 7
x̄^2 = 1/5(4^2 + 8^2 + 10^2 + 6^2 + 7^2)
= 265/5
= 53
x̄^2 - (x̄)^2 = 53 - 49
= 4
σ^2 = s^2
= 5/5-1 * 4
= 20/4
= 5

example:
compute an unbiased estimate of the population variance
-10, 20, -30, 40
x̄ = 1/4(-10 + 20 - 30 + 40)
= 1/4(20)
= 5
x̄^2 = 1/4(-10^2 + 20^2 - 30^2 + 40^2)
= 1/4(3000)
= 750
x̄^2 - (x̄)^2 = 750 - 5^2
= 725
σ^2 = s^2
4/3 * 725
= 2900/3
~= 966

Proof of the alternative formula
S^2 = n/n-1[X̄^2 - (X̄)^2]
definition
S^2 = 1/n-1 * Σ^n_i=1(X̄i - X̄)^2
expaning paranetheses and distributing
S^2 = 1/n-1 Σ^n_i=1(X̄i^2 - 2XiX̄ + (X̄)^2)
= 1/n-1[Σ^n_i=1(X̄i^2) - Σ^n_i=1(2XiX̄) + Σ^n_i=1(X̄)^2)]
= 1/n-1[Σ^n_i=1(X̄i^2) - 2Σ^n_i=1(XiX̄) + (X̄)^2*Σ^n_i=1(1))]
considering each sum separately
Σ^n_i=1(X̄i^2) = n * X̄^2 from the definition of the mean of X^2
Σ^n_i=1(Xi) = n * X̄ from the definition of the mean of X
Σ^n_i=1(1) = 1 + 1 + ... + 1 = n (n times)
S^2 = 1/n-1[Σ^n_i=1(X̄i^2) - 2Σ^n_i=1(XiX̄) + (X̄)^2*Σ^n_i=1(1))]
= 1/n-1[n * X̄^2 - 2X̄*n*X̄ + (X̄)^2*n]
= n/n-1[X̄^2 - 2(X̄)^2 + (X̄)^2]
= n/n-1[X̄^2 - (X̄)^2]

Proof that the sample variance is an unbiased estimator
S^2 is an unbiased estimator of σ^2
E[S^2] = σ^2
we'll prove
E[X̄^2 - (X̄)^2] = n-1/n * σ^2
X̄^2 - (X̄)^2 is the variance sample
X̄^2 - (X̄)^2 = 1/n Σ^n_i=1(X̄i - X̄)^2
the variance of the sample is not the same as the sample variance S^2
we start by computing the expected difference between σ^2 and the variance of the sample, simplify
σ^2 - (X̄^2 - (X̄)^2)
= 1/n Σ^n_i=1(Xi - μ)^2 - 1/n Σ^n_i=1(Xi - X̄)^2
= 1/n Σ^n_i=1[(Xi - μ)^2 - (Xi - X̄)^2]
= 1/n Σ^n_i=1[(Xi^2 - 2μXi + μ^2) - (Xi^2 - 2X̄Xi + (X̄)^2)]
= 1/n Σ^n_i=1[μ^2 - (X̄)^2 + 2X̄Xi - 2μXi]
compute the expectation, E[Xi] = X̄ and Var[X̄] = σ^2/n
E[σ^2 - (X̄^2 - (X̄)^2)]
= E[1/n Σ^n_i=1(μ^2 - (X̄)^2 + 2X̄Xi - 2μXi)]
= 1/n Σ^n_i=1(μ^2 - (X̄)^2 + 2X̄E[Xi] - 2μE[Xi])
= 1/n Σ^n_i=1(μ^2 - (X̄)^2 + 2(X̄)^2 - 2μX̄)
= 1/n Σ^n_i=1((X̄)^2 - 2X̄μ + μ^2)
= 1/n Σ^n_i=1(X̄ - μ)^2
= Var[X̄]
= σ^2/n

E[X̄^2 - (X̄)^2] = E[σ^2 - (X̄^2 - (X̄)^2)]
= E[σ^2] - E[σ^2 - X̄^2 - (X̄)^2]
= σ^2 - σ^2/n
= n-1/n * σ^2

E[S^2] = E[n-1/n[X̄^2 - (X̄)^2]]
= n/n-1 * E[X̄^2 - (X̄)^2]]
= n/n-1 * n-1/n*σ^2
= σ^2

===================================================

Sample means from normal populations
X1,X2,...Xn be a random sample of size n drawn from a normal population N(μ,σ^2)
X1,X2,...Xn ~ N(μ,σ^2)
sampling distribution of the sample mean
X̄ = 1/n Σ^n_i=1(Xi)
The sum of n independent and identically distributed normal random variables is normally distributed
Y = Σ^n_i=1(Xi)
a normally distributed random variable scaled by a constant factor is normally distributed
X̄ = 1/n * Y
expected value of the sample mean equals population mean
E[X̄] = μ
variance of sample mean
Var[X̄] = σ^2/n
altogether
X̄ ~ N(μ, σ^2/n)
sample mean X̄ is normally distributed with mean μ and variance σ^2/n

Stating the distribution of the sample mean
X1,X2,...,X28 ~ N(8,14^2) is a random sample size of n = 28 from a normal population what is the distribution of the sample mean X̄?
X̄ ~ N(μ, σ^2), 1 <= i <= n
sample mean X̄ has the distribution
X̄ ~ N(μ, σ^2/n)
n = 28, μ = 8, σ = 14
distribution of the sample mean X̄
X̄ ~ N(8, 14^2/28)
~ N(8,7)

example:
X1,X2,...,X50 ~ N(5,10^2) random sample size n = 50 what is the distribution of the sample mean X̄?
~ N(5,10^2/50) ~ N(5,2)

example:
X1,X2,...,X100 ~ N(16,20^2) sample size n = 100 what is the distribution of the sample mean X̄?
~ N(16,20^2/100) ~ N(16,4)

Calculating a probability involving sample mean
X1,X2,...,X100 ~ N(-4,20^2) is a random sample of size 100 from a normal population calculate P(-4.3 < X̄ < -3.9)
sample distribution of the sample mean X̄ is
X̄ ~ N(-4, 20^2/100)
~ N(-4,4)
tansform X into a standard normal random variable by z-scoring
P(-4.3 < X̄ < -3.9)
= P((-4.3 - (-4))/sqrt(4) < Z < (-3.9 - (-4))/sqrt(4))
= P(-0.15 < Z < 0.05)
= Φ(0.05) - Φ(-0.15)
Φ(0.05) = 0.5199
Φ(-0.15) = 0.4404
0.5199 - 0.4404
= 0.0795

example:
X1,X2,...,X50 ~ N(5,10^2) 
sample size of 50 calculate P(4.8 < X̄ < 5.2)
X̄ ~ N(5, 10^2/50) ~ N(5,2)
= P((4.8 - 5)/sqrt(2) < Z < (5.2 - 5)/sqrt(2))
= P(-0.14 < X̄ < 0.14)
Φ(0.14) - Φ(-0.14)
Φ(0.14) = 0.5557
symmetry of normal distribution
Φ(-0.14) = 1 - Φ(0.14)
= 1 - 0.5557
= 0.4443
= 0.5557 - 0.4443
= 0.1114

example:
X1,X2,...,X36 ~ N(0,18^2) is a random sample size of 36
calculate P(|X̄| > 0.8)
~ N(0,18^2/36) ~ N(0,9)
= 1 - P(|X̄| < 0.8)
= 1 - P(-0.8 < X̄ < 0.8)
= 1 - P(-(0.8 - 0)/sqrt(9) < Z < (0.8 - 0)/sqrt(9))
= 1 - P(-0.27 < Z < 0.27)
= 1 - (Φ(0.27) - Φ(-0.27))
= 1 + Φ(0.27) - Φ(-0.27)
Φ(0.27) = 0.6064
Φ(-0.27) = 1 - 0.6064
= 0.3936
= 1 + 0.3936 - 0.6064
= 0.7872

Calculating a probability involving a sample mean in context
blood pressure of adult women in a population that is normally distributed with mean 75mm and SD 10mm. 10 women are selected at random what is the probability that the blod pressure mean for this sample wil be less than 70mm?
X̄ ~ N(75, 10^2/10) ~ N(75,10)
P(X̄ < 70)
= P(Z < (70 - 75)/sqrt(10))
~= P(Z < -1.58)
= Φ(-1.58)
= 0.0571

example:
IQ scores are normally distributed with a mean of 100 and SD 15, 10 people are selected randomly. What is the probability that the mean score for this sample will be greater than 105?
X̄ ~ N(100, 15^2/10) ~ N(100,22.5)
P(X̄ > 105)
= P(Z > (105 - 100)/sqrt(22.5))
= 1 - Φ(1.05)
= 1 - 0.8531
= 0.1469

example:
weights of a particular breed of duck are normally distributed with a mean of 7kg and SD 1kg, 4 ducks are randomly selected. What is the probability that the mean for this sample will be more than 6.8kg but less than 7kg?
P(6.8 < X̄ < 7)
X̄ ~ N(7, 1^2/4) ~ N(7,0.25)
= P((6.8 - 7)/sqrt(0.25) < Z < (7 - 7)/sqrt(0.25))
= P(-0.4 < Z < 0)
= Φ(0.0) - Φ(-0.4)
= 0.5000 - 0.3446
= 0.1554

===================================================

The central limit theorem
if X1,X2,...,Xn is a random sample of size n drawn from a normal population N(μ, σ^2) then the sample mean X̄ is normally distributed with the sampling distribution
X̄ = 1/n Σ^n_i=1(Xi) ~ N(μ, σ^2/n)
The sampling distribution X̄ depends on the population distribution, no general result exists that tells us how X̄ is distributed for an arbitrary population distribution.
An important result, known as the central limit theorem, tells us that when the sample size n is large, the sample mean X̄ is approximately normally distributed the sampling distribution of X̄ for large n does not depend on the population distribution
theorem:
Suppose that X1,X2,...,Xn is a random sample of size n from a population with a population mean μ and population SD σ. THe centeral limit theorem (CLT) states that for sufficiently large n, the distribution of the sample mean X̄ can be approximated as
X̄ ~ N(μ, σ^2/n)
in other words as the sample size increases the distribution of the sample mean is approximately normal

We assume that X1,X2,...,Xn are independent and identically distributed and they can have any distribution (discrete or continuous)
the theorem is valid for sufficiently large n, typically n >= 30 is assumed to be sufficiently large, although this does vary in general the larger the better
CLT assumes that μ and σ both exist, this is not true for every probability distribution (e.g. the Cauchy distribution)

Consider a random variable X that follows a binomial distribution where
X ~ B(20, 0.1) the parameters are m = 20, p = 0.1 lets start by calculating the mean and the variance of X
mean μ is
μ = E[X]
= mp
= 20*0.1
= 2
variance σ^2
σ^2 = Var[X]
= mp(1 - p)
= 20 * 0.1 * 0.9
= 1.8
To understand the sampling distribution of X̄ for this population suppose we conduct the following experiment
1. Pick a value for the sample size n
2. Using a computer randomly generate 1000 sample size n drawn from our binomial population
3. Calculate the sample mean for each sample
4. Create a histogram of the sample means to visualize the approximate sampling distribution
5. repeat this process for increasing values of n
lets do this procedure for sample sizes of 2, 4 and 9
lets consider sample size n = 2
E[X̄] = 2, Var[X̄] = σ^2/n = 1.8/2 = 0.9
next generating 1000 sample of size n = 2 and calculating the corresponding sample mean for each sample, when visualizing the results (histogram) the sample mean does not closely resemble the normal distribution
n = 4
E[X̄] = 2, Var[X̄] = σ^2/n = 1.8/4 = 0.45
again 1000 samples of size n = 4, the visualization of this is closer to normal distribution than n = 2 case
n = 9
E[X̄] = 2, Var[X̄] = σ^2/n = 1.8/9 = 0.2
1000 samples of size n = 9, this is much closer to what appears to be a normal distribution compared to the n=2, and n=4 cases
As the sample size n increases the histograms become more and more similar to their corresponding normal distribution, and the standard error of the sample mean becomes smaller and smaller.

Finding an approximate probability using the central limit theorem
X1,X2,...,X120 be a random sample of size n = 120 from a population with a population mean μ = 100 and population SD σ = 60 find an approximation for the probability that the sample mean is between 102 and 105
n = 120, μ = 100, σ = 60
~ N(100, 60^2/120)
~ N(100, 30)
P(102 < X̄ < 105) = P((102 - 100)/sqrt(30) < Z < (105 - 100)/sqrt(30))
= P(2/sqrt(30) < Z < 5/sqrt(30))
~= P(0.37 < Z < 0.91)
= P(Z < 0.91) - P(Z < 0.37)
= Φ(0.91) - Φ(0.37)
= 0.8186 - 0.6443
= 0.1743

example:
X1,X2,...,X100 be a random sample of size n = 100 from a population mean μ = 500 and population SD σ = 80 find an approximation for the probability that the sample mean is between 490 and 510
n = 100, μ = 500, σ = 80
~ N(500, 80^2/100) ~ N(500,64)
P(490 < X̄ < 510) = P((490 - 500)/8 < Z < (510 - 500)/8)
P(-1.25 < Z < 1.25)
= Φ(1.25) - Φ(-1.25)
Φ(-1.25) = 1 - Φ(1.25)
= 0.8944 - 0.1056
= 0.7888

example:
X1,X2,...,X176 be a random sample of size n = 176 from a population with a population mean μ = 8 and population SD σ = 88 find an approximation for P(|X̄| > 4)
n = 176, μ = 8, σ = 88
~ N(8,88^2/176) ~ N(8,44)
= P(|X̄| < 4) = P(X̄ < -4) + P(X̄ > 4)
= P(Z < (-4 - 8)/sqrt(44)) + P(Z > (4 - 8)/sqrt(44))
~= P(Z < -1.81) + P(Z > -0.60)
= P(Z < -1.81) + (1 - P(Z <= -0.60))
= 1 + Φ(-1.81) - Φ(-0.60)
= 1 + 0.0351 - 0.2743

using the CLT with a population whose probability is discrete
x   | -3| -1|  1|  3|
f(x)|0.4|0.1|0.1|0.4|
X1,X2,...,X185 be a random sample of size n = 185 from a population with the probability mass function, find the approximation in terms of Φ(z) for P(|X̄| > 0.1)
Φ(z) is the cumulative distribution function for the standard normal distribution
n = 185
population mean
μ = E[X]
= Σ^n_i=1(xi * f(xi))
= -3 * 0.4 + (-1) * 0.1 + 1 * 0.1 + 3 * 0.4
= -1.2 - 0.1 + 0.1 + 1.2
= 0
population variance
σ^2 = Var[X]
= E[X^2] - E[X]^2
= ((-3)^2 * 0.4 + (-1)^2 * 0.1 + 1^2 * 0.1 + 3^2 * 0.4) - 0^2
= (3.6 + 0.1 + 0.1 + 3.6) - 0
= 7.4
by the CLT we have that the distribution of the sample mean
X̄ ~ N(μ, σ^2/n)
~ N(0, 7.4/185)
P(|X̄| > 0.1) = P(X̄ < -0.1) + P(X̄ > 0.1)
~= P(Z < (-0.1 - 0)/sqrt(0.04) + P(Z > (0.1 - 0)/sqrt(0.04)))
= P(Z < -0.5) + P(Z > 0.5)
= P(Z < -0.5) + 1 - P(Z < 0.5)
= 1 + Φ(-0.5) - Φ(0.5)
(Φ(-0.5) = 1 - Φ(0.5))
P(|X̄| > 0.1) = 1 + (1 - Φ(-0.5)) - Φ(0.5)
= 2 - 2Φ(0.5)
= 2(1 - Φ(0.5))

example:
x   |  1|  2|  3|  4|
f(x)|0.5|0.2|0.1|0.2|
X1,X2,...,X50 be a random sample of size n = 50, probability sample mean is greater than 2.2
n = 50
μ = 1*0.5 + 2*0.2 + 3*0.1 + 4*0.2
= 2
σ^2 = (1^2*0.5 + 2^2*0.2 + 3^2*0.1 + 4^2*0.2) - 2^2
= 5.4 - 4
= 1.4
~ N(2, 1.4/50) ~ N(2,0.028)
P(X̄ > 2.2)
~= P(Z > (2.2 - 2)/sqrt(0.028))
= P(Z > 1.20)
= 1 - Φ(1.20)

example:
x   | -2| -1|  0|  2|
f(x)|0.3|0.2|0.1|0.4|
X1,X2,...,X30 be a random sample of size n = 30, find an approximation for P(|X̄| > 0.2) = P(X̄ < -0.2) + P(X̄ > 0.2)
n = 30
μ = (-2)*0.3 + (-1)*0.2 + 0*0.1 + 2*0.4
= 0
σ^2 = ((-2)^2*0.3 + (-1)^2*0.2 + 0^2*0.1 + 2^2*0.4) - (0)^2
= 3 - 0
= 3
~ N(0, 3/30) ~ N(0,0.1)
~= P(Z < (-0.2 - 0)/sqrt(0.1) + P(Z > (0.2 - 0)/sqrt(0.1)))
~= P(Z < -0.63) + P(Z > 0.63)
= P(Z < -0.63) + 1 - P(Z < 0.63)
= 1 + Φ(-0.63) - Φ(0.63)
Φ(0.63) = 0.2643
Φ(0.63) = 1 - Φ(-0.63)
= 2Φ(0.63)
= 2 * 0.2643
= 0.5286

Using the CLT with a population is continuous
X1,X2,...,X100 be a random sample of size n = 100, with probability density function
f(x) = {
	2/9(x), 0 <= x <= 3
	0,      otherwise
}
given that the population variance σ^2 = 0.5 calculate P(X̄ > 1.9)
μ = E[X]
∫^∞_-∞ xf(x) dx
= ∫^3_0 x * 2/9(x) dx
= ∫^3_0 2/9(x^2) dx
= [2x^3/27]|_0-3
= 2
σ^2 = 0.5
~ N(2, 0.5/100) ~ N(2, 0.005)
= P(Z > (1.9 - 2)/sqrt(0.005))
~= P(Z > -1.41)
= 1 - P(Z <= -1.41)
= 1 - Φ(-1.41)
= 1 - 0.0793
= 0.9207

example:
X1,X2,...,X20 be a random sample of size n = 20, with probability density function
f(x) = {
	3/2(x^2), -1 <= x <= 1
	0,        otherwise
}
given that the population variance σ^2 = 0.6 calculate P(0 < X̄ < 0.2)
= ∫^1_-1 x * 3/2(x^2) dx
= ∫^1_-1 3/2(x^3) dx
= [3x^4/8]|_-1-1
= ((3/8) - (3/8))
= 0
~ N(0, 0.6/20) ~ N(0,0.03)
P((0 - 0)/sqrt(0.03) < Z < (0.2 - 0)/sqrt(0.03))
~= P(0 < Z < 1.15)
= P(Z < 1.15) - P(Z < 0)
= Φ(1.15) - Φ(0)
= Φ(1.15) = 0.8749
= Φ(0) = 0.5
= 0.8749 - 0.5
= 0.3749

example:
X1,X2,...,X25 be a random sample of size n = 25, with probability density function
f(x) = {
	3/8(x^2), 0 <= x <= 2
	0,        otherwise
}
given the population variance σ^2 = 0.15 calculate P(X̄ > 1.6)
= ∫^2_0 x * 3/8(x^2) dx
= ∫^2_0 3/8(x^3) dx
= [3x^4/32]|_0-2
= (48/32 - (0))
= 24/16 = 12/8 = 6/4 = 3/2
= 1.5
~ N(1.5, 0.15/25) ~ N(1.5,0.006)
P(Z > (1.6 - 1.5)/sqrt(0.006))
~= P(Z > 1.29)
= 1 - P(Z < 1.29)
= 1 - Φ(1.29)
= 1 - 0.9015
= 0.0985

Sums of I.I.D. Random variables
Assume that X1,X2,...,Xn is a random sample of size n from a population with a population mean μ and population SD σ the central limit theorem states that for sufficiently large n
X̄ ~= N(μ,σ^2/n)
an important consequence of the central limit theorem is that if X1,X2,...,Xn are I.I.D
X = Σ^n_i=1(Xi ~= N(nμ,nσ^2))
in other words a sum of I.I.D random variables is approximately normally distributed
if: X̄ = 1/n Σ^n_i=1(Xi ~= N(μ,σ^2/n))
then the random variable: X = nX̄ = X̄ + X̄ + ... + X̄ (n times)
is a sum of n (approximately) normally distributed random variables and is normally distributed, the mean and variance of X are
E[X] = n*μ, Var[X] = n^2 * σ^2/n = nσ^2
writing this in terms of X̄ we have
nX̄ ~= N(nμ,nσ^2)

Equivalent forms of the Central limit theorem
We can rewrite the central limit in several different ways
One way of restating the central limit theorem
X̄ - μ ~= N(0,σ^2/n)
this tells us that the deviation of X̄ from the mean μ is (approximately) normally distributed with a mean of zero and a variance thats the same as X̄
we can also rewrite CLT as
(X̄ - μ)/(σ/sqrt(n)) ~= N(0,1)
this statement tells us that z-scoring the sample mean gives a random variable that (approximately) follows a standard normal distribution
we saw
nX̄ ~= N(nμ,nσ^2)
z-score this result we get
(nX̄ - nμ)/sqrt(nσ^2) = n(X̄ - μ)/sqrt(n)*σ
= sqrt(n)(X̄ - μ)/σ
which means that we can express the central limit theorem in the form
sqrt(n)(X̄ - μ)/σ ~= N(0,1)

===================================================

Sampling proportions from finite populations
Suppose that a business is about to elect a new CEO, we're interested in determining the proportion p of a company's employees that will vote for "candidate A"
if a company has N = 50 employees and 30 of them will vote for candidate A then the proportion p is
p = 30/50 = 0.6
60% of the company's employees will vote for candidate A
N is the population size
p is the population proportion
the number of employees that will vote for candidate A is
Np = 50*0.6 = 30
Lets start by modeling the response from each member of the population as a random variable Xi defined by
Xi = {
	1, ith member of the population will vote for candidate A
	0, otherwise
}
for i = 1,...,50
Calculating the probability that a randomly selected member of the population will vote for candidate A is
P(Xi = 1) = # of members vote for candidate A/population size
= Np/N
= 30/50
= 0.6
its important to note that the Xi's are dependent
P(X2 = 1|X1 = 1) != P(X2 = 1)
to see why note P(X2 = 1) = 0.6 as before
P(X2 = 1|X1 = 1)
= (# of members vote for candidate A) - 1/population size - 1
= (Np - 1)/(N - 1)
= (50*0.6 - 1)/(50 - 1)
= (30 - 1)/49
= 29/49
~= 0.592
!= P(X2 = 1)
we know that the first member will vote for canadidate A so to compute the conditional probability that the second member will also vote for candidate A we consider a reduced population with N-1 = 49 members of which only 30 - 1 = 29 will vote for candidate A

finding aa conditional probability given a population proportion
Apple decided to elect a new board member, there are 50 workers and 60% of this worker population will vote for Tim Cook let the random variable Xi be
Xi = {
	1, ith member of the population will vote for Tim Cook
	0, otherwise
}
for i = 1,...,50 find P(X2 = 0|X1 = 0)
we have a population size N = 50 and p = 0.6 represents the proportion of the population that will vote for Tim Cook
The number of members that will vote for Tim Cook:
Np = 50(0.6) = 30
members that will not vote for Tim Cook:
N(1 - p) = 50(1 - 0.6) = 20
two members of the population at random, we need to find P(X2 = 0|X1 = 0) the probability that the second member will not vote for Tim Cook given that the first member will also not vote for Tim Cook
if first member does not vote for Tim Cook
N - 1 = 50 - 1 = 49 members remaining
N(1 - p) - 1 = 19 members remaining that will not vote for Tim Cook
P(X2 = 0|X1 = 0) = (N(1 - p) - 1)/(N - 1) = 19/49

example:
we have a population size N = 20 where 55% of the population has a particular gene
Xi = {
	1, ith member of the population has the gene
	0, otherwise
}
for i = 1,...,20 find P(X2 = 1|X1 = 1)
0.55(20) = 11 has the gene
the probability of the second person having the gene given the first person having it: 10/19

example:
Google decided to elect a new board member, there are 20 workers and 55% of this worker population will vote for Sergey Brin
Xi = {
	1, ith member that will vote for Sergey Brin
	0, otherwise
}
for i = 1,...,20. Find P(X2 = 1|X1 = 0)
0.55(20) = 11 will vote for Sergey Brin
P(X2 = 1|X1 = 0) = Np/N-1 = 11/19

Independence in large populations
population of size N = 1000 of which teh proportion p = 0.5 has a particular genetic
Xi = {
	1, ith member of the population has the genetic
	0, otherwise
}
for i = 1,...,1000
Xi's are dependent
this time since the population size N = 1000 is large we may approximate that the Xi's are independent
to see why we first note
P(X2 = 1) = Np/N = 0.5
computing P(X2 = 1|X1 = 1)
= (Np - 1)/(N - 1)
= (1000*0.5 - 1)/)(1000 - 1)
= 499/999
~= 0.5
P(X2 = 1|X1 = 1) ~= P(X2 = 1)
X1 and X2 are (approximately) independent

The number of sample elements with a genetic
Xi = {
	1, ith member of the sample has the genetic
	0, otherwise
}
for i = 1,...,n
Xi's can be modeled as mutually independent random variables provided that the population size N is significantly larger than the sample size n this typically means that the sample size should be no larger than 5% of the population size
The population size is much larger than the sample size is true in many cases. For example, the users using the Chrome browser consists of millions of people yet a typical sample from this population might consist of only a few thousand users.
X = Σ^n_i=1(Xi)
where Xi ~ Bernoulli(p)
since the Xi's take the value 1 if the ith element has the genetic or 0 if the ith element doesn't have the genetic, the statistic X can be used to count the number of elements with the genetic. X represents the number of sample elements with the genetic
Each Xi can be modeled as a Bernoulli random variable Xi ~ Bernoulli(p)
We're assuming N >> n it follows that X1,X2,...,Xn are (approximately) independent and identically distributed (I.I.D) Bernoulli random variables
A sum of n I.I.D. Bernoulli random variables with parameter p is a binomial random variable B(n,p)
process:
Xi ~ Bernoulli(p)
		↓
N >> n
   		↓
X1,X2,...,Xn ~= I.I.D
		↓
X = Σ^n_i=1(Xi ~= B(n,p))
using the formulas for the mean and variance of Binomial random variable we have the following approximations
E[X] = np, Var[X] = np(1 - p)

Properties of sums of bernoulli random variables
random sample of size n = 360 from a population where 35% of the population has a particular genetic
Xi = {
	1, ith member of the sample has the genetic
	0, otherwise
}
where i = 1,...,360
X = Σ^360_i=1(Xi)
find approximate values of E[X] and Var[X]
hint: we may assume that the population size is significantly larger than the sample size
provided that N >> n the random variables Xi can be considered independent and identically distributed
Xi ~ Bernoulli(p) 1 <= i <= n
where p is the proportion of the population that has the genetic
X = Σ^n_i=1(Xi)
can be approximated as a binomial random variable X ~= B(n,p)
E[X] = np, Var[X] = np(1 - p)
p = 35% = 0.35, n = 360
X ~= B(360,0.35)
mean and variance
E[X] = np
= 360*0.35
= 126
Var[X] = np(1 - p)
= 360*0.35 * (1 - 0.35)
= 81.9

example:
random sample size n = 20 from a population where 55% of the population has a particular genetic
Xi = {
	1, ith member of the sample has the genetic
	0, otherwise
}
where i = 1,...,20 approximate sampling distribution of the statistic X = Σ^20_i=1(Xi)
= B(20, 0.55)

example:
random sample size n = 20 from a population where 55% of the population has a particular genetic
Xi = {
	1, ith member of the sample has the genetic
	0, otherwise
}
where i = 1,...,20 if the statistic is defined as
X = Σ^20_i=1(Xi)
find approximate values of E[X], and Var[X]
E[X] = 20*0.55 = 11, Var[X] = 11 * (1 - 0.55) = 4.95

Mean and variance of sample proportions
random sample size n where some proportion p of the population has a particular genetic
Xi = {
	1, ith member of the sample has the genetic
	0, otherwise
}
for i = 1,...,n
X = Σ^20_i=1(Xi ~= B(n,p))
we can form an estimate for the population proportion p
p̂ = X/n = 1/n Σ^n_i=1(Xi) = X̄
E[p̂] = E[X/n]
= 1/n * E[X]
= 1/n * np
= p
Var[p̂] = Var[X/n]
= 1/n^2 * Var[X]
= 1/n^2 * np(1 - p)
= p(1 - p)/n
E[p̂] = p, Var[p̂] = p(1 - p)/n
we don't yet know the sampling distribution of p̂

The standard Error
the standard deviation of p̂ is called the standard error and is denoted SE[p̂]
SE[p̂] = sqrt(Var[p̂]) = sqrt(p(1 - p)/n)
if p is unknown we can replace it with its estimate p̂ in the standard error
SE[p̂] = sqrt(p̂(1 - p̂)/n)

Mean and variance of sample proportions
in a population of adults with only one computer 42% have an Apple. A random sample of n = 40 adults is taken from the population and asked if they have a windows or an apple.
What is the mean and variance of p̂ the proportion of adults in the sample that have an Apple
E[p̂] = 42% = 0.42 and n = 40
Var[p̂] = 0.42(1 - 0.42)/40
~= 0.0061

example:
45% of share holders plans to vote for Steve Jobs in the upcoming Apple's shareholders meeting. A random sample of size n = 20 is taken from the population and asked about their voting preference
What is the mean and variance of p̂

E[p̂] = 45% = 0.45
Var[p̂] = 0.45(1 - 0.45)/20 ~= 0.0124

example:
15% of a particular group of young adults ride skateboards for exercise, a random sample of n = 60 young adults is taken from the population and asked whether they ride a skateboard for exercise
What is the mean and variance of p̂ who ride skateboards?
E[p̂] = 0.15
Var[p̂] = 0.15(1 - 0.15)/60 ~= 0.0021

Proof that a sum of vernoulli random variables is binomial
X1,X2,...,Xn ~ Bernoulli(p) are independent
Y = X1,X2,...,Xn ~ B(n,p)
we will demonstrate why this result is true using moment-generating functions
Let MX_i(t) be the moment-generating functino Xi for i = 1,2,...,n using a table of known results
MX_i(t) = 1 - p + pe^t
By the multiplicative property of moment-generating functions we have that M_Y(t) = MX_1 + X2 + ... + Xn(t)
= MX_1(t) * MX_2(t) * ... * MX_n(t)
= (1 - p + pe^t) * (1 - p + pe^t) * ... * (1 - p + pe^t)
= (1 - p + pe^t)^n
which is the moment-generating function of a binomial random variable with distribution B(n,p)
since any given distribution is characterized by its moment-generating function (meaning that no other distribution can have the same moment-generating function)
Y ~ B(n,p)

===================================================

Point estimates of population proportions
Xi = {
	1, ith member of the sample has the characteristic
	0, otherwise
}
Xi ~ Bernoulli(p)
Lets assume the sample size is <= 5% of the population size X1,X2,...,Xn can be considered to be (approximately) independent and identically distributed which means that 
X = Σ^n_i=1(Xi ~= B(n,p))
B(n,p) is a binomial distribution
E[X] = np, Var[X] = np(1 - p)
X represents the number of sample members with the characteristic
assume that np > 5 and n(1 - p) > 5 we can apply the normal approximation of the binomial theorem (i.e. central limit theorem)
X ~= N(np, np(1 - p))
we can construct an estimator p̂
p̂ = X/n
E[p̂] = p, Var[p̂] = p(1 - p)/n
since X is approximately normally distributed p̂ is also normally distributed, the sampling distribution is
p̂ = N(p, p(1 - p)/n)
we usually don't know p so we can use an estimate to approximate E[p̂] and Var[p̂]

A summary of the sample distribution model
Suppose we have a population size N in which a proportion p has a particular characteristic. Consider a sample of size n drawn from the population where
n <= 5% * N (the sample size is smaller than or equal to 5% of the population size)
np > 5
n(1 - p) > 5
then the sampling distribution of the sample proportion p̂ can be approximated as the noramlly distributed random variable
p̂ ~ N(p, p(1 - p)/n)

Xi ~ Bernoulli(p)
		↓
N >> n
   		↓
X1,X2,...,Xn ~= I.I.D
		↓
X = Σ^n_i=1(Xi ~= B(n,p))
		↓
np > 5, n(1 - p) > 5
		↓
X ~= N(np, np(1 - p))
		↓
p̂ = X/n ~= N(p, p(1 - p)/n)

np > 5 means that at least five members of the sample have the characteristic
n(1 - p) > 5 means that at least five members of the sample do not have the characteristic

Identifying situations when the CLT for proportions is appropiate
p̂ ~ N(p, p(1 - p)/n)
where p is the population proportion?
1. A sample of 120 apples from a population of 20000 where 75% of the apples weigh more than 50g
2. A sample of 25 students from a school containing 200 students where 80% of the students study for more than 100 minutes a day
3. A sample of 45 ubers at a mall given 90% of drivers arrive on time you may assume the population size is significantly larger than the sample size
situation 1:
n = 120
<= 5% * N
= 0.05 * 20000
= 1000 ✔
np = 120(0.75)
= 90
> 5 ✔
n(1 - p) = 120(1 - 0.75)
= 30
> 5 ✔
we can approximate the distribution of p̂ using this
in situation 2:
n = 25
= 0.05 * 200
= 10
< 25
sample size is more than 5% if the population size
situation 3:
since we are given that the population is significantly larger than the sample size we can assume the first condition is satisfied
np = 45(0.9)
= 40.5
> 5 ✔
n(1 - p) = 45(1 - 0.9)
= 4.5
!> 5

Finding an approximate probability using the CLT for proportions
random sample size n = 84 from population where 42% of the population has a particular characteristic find a normal approximation for the probability that more than 50% of the sample has the characteristic, assume that the population size is significantly larger than the sample size.
n = 84
population proportion is p = 0.42
np = 84(0.42) = 35.28 > 5
n(1 - p) = 84(1 - 0.42) = 48.72 > 5
~ N(0.42, 0.42(1 - 0.42)/84)
~ N(0.42, 0.0029)
P(p̂ > 0.5) = P(Z > (0.5 - 0.42)/sqrt(0.0029))
= P(Z > 1.49)
= 1 - P(Z < 1.49)
= 1 - Φ(1.49)
Φ(1.49) = 0.9319
= 1 - 0.9319
= 0.0681

example:
a random sample of size n = 100 from a population where 60% of the population has a particular characteristic, find a normal approximation for the probability that between 55% and 65% of the sample have the characteristic
~ N(0.60, 0.60(1 - 0.60)/100)
~ N(0.60, 0.0024)
P(0.55 < p̂ < 0.65)
= P((0.55 - 0.60)/sqrt(0.0024) < Z < (0.65 - 0.60)/sqrt(0.0024))
-1.02 < Z < 1.02
Φ(1.02) - Φ(-1.02)

example:
a random sample of size n = 80 from a population where 42% of the population has a particular characteristic, find a normal approximation for the probability that more than 35% of the sample has the characteristic.
~ N(0.42, 0.42(1 - 0.42)/80)
~ N(0.42, 0.0030)
P(0.35 > p̂) = P(Z > (0.35 - 0.42)/sqrt(0.0030))
P(Z > -1.28)
= 1 - P(Z < -1.28)
= 1 - Φ(-1.28)
= 1 - 0.1003
= 0.8997

Finding an approximate probability using the CLT for proportions applications
Spotify has a db of 100 million songs, 64 million have a duration of less than 3 minutes, if a user creates a playlist with 128 songs find a normal approximation for the probability that between 64 and 96 (inclusive) songs in the sample have a duration less than 3 minutes. You may assume that the population size is significantly large than the sample size.
n = 128, poppulation proportion is p = 64000000/100000000 = 0.64
np = 128(0.64) = 81.92 > 5
n(1 - p) = 128(1 - 0.64) = 46.08 > 5
~ N(0.64, 0.64(1 - 0.64)/128)
~ N(0.64, 0.0018)
probability between 64 and 96 of the songs in the play list last less than 3 min
64/128 <= p̂ 96/128
0.5 <= p̂ <= 0.75
P(0.5 <= p̂ <= 0.75)
= P((0.5 - 0.64)/sqrt(0.0018) <= Z <= (0.75 - 0.64)/sqrt(0.0018))
= P(-3.30 <= Z <= 2.59)
= P(Z <= 2.59) - P(Z <= -3.30)
= Φ(2.59) - Φ(-3.30)
= 0.9952 - 0.0005
= 0.9947

example:
38% of households owns a computer. If a town randomly samples 200 households find a normal approximation for the probability that more than 80 of them have a computer.
n = 200
np = 200(0.38) = 76 > 5
n(1 - p) = 200(1 - 0.38) = 124 > 5
~ N(0.38, 0.38(1 - 0.38)/200)
~ N(0.38, 0.0011)
p > 80/200 = 0.4
(p̂ > 0.4) = P(Z > (0.4 - 0.38)/sqrt(0.0012))
= P(Z > 0.58)
= 1 - P(Z < 0.58)
= 1 - Φ(0.58)

example:
an online class of 900 students, 432 are female, the admins sample 50 students find a normal approximation for the probability that between 20 and 23 (inclusive) of students in the sample are female
n = 50, p = 432/900 = 0.48
np = 50(0.48) = 24 > 5
n(1 - p) = 50(1 - 0.48) = 26 > 5
~ N(0.48, 0.48(1 - 0.48)/50)
~ N(0.48, 0.005)
20/50 = 0.4, 23/50 = 0.46
P(0.4 <= p̂ <= 0.46)
= P((0.4 - 0.48)/sqrt(0.005) <= Z <= (0.46 - 0.48)/sqrt(0.005))
= P(-1.13 <= Z <= -0.28)
= P(Z <= -0.28) - P(Z <= -1.13)
= Φ(-0.28) - Φ(-1.13)
= 0.3897 - 0.1292
= 0.2605

===================================================

The sample covariance matrix
number of courses taken per semester and average final grade in a sample of 4 college students
# courses| 4| 3| 5| 4|
grade    |80|85|75|72|
collect data in a matrix
X = [
	 4  3  5  4
	80 85 75 72
]
matix X is called the obersation matrix
we want to determine the mean number of courses and the mean average grade of this sample, the sample mean vector denoted m
m = 1/n Σ^n_i=1(xi)
where x1,x2,...,xn are the individual (columns of X)
x1 = [4,80], x2 = [3,85], x3 = [5,75], x4 = [4,72]
sample mean vector
m = 1/4(x1 + x2 + x3 + x4)
= 1/4([4,80] + [3,85] + [5,75] + [4,72])
= 1/4[16,312]
= [4,78]
ther average # of courses 4 and average final grade is 78

Finding the sample mean of an observation matrix
X = [
	1 1 0 2
	2 5 3 2
]
x1 = [1,2], x2 = [1,5], x3 = [0,3], x4 = [2,2]
m = 1/4(x1 + x2 + x3 + x4)
= 1/4([1,2] + [1,5] + [0,3] + [2,2])
= 1/4[4,12]
= [1,3]

example:
X = [
	1 0 -2 5
	2 4 12 2
]
= 1/4([1,2] + [0,4] + [-2,12] + [5,2])
= 1/4([4,20])
= [1,5]

example:
X = [
	 7  8  0 -3
	-2 10 -3  3
]
= 1/4([7,-2] + [8,10] + [0,-3] + [-3,3])
= 1/4([12,8])
= [3,2]

Mean-Deviation form
to find the mean-deviation form of X first lets start out with observation matrix
X = [
	 4  3  5  4
	80 85 75 72
]
x1 = [4,80], x2 = [3,85], x3 = [5,75], x4 = [4,72]
mean vector
m = [4,78]
how much each observation differs from the mean values
b1 = x1 - m = [4,80] - [4,78] = [0,2]
b2 = x2 - m = [3,85] - [4,78] = [-1,7]
b3 = x3 - m = [5,75] - [4,78] = [1,-3]
b4 = x4 - m = [4,72] - [4,78] = [0,-6]
B = [
	0 -1  1  0
	2  7 -3 -6
]
B is the mean-deviation form of X its easier to work with the mean-deviation form especially if the original data contains large numbers, the mean of a mean-deviation form equals the zero vector:
m_B = 1/4([0,2] + [-1,7] + [1,-3] + [0,-6])
= 1/4([0,0])
= [0,0]

finding the mean deviation form of an observation matrix
X = [
	 5  9 4
	15 -7 1
]
find the mean deviation
x1 = [5,15], x2 = [9,-7], x3 = [4,1]
= 1/3([5,15] + [9,-7] + [4,1])
= 1/3[18,9]
= [6,3]
b1 = x1 - m = [5,15] - [6,3] = [-1,12]
b2 = x2 - m = [9,-7] - [6,3] = [3,-10]
b3 = x3 - m = [4,1] - [6,3] = [-2,-2]
mean-deviation
B = [
	-1  3  -2
	12 -10 -2
]

example:
X = [
	1 0 2
	2 3 1
]
x1 = [1,2], x2 = [0,3], x3 = [2,1]
= 1/3([1,2] + [0,3] + [2,1])
= 1/3[3,6]
= [1,2]
b1 = x1 - m = [1,2] - [1,2] = [0,0]
b2 = x2 - m = [0,3] - [1,2] = [-1,1]
b3 = x3 - m = [2,1] - [1,2] = [1,-1]

example:
X = [
	 7  6  8
	-1 -3 -2
]
x1 = [7,-1], x2 = [6,-3], x3 = [8,-2]
= 1/3([7,-1] + [6,-3] + [8,-2])
= 1/3[21,-6]
= [7,-2]
b1 = x1 - m = [7,-1] - [7,-2] = [0,1]
b2 = x2 - m = [6,-3] - [7,-2] = [-1,-1]
b3 = x3 - m = [8,-2] - [7,-2] = [1,0]

The covariance matrix
mean deviation matrix
B = [
	0 -1  1  0
	2  7 -3 -6
]
calculating the product BB^T
[
	0 -1  1  0
	2  7 -3 -6
][
	 0  2
	-1  7
	 1 -3
	 0 -6
] = [
	 2 -10
	-10 98
]
The entry in the first row first column (2) is the sum of the squares of the mean deviations of the number of courses
The entry in the second row second column (98) is the sum of the squares of the mean deviations of the final grades
The off diagonal entries (-10) are the sum of the products of the mean deviations
since there were 4 observations in total, if we divide each entry by 4 - 1 = 3 we will get a matrix C with the sample variances in the diagonal and the sample covaraince in the off-diagonal positions
C = 1/3[
	 2 -10
	-10 98
]
this matrix is called the covariance matrix of the data
in general the sample covariance matrix is
C = 1/n-1(BB^T)
B is the observation matrix in the mean-deviation form, and n is # of observations

Finding the covariance matrix
X = [
	 2 6 4
	-4 1 6
]
find the sample covariance matrix of data
sample covariance matrix
C = 1/n-1(BB^T)
sample mean
= 1/3([2,-4],[6,1],[4,6])
= 1/3[12,3]
= [4,1]
b1 = x1 - m = [2,-4] - [4,1] = [-2,-5]
b2 = x2 - m = [6,1] - [4,1] = [2,0]
b3 = x3 - m = [4,6] - [4,1] = [0,5]
mean-deviation form
B = [
	-2 2 0
	-5 0 5
]
C = 1/3-1[
	-2 2 0
	-5 0 5
][
	-2 -5
	 2  0
	 0  5
]
= 1/2[
	 8 10
	10 50
]

example:
B = [
	1 -2  4 -3
	0  3 -1 -2
]
find the sample covariance matrix
1/3[
	1 -2  4 -3
	0  3 -1 -2
][
	 1  0
	-2  3
	 4 -1
	-3 -2
] = 1/3[
	30 -4
	-4 14
]

example:
X = [
	1 -10 0
	3   3 0
]
find the sample covariance matrix
x1 = [1,3], x2 = [-10,3], x2 = [0,0]
= 1/3([1,3] + [-10,3] + [0,0])
= 1/3[-9,6]
= [-3,2]
b1 = x1 - m = [1,3] - [-3,2] = [4, 1]
b2 = x2 - m = [-10,3] - [-3,2] = [-7,1]
b3 = x3 - m = [0,0] - [-3,2] = [3,-2]
B = [
	 4 -7  3
	 1  1 -2
][
	 4  1
	-7  1
	 3 -2
] = 1/2[
	74 -9
	-9  6
]

===================================================

Product notation
summation is usually written using sigma notation
Σ^4_i=1(2i - 3)
represents the sum of the first 4 terms of the sequence ai = 2i - 3
= (2(1) - 3) + (2(2) - 3) + (2(3) - 3) + (2(4) - 3)
= (-1) + 1 + 3 + 5
= 8
similar compact notation for products
Π^4_i=1(2i - 3)
represents the product of the first 4 terms of the sequence ai = 2i - 3
Π^4_i=1(2i - 3)
= (2(1) - 3) * (2(2) - 3) * (2(3) - 3) * (2(4) - 3)
= (-1) * 1 * 3 * 5
= -15

Evaluating a product
Π^4_k=1(3k-1/6k-1)
expand the product by multiplying over the index k = (1-4)
= (3(1)-1/6(1)-1) * (3(2)-1/6(2)-1) * (3(3)-1/6(3)-1) * (3(4)-1/6(4)-1)
= 2/5 * 5/11 * 8/17 * 11/23
(before multiplying we can cross cancel the 11s and 5s)
= 2*8/17*23
= 16/391

example:
Π^6_k=2(2i + 1)
= (2(2) + 1) * (2(3) + 1) * (2(4) + 1) * (2(5) + 1) * (2(6) + 1)
= 5 * 7 * 9 * 11 * 13
= 45045

example:
Π^8_k=5(2k+1/1-2k)
= (2(5)+1/1-2(5)) * (2(6)+1/1-2(6)) * (2(7)+1/1-2(7)) * (2(8)+1/1-2(8))
= -11/9 * -13/11 * -15/13 * -17/15
= (-1)^4 * 17/9
= 17/9

Writing a product using product notation
(2 - 3^2) * (2 - 4^2) * (2 - 5^2) * ... * (2 - 12^2)
we can collapse the given expression using the product notation over the index i from i = 3 to i = 12
= Π^12_i=3(2 - i^2)

example:
(2*3^2 - 1) * (2*4^2 - 1) * (2*5^2 - 1) * ... * (2*n^2 - 1)
= Π^n_i=3(2*(i)^3 - 1)

example:
1^2/1^3-1 * 2^2/2^3-1 * 3^2/3^3-1 * ... * 10^2/10^3-1
Π^10_i=1((i)^2/(i)^3 - 1)

Simplifying a product
express Π^m_n=3(n+1/n-1) as a function of m
fist expand the product by multiplying over the index n = 3 to n = m
= 3+1/3-1 * 4+1/4-1 * 5+1/5-1 * 6+1/6-1 * ... * (m-1)+1/(m-1)-1 * m+1/m-1
= 4/2 * 5/3 * 6/4 * 7/5 * ... * m/m-2 * m+1/m-1
(canceling out common terms 7=m-2, 6=m-1)
= m*(m+1)/2*3
= m*(m+1)/6

example:
Π^n-1_m=2(2(m) + 2)
(substituting last term)
(2m+2) = (2(n-1)+2) = 2n-2+2 = 2n
= 6 * 8 * 10 * ... * 2n

example:
Π^m_r=2(r(r + 1)/r - 1)
= (2(2+1)/2-1) * (3(3+1)/3-1) * (4(4+1)/4-1) * ... * (m(m+1)/m-1)
= 3*2/1 * 4*3/2 * 5*4/3 * ... * (m(m+1)/m-1)
(canceling out common terms)
= 3 * 4 * 5 * ... * m*(m+1)
= (1 * 2 * 3 * 4 * 5 * ... * m * (m+1))/2
= (m + 1)!/2

===================================================

Logarithmic differentiation is a technique to compute derivatives that are difficult to find using the usual rules of differentiation such as the product, quotient, and chain rules
y = x^x 
we want to compute dy/dx
both the base and the exponent are functions of x
this means we cannot apply the power rule since the exponent is not a fixed number
We also cannot apply the rules for differentiating exponentials since the base is not a fixed number
so instead we'll use logarithmic differentiation
step 1: we take the natural logarithm of both sides and apply the power rule for logarithms:
y = x^x
ln(y) = ln(x^x)
ln(y) = xln(x)
step 2: we differentiate both sides of the above equation with respect to x using implicit differentiation and the product rule:
d/dx(ln(y)) = d/dx(xln(x))
1/y * dy/dx = d/dx(x) * ln(x) + x * d/dx(ln(x))
1/y * dy/dx = 1 * ln(x) + x * (1/x)
1/y * dy/dx = ln(x) + 1
step 3: now we solve for dy/dx
dy/dx = y(ln(x) + 1)
step 4: we substitute back our original expression for y = x^x
dy/dx = x^x(ln(x) + 1)

Differentiating X raised to the power of a function
find the derivative of the function y = x^(x^3)
first take the natural logarithm of both sides
ln(y) = ln(x^(x^3))
ln(y) = x^3ln(x)
next differentiate both sides with respect to x
d/dx(ln(y)) = d/dx(x^3ln(x))
1/y * y' = (x^3)' * ln(x) + (x^3) * ln(x)'
1/y * y' = 3x^2 * ln(x) + x^3 * 1/x
y'/y = 3x^2ln(x) + x^2
now we solve for y'
y' = y(3x^2ln(x) + x^2)
substitute y = x^(x^3)
y' = x(^(x^3)+2)(3ln(x) + 1)

example:
y = x^(2x) its derivative is given by
y' = 2x^(2x) * g(x)
find the function g(x)

ln(y) = ln(x^(2x))
ln(y) = 2xln(x)
respect to x
d/dx(ln(y)) = d/dx(2xln(x))
1/y * y' = (2x)' * ln(x) + 2x * (ln(x))'
y'/y = 2 * ln(x) + 2x * 1/x
solve for y'
y' = y(2ln(x) + 2)
substitute
y' = 2x^(2x)(ln(x) + 1)
g(x) = ln(x) + 1
(noticed we factored out a 2)

example:
consider the function y = x^(cos(x)) its derivative is
dy/dx = x^cos(x - 1) * g(x)
find the function g(x)
y = x^(cos(x))
ln(y) = ln(x^(cos(x)))
ln(y) = cos(x)ln(x)
respect to x
d/dx(ln(y)) = d/dx(cos(x)ln(x))
1/y * dy/dx = d/dx(cos(x)) * ln(x) + (cos(x)) * d/dx(ln(x))
1/y * dy/dx = (-sin(x)) * ln(x) + (cos(x)) * 1/x
1/y * dy/dx = -ln(x)sin(x) + cos(x)/x
solve for dy/dx
dy/dx = y(cos(x)/x - ln(x)sin(x))
substitute y = x^(cos(x))
(common denominator)
= x^(cos(x))(cos(x)/x - xln(x)sin(x)/x)
= x^(cos(x - 1))(cos(x) - xln(x)sin(x))
g(x) = cos(x) - xln(x)sin(x)

Differentitating a function raised to the power of X
find the derivative of the function
y = (x^2 - 1)^x
first we take the natrual logarithm of both sides
ln(y) = ln((x^2 - 1)^x)
ln(y) = xln(x^2 - 1)
respect to x
d/dx(ln(y)) = d/dx(xln(x^2 - 1))
1/y * dy/dx = d/dx(x) * ln(x^2 - 1) + x * d/dx(ln(x^2 - 1))
1/y * dy/dx = 1 * ln(x^2 - 1) + x * ((1/x^2 - 1) * 2x)
1/y * dy/dx = ln(x^2 - 1) + (2x^2/x^2 - 1)
now solve for dy/dx
dy/dx = y(ln(x^2 - 1) + (2x^2/x^2 - 1))
substitute
= (x^2 - 1)^x(ln(x^2 - 1) + (2x^2/x^2 - 1))
(common denominator)
= (x^2 - 1)^x((x^2 - 1)ln(x^2 - 1)/(x^2 - 1) + (2x^2/x^2 - 1))
= (x^2 - 1)^(x-1)((x^2 - 1)ln(x^2 - 1) + 2x^2)

example:
consider the function y = (2x)^x its derivative is
y' = (2x)^x * g(x)
first we take the natrual logarithm of both sides
ln(y) = ln((2x)^x)
ln(y) = xln(2x)
respect to x
d/dx(ln(y)) = d/dx(xln(2x))
1/y * dy/dx = d/dx(x) * ln(2x) + x * d/dx(ln(2x))
1/y * dy/dx = ln(2x) + x * 2 * 1/2x
1/y * dy/dx = ln(2x) + 1
substitute
y' = y(ln(2x) + 1)
y' = (2x)^x(ln(2x) + 1)
g(x) = ln(2x) + 1

example:
consider the function y = (x + 1)^x its derivative is
y' = (x + 1)^(x-1) * g(x)
find the function g(x)
first we take the natrual logarithm of both sides
ln(y) = ln((x + 1)^x)
ln(y) = xln(x + 1)
respect to x
d/dx(ln(y)) = d/dx(xln(x + 1))
1/y * dy/dx = d/dx(x) * ln(x + 1) + x * d/dx(ln(x + 1))
1/y * dy/dx = ln(x + 1) + (x/x + 1)
substitute
y' = y(ln(x + 1) + (x/x + 1))
y' = (x + 1)^x(ln(x + 1)(x + 1)/x + (x/x + 1))
y' = (x + 1)^(x-1)(ln(x + 1)(x + 1) + (x))
y' = (x + 1)^(x-1)((x + 1)ln(x + 1) + (x))
g(x) = (x + 1)ln(x + 1) + x

Differentiating a function raised to the power of a function
find the derivative of the function
y = (sin(x))^(cos(x))
first we take the natrual logarithm of both sides
ln(y) = ln(sin(x)^(cos(x)))
ln(y) = cos(x)ln(sin(x))
respect to x
d/dx(ln(y)) = d/dx(cos(x)ln(sin(x)))
1/y * y' = cos(x)' * (ln(sin(x))) + cos(x) * ln(sin(x))'
1/y * y' = (-sin(x)) * ln(sin(x)) + cos(x) (1/sin(x) * cos(x))
y'/y = cos(x)cot(x) - sin(x)ln(sin(x))
solve for y'
y' = y(cos(x)cot(x) - sin(x)ln(sin(x)))
substitute
y' = (sin(x))^(cos(x))(cos(x)cot(x) - sin(x)ln(sin(x)))

example:
consider the function y = (cos(x)^sin(x)) its derivative is
y' = (cos(x))^(sin(x)) * g(x)
find the function g(x)
first we take the natrual logarithm of both sides
ln(y) = ln(cos(x))^(sin(x))
ln(y) = sin(x)ln(cos(x))
respect to x
d/dx(ln(y)) = d/dx(sin(x)ln(cos(x)))
1/y * y' = (sin(x))' * ln((cos(x))) + sin(x) * (ln(cos(x)))'
1/y * y' = cos(x) * ln((cos(x))) + sin(x) * (1/cos(x) * (-sin(x)))
y'/y = cos(x)ln(cos(x)) - sin^2(x)/cos(x)
solve for y'
y' = y(cos(x)ln(cos(x)) - sin^2(x)/cos(x))
substitute
y' = cos(x)^(sin(x))(cos(x)ln(cos(x)) - sin^2(x)/cos(x))
= cos(x)^(sin(x))(cos(x)ln(cos(x)) - sin(x)tan(x))
g(x) = cos(x)ln(cos(x)) - sin(x)tan(x)

example:
find the slope of the tagent to the curve
y = (x + 1)^(x^2) at x = 1
first we take the natrual logarithm of both sides
ln(y) = ln((x + 1)^(x^2))
ln(y) = x^2ln((x + 1))
respect to x
1/y * y' = (x^2)' * ln(x + 1) + (x^2) * ln(x + 1)'
1/y * y' = 2x * ln(x + 1) + x^2 * 1/x+1
y'/y = 2x * ln(x + 1) + x^2/x+1
substitute:
y' = y(2xln(x + 1) + x^2/x+1)
y' = (x + 1)^(x^2)(2x * ln(x + 1) + x^2/x+1)
at x = 1
y'|_x=1 = (1 + 1)^(1^2)(2(1) * ln(1 + 1) + 1^2/1+1)
= 2^(1)(2 * ln(2) + 1/2)
= 4ln(2) + 1

===================================================

NOTE:
You may see a weird (4,xi) notation I use after the condensed product notation, think of it as a column 4 choose xi

Likelihood functions for discrete probability distributions
We might know the general form of the probability distribution for a population yet we may not know some of its underlying parameters. We want to develop strategies to estimate unknown population parameters from sample data. One strategy is to use likelihood functions.
suppose we have an Idependent and Identically distributed (I.I.D.) random sample
X1,X2,...,Xn
where the probability mass function of each Xi is given by
f(x;θ)
θ is an unknown a fixed parameter
and we have a sample from this distribution
x1,x2,...,xn
we define the likelihood function L(θ)
L(θ) = P(X1 = x1,X2 = x2,...,Xn = xn)
the likelihood function gives the probability of getting a particular smaple for a specific parameter value.
Since we assumed that all Xi's are independent and identically distributed by the multiplication law we have
L(θ) = P(X1 = x1,X2 = x2,...,Xn = xn)
= P(X1 = x1) * P(X2 = x2) ... P(Xn = xn)
= Π^n_i=1(P(Xi = xi))
= Π^n_i=1(f(xi;θ))
likelihood function is defined as
L(θ) = Π^n_i=1(f(xi;θ))

Worked example
suppose that
X1, X2, X3, X4, X5, X6
is an I.I.D random sample from a population where
Xi ~ Bernoulli(θ)
and θ is the unknown probability of success
we conduct a sample and get the following data
x1 = 0, x2 = 1, x3 = 1, x4 = 0, x5 = 1, x6 = 1
lets compute the likelihood
if X1, X2,...,Xn are I.I.D random variables with probability mass function f(x;θ) then the likelihood function of a random sample x1,x2,...,xn is
L(θ) = P(X1 = x1,X2 = x2,...,Xn = xn) = Π^n_i=1(f(xi;θ))
the probability mass function for a Bernoulli random variable with an unknown probability of success θ is
f(x;θ) = θ^x(1 - θ)^(1-x), x = 0,1
L(θ) = Π^n_i=1(f(xi;θ))
= Π^n_i=1(θ^(xi)(1 - θ)^(1-xi))
= (θ^(x1)(1 - θ)^(1-x1)) * (θ^(x2)(1 - θ)^(1-x2)) ... (θ^(xn)(1 - θ)^(1-xn))
= θ^(Σ(xi))(1 - θ)^(Σ(1-xi)))
= θ^(Σ(xi))(1 - θ)^(n - Σ(1-xi)))
= θ^(S)(1 - θ)^(n - S))
S = Σ^n_i=1(xi)
our sample we have n = 6
= 0 + 1 + 1 + 0 + 1 + 1
= 4
the likelihood function for this sample
L(θ) = θ^4(1 - θ)^(6 - 4)
= θ^4(1 - θ)^2
L(θ) has a maximum inside the interval [0,1] the value of θ(hat) that corresponds to the maximum likelihood is called the maximum likelihood estimate of the parameter θ for a given sample.

Likelihood functions for Bernoulli Random variables
X1,X2,...,X10 is an I.I.D random sample with Xi ~ Bernoulli(θ) and unknown probability of success θ for a particular smaple x1,x2,...,x10 given
Σ^10_i=1(xi = 7)
find the likelihood function of the sample
Hint: the probability mass function for a Bernoulli random variable is given by
f(x) = θ^x(1 - θ)^(1-x), x = 0,1
X1,X2,...,Xn are independent and identically distributed discrete random variables with probability mass function f(x;θ) where θ is an unknown parameter then the likelihood function of a random sample x1,x2,...,xn is
L(θ) = P(X1 = x1,X2 = x2,...,Xn = xn) = Π^n_i=1(f(xi;θ))
in this case sample of n = 10 independent Bernoulli random variables with unknonw parameter θ for which the probability mass function is
f(x;θ) = θ^x(1 - θ)^(1-x), x = 0,1
S = Σ^n_i=1(xi = 7) and n = 10
L(θ) = θ^7(1 - θ)^(10 - 7)
= θ^7(1 - θ)^3

example:
Σ^6_i=1(xi = 5)
= θ^5(1 - θ)^(6 - 5)
= θ^5(1 - θ)

example:
X1,X2,X3,X4,X5, is an I.I.D random sample with Xi ~ Bernoulli(θ) and unknown probability of success θ, find the likelihook of the sample 1,0,0,1,0
Σ^5_i=1(xi = 2)
= θ^2(1 - θ)^(5 - 2)
= θ^2(1 - θ)^3

Likelihood functions for Binomial random variables
X1,X2,...,X12 is an I.I.D random sample with Xi ~ B(4,θ) and unknown probability of success θ for a sample x1,x2,...,x12
Σ^12_i=1(xi = 22)
find the likelihood function of the sample up to a constant C
Hint: The probability mass function for a binomial random variable is given by:
f(x) = (4,x)θ^x(1 - θ)^(n - x), x = 0,1,2,3,4
in our case we have sample n = 12 binomial random variables with unknown probability of success θ and N = 4 trails for which the probability mass function is
f(x;θ) = (4,x)θ^x(1-θ)^(4-x), x = 0,1,2,3,4
L(θ) = Π^n_i=1(f(xi;θ))
= Π^n_i=1(4,xi)θ^xi(1 - θ)^(4-xi)
= ((4,x1)^x1(1 - θ)^(4-x1)) ... ((4,xn)^xn(1 - θ)^(4-xn))
= Cθ^(Σ(xi))(1 - θ)^(Σ(4 - xi))
= Cθ^(Σ(xi))(1 - θ)^(4n - Σ(xi))
= Cθ^S(1 - θ)^(4n - S)
where C = Π^n_i=1(4,xi) is a constant and S = Σ^n_i=1(xi)
S = Σ^n_i=1(xi = 22) and n = 12
L(θ) = Cθ^22(1 - θ)^(4(12) - 22)
= Cθ^22(1 - θ)^26

example:
X1,X2,...,X10 is an I.I.D random sample with Xi ~ B(5,θ) and unknown probability of success θ for a particular sample x1,x2,...,x3
Σ^10_i=1(xi = 36)
find the likelihood function of the sample up to a constant C
Hint: The probability mass function for a binomial random variable is given by
f(x) = (5,x)θ^x(1 - θ)^(n - x), x = 0,1,2,...,5
L(θ) = Cθ^36(1 - θ)^(5(10) - 36)
= Cθ^36(1 - θ)^14

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ B(6,θ) and unknown probability of success θ find the likelihood function of the sample 3,0,2,5 up to a constant C
= 3 + 0 + 2 + 5
= 10
Σ^4_i=1(xi = 10)
L(θ) = Cθ^10(1 - θ)^(6(4) - 10)
= Cθ^10(1 - θ)^14

Likelihood Functions for poisson random variables
X1,X2,X3,X4,X5 is an I.I.D random sample with Xi ~ Po(θ) and unknown rate parameter θ find the likelihood function of the sample 0,2,3,1,1
in our case we are given a sample of n = 5 independent Poisson random variables with unknown parameter θ for which the PMF is
f(x;θ) = θ^xe^-θ/x!, x = 0,1,2...
L(θ) = Π^n_i=1(f(xi;θ))
= Π^n_i=1(θ^xie^-θ/xi!)
= (θ^x1e^-θ/x1!) * (θ^x2e^-θ/x2!) ... (θ^xne^-θ/xn!)
= θ^(Σ(xi))*e^(-Σ(θ)) * Π^n_i=1(1/(xi)!)
= θ^(Σ(xi))*e^(-nθ) * 1/Π^n_i=1(xi)!
= 1/C(θ^S)*e(-nθ)
S = Σ^n_i=1(xi)
= 0 + 2 + 3 + 1 + 1
= 7
C = Π^n_i=1(xi)!
= 0! * 2! * 3! * 1! * 1!
= 1 * 2 * 6 * 1 * 1
= 12
n = 5
L(θ) = 1/12(θ^7)e^(-5θ)

example:
X1,X2,...,X16 is an I.I.D random sample with Xi ~ Po(θ) and unknown rate parameter θ for a particular sample x1,x2,...,x16
Σ^16_i=1(xi = 18), Π^16_i=1(xi!) = 6
find the likelihood function of a sample
Hint probability mass function for a Poisson distribution is
f(x) = (θ^xe^-θ/x!), x = 0,1,2,...
L(θ) = 1/6(θ)^18*e^(-16θ)

example:
X1,X2,X3,X4,X5 is an I.I.D random sample with Xi ~ Po(θ) and unknown rate parameter θ find the likelihood function of the sample 1,3,3,1,4
S = Σ^n_i=1(xi)
1 + 3 + 3 + 1 + 4
= 12
C = Π^n_i=1(xi!)
1! * 3! * 3! * 1! * 4!
= 1 * 6 * 6 * 1 * 24
= 864
L(θ) = (1/864)(θ^12)(e^(-5θ))

===================================================

NOTE:
binomial coefficient (n, k) is calculated as follows
(n
 k) = n!/k!(n-k)!
example:
(5
 3) = 5!/3!(5 - 3)! = 5!/3!*2! = 5*4/2*1 = 10

Log-likelihood functions for discrete probability distributions
if X1,X2,...,Xn are independent and indentically distributed discrete random variables with probability mass function f(x;θ) where θ is an unkown parameter then the likelihood function of a random sample x1,x2,...,xn is
L(θ) = Π^n_i=1f(xi;θ)
if: X1,X2,X3,X4,X5,X6
is an I.I.D random sample with Xi ~ Bernoulli(θ) where θ is the unknown probability of success then the likelihood function of the sample
x1 = 0, x2 = 1, x3 = 1, x4 = 0, x5 = 1, x6 = 1
is given by
L(θ) = θ^4(1 - θ)^2
The likelihood function contains lots of products which can be cumbersome to make things easier we convert the products into sums by finding the natural logarithm of L(θ)
l(θ) = ln(L(θ))
= ln(Π^n_i=1(f(xi;θ)))
= ln(f(x1;θ) * f(x2;θ) ... f(xn;θ))
= ln(f(x1;θ)) + ln(f(x2;θ)) + ... + ln(f(xn;θ))
= Σ^n_i=1(ln(f(xi;θ)))
This new function l(θ) is called the log-likelihood function of the sample
the corresponding log-likelihood function
l(θ) = ln(θ^4(1 - θ)^2)
= ln(θ^4) + ln((1 - θ)^2)
= 4ln(θ) + 2ln(1 - θ)

Log-likelihood functions for Bernoulli random variables
X1,X2,...,X10 is an I.I.D random sample where Xi ~ Bernoulli(θ) with unknown probability of success θ for a particular sample x1,x2,...,x10 you're given that there are 7 successes find the log-likelihood function of the sample.
we are given a sample of n = 10 independent Bernoulli random variables with unknown parameter θ therefore the probability mass function is
f(x;θ) = θ^x(1 - θ)^(1 - x), x = 0,1
l(θ) = Σ^n_i=1ln(θ^(xi)(1 - θ)^(1-xi))
= Σ^n_i=1(xi)ln(θ^(xi)) + ln((1 - θ)^(1-xi))
= ln(θ)Σ^n_i=1(xi) + ln(1 - θ)Σ^n_i=1(1 - xi)
= Sln(θ) + (n - S)ln(1 - θ)
S = Σ^n_i=1(xi) is the number of successes in the sample
finally since there are 7 successes in the sample
l(θ) = 7ln(θ) + (10 - 7)ln(1 - θ)
= 7ln(θ) + 3ln(1 - θ)

example:
X1,X2,...,X12 is an I.I.D random sample where Xi ~ Bernoulli(θ) with unknown probability of success θ for a particular sample, x1,x2,...,x12 you're given that there are 5 successes find the log-likelihood function of the sample
l(θ) = 5ln(θ) + (12 - 5)ln(1 - θ)
= 5ln(θ) + 7ln(1 - θ)

example:
X1,X2,...,X20 is an I.I.D random sample where Xi ~ Bernoulli(θ) with unknown probability of success θ for a sample x1,x2,...,x20 you're given that there are 14 successes find the log-likelihood function of the sample
l(θ) = 14ln(θ) + (20 - 14)ln(1 - θ)
= 14ln(θ) + 6ln(1 - θ)

Log-Likelihood functions for binomial random variables
X1,X2,X3 is an I.I.D random sample Xi ~ B(5,θ) with unknown probability of success θ you're given the particular sample
x1 = 4, x2 = 1, x3 = 1
find the log-likelihood function of this sample
Hint: The probability mass function for a binomial random variable is given by
f(x) = (5,x)θ^x(1 - θ)^(n - x), x = 0,1,...,5
in our case we are given a sample of n = 3 independent binomial random variables with an unknown probability of success θ and N = 5 as the number of trials, the probability mass function is
f(x,θ) = (N,x)θ^x(1 - θ)^(N - x), x = 0,1,...,N
= (5,x)θ^x(1 - θ)^(5 - x), x = 0,1,...,5
l(θ) = Σ^n_i=1(ln((5,xi)θ^xi(1 - θ)^(5 - xi)))
= Σ^n_i=1(ln(5,xi) + ln(θ^xi) + ln((1 - θ)^(5 - xi)))
= Σ^n_i=1(ln(5,xi) + Σ^n_i=1(xiln(θ)) + Σ^n_i=1(5-xi)ln(1 - θ))
= Σ^n_i=1(ln(5,xi) + ln(θ)Σ^n_i=1(x1) + ln(1 - θ)Σ^n_i=1(5-xi))
= Σ^n_i=1(ln(5,xi) + Sln(θ) + (5n - S)ln(1 - θ))
Σ^n_i=1(ln(5,xi) = ln(5,x1) + ln(5,x2) + ln(5,x3))
= ln(Π^n_i=1(5,xi))
l(θ) = ln(Π^n_i=1(5,xi)) + Slnθ + (5n - S)ln(1 - θ)
sample data with n = 3
Π^n_i=1(5,xi) = (5,4)*(5,1)*(5,1)
= 5*5*5
= 125
S = 4 + 1 + 1 = 6
substitution
l(θ) = ln(125) + 6ln(θ) + (5 * 3 - 6)ln(1 - θ)
= ln(125) + 6ln(θ) + 9ln(1 - θ)

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ B(3,θ) with unknown probability of success θ you're given the sample
x1 = 2, x2 = 1, x3 = 2, x4 = 0
l(θ) = ln(a) + bln(θ) + cln(1 - θ)
find the value of a + b + c
Hint: the probability mass function for a binomial random variable is given by
f(x) = (3,x)θ^x(1 - θ)^(n - x), x = 0,1,2,...,3
(3,2)*(3,1)*(3,2)*(3,0)
3*3*3*1 = 27
2 + 1 + 2 + 0 = 5
= ln(27) + 5ln(θ) + (3 * 4 - 5)ln(1 - θ)
= ln(27) + 5ln(θ) + 7ln(1 - θ)
a + b + c = 39

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ B(5,θ) with unknown probability of success θ given the sample
x1 = 5, x2 = 3, x3 = 2, x4 = 1
ln(θ) = ln(a) + bln(θ) + cln(1 - θ)
find the value of a + b + c
(5,5)(5,3)(5,2)(5,1)
1*10*10*5 = 500
5+3+2+1 = 11
substituting
ln(500) + 11ln(θ) + 9ln(1 - θ)
500 + 11 + 9 = 520

Log-likelihood functions for Poisson random variables
X1,X2,X3 is an I.I.D random sample with Xi ~ Po(θ) with unknown rate parameter θ with sample
x1 = 3, x2 = 4, x3 = 1
find the log-likelihood function
In our case we are given a sample of n = 3 independent Poisson random variables with unknown parameter θ the probability mass function is
f(x,θ) = e^-θ * θ^x/x!
l(θ) = Σ^n_i=1(ln(e^-θ * θ^x/x!))
= Σ^n_i=1(ln(e^-θ) + ln(θ^xi) ln(x!))
= Σ^n_i=1(-θ) + Σ^n_i=1(ln(θ^xi)) - Σ^n_i=1(ln(xi!))
= Σ^n_i=1(-θ) + Σ^n_i=1(xiln(θ)) - Σ^n_i=1(ln(xi!))
= -θΣ^n_i=1(1) + ln(θ)Σ^n_i=1(xi) - Σ^n_i=1(ln(xi!))
= -nθ + lnθΣ^n_i=1(xi) - ln(Π^n_i=1(xi!))
using the same data with n = 3
Π^n_i=1(xi!) = 3! * 4! * 1! = 6 * 24 * 1 = 144
Σ^n_i=1(xi) = 3 + 4 + 1 = 8
l(θ) = -3θ + 8ln(θ) - ln(144)

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ Po(θ) with unknown rate parameter θ given the sample
x1 = 4, x2 = 2, x3 = 1, x4 = 2
l(θ) = aθ + bln(θ) - lnc
find the value of a + b + c
Hint: the probability mass function for a poisson distribution is
f(x) = θ^xe^-θ/x!, x = 0,1,2,...
4! * 2! * 1! * 2! = 96
l(θ) = -4θ + 9ln(θ) - ln(96)
a + b + c = 101

example:
X1,X2,X3 is an I.I.D random sample with Xi ~ Po(θ) with unknown rate parameter θ given the sample
x1 = 3, x2 = 4, x3 = 2
3! * 4! * 2! = 288
3+4+2 = 9
l(θ) = -3θ + 9ln(θ) - ln(288)
a + b + c = 294

===================================================

Likelihood functions of samples drawn from populations modeled using continuous random variables are defined similarly to discrete variables. The only difference is that we use probability density functions instead of probability mass functions
if X1,X2,...,Xn are independent and identically distributed continuous random variables with probability density function f(x;θ) then the likelihood function of a random sample x1,x2,...,xn
L(θ) = P(X1 = x1,X2 = x2,...,Xn = xn) = Π^n_i=1f(xi;θ)
let X1,X2,X3 be continuous, I.I.D random variables with the following probability density function
f(x) = θx^(-θ-1), x >= 1
θ is an unknown parameter, conduct our random sample 
x1 = 2.5, x2 = 3.2, x3 = 1.2
compute the likelihood
L(θ) = Π^n_i=1(θxi^(-θ-1))
= (θx1^(-θ-1)) * (θx2^(-θ-1)) ... (θxn^(-θ-1))
= θ^n (x1^(-θ-1)) * (x2^(-θ-1)) ... (xn^(-θ-1))
= θ^n(x1 * x2 ... xn)^(-θ-1)
= θ^n(Π^n_i(xi))^(-θ-1)
our sample n = 3
Π^n_i(xi) = 2.5 * 3.2 * 1.2 = 9.6
L(θ) = θ^3(9.6)^(-θ-1)

Likelihood functions for continuous random variables
X1,X2,X3 be I.I.D. random variables with probability density function
f(x) = (θ + 1)x^(-θ-2), x >= 1
where θ is an unknown parameter, find the likelihood of the sample 2.0, 4.5, 5.1
Π^n_i=1(xi) = 2.5 * 4.5 * 5.1 = 45.9
n = 3
L(θ) - (θ + 1)^3(45.9)^(-θ-2)

example:
X1,X2,...,X15 be I.I.D. random variables with probability density function
f(x) = θx^(-θ-1), x >= 1
where θ is an unknown parameter for a particular sample x1,x2,...,x15
Π^15_i=1(xi) = 40.6
= θ^15(40.6)^(-θ-1)

example:
X1,X2,X3 be I.I.D random variables with probability density function
f(x) = θx^(-θ-1), x >= 1
where θ is an unknown parameter find the likelihood function of the sample:
2.1, 3.2, 5.0
Π^n_i=1(xi) = 2.1 * 3.2 * 5.0 = 33.6
n = 3
L(θ) - (θ + 1)^3(33.6)^(-θ-1)

Likelihood functions for exponential random variables
X1,X2,...X6 is an I.I.D random sample with Xi ~ Exp(θ) and unknown rate parameter θ find the likelihood of the sample
1.5, 2.0, 1.1, 0.4, 3.3, 2.7
sample of n = 6 independent exponential random variables with unknown parameter θ for which the probability density function is
f(x,θ) = θe^(-θx), x >= 0
calculate the likelihood function
L(θ) = Π^n_i=1(θe^(-θxi))
= (θe^(-θx1)) * (θe^(-θx2)) ... (θe^(-θxn))
= θ^n e^(-θ(x1 + x2 + ... + xn))
= θ^n e^(-θΣ(xi))
= θ^n e^(-Sθ)
= 1.5 + 2.0 + 1.1 + 0.4 + 3.3 + 2.7
= 11
L(θ) = θ^6e^(-11θ)

example:
X1,X2,...X7 is an I.I.D random sample with Xi ~ Exp(θ) and unknown rate parameter θ for a sample x1,x2,...,x7
Σ^7_i=1(xi) = 18.2
find the likelihood sample
Hint: The probability density function for an exponential random variable is given by
f(x) = θe^(-θx), x >= 0
= θ^7e^(-18.2θ)

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ Exp(θ) and unknown rate parameter θ find the likelihood of sample 2.1, 5.0, 4.2, 1.4
= 12.7
L(θ) = θ^4e^(-12.7θ)

The normal distibution
the probability density function of a normally distributed random variable X ~ N(μ,σ^2)
f(x) = 1/σsqrt(2pi)e^(-1/2(x-μ/σ))^2
if we set σ = 1 then f(x) simplifies
f(x) = 1/sqrt(2pi)e^-(x-μ)^2/2
this is a standard normal random variable whose mean is shifted right by μ units
finally setting the unknown parameter as θ when discussing likelihood functions is common, following this convention we set μ = θ
f(x;θ) = 1/sqrt(2pi)e^-(x-θ)^2/2

Likelihood functions for normal random variables
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ N(θ,1) where θ is an unknown mean find the likelihood function of the sample
0.4, -0.3, -2.2, -0.4
sample n = 4 independent normal random variables with unknown mean θ and variance σ^2 = 1 for which the probability density function is
f(x,θ) = 1/σsqrt(2pi)e^(-1/2(x-θ/σ))^2 = 1/sqrt(2pi)e^-(x-θ)^2/2
we can calculate the likelihood
L(θ) = Π^n_i(1/sqrt(2pi)e^-(x-θ)^2/2)
= (2pi)^(-n/2)e^-Σ(xi-θ)^2/2
= (2pi)^(-n/2)e^-(Σ(xi^2-2θ)Σ(xi+nθ^2))/2
= (2pi)^(-n/2)e^-(C - 2Sθ + nθ^2)/2
S = Σ^n_i=1(xi) and C = Σ^n_i=1(xi^2)
sample is n = 4
S = Σ^n_i=1(xi)
= 0.4 + (-0.3) + (-2.2) + (-0.4)
= -2.5
C = Σ^n_i=1(xi^2)
= (0.4)^2 + (-0.3)^2 + (-2.2)^2 + (-0.4)^2
= 5.25
L(θ) = (2pi)^(-4/2)e^-(5.25 - 2(-2.5)θ + 4θ^2)/2
= 1/4pi^2e^(-2.625 - 2.5θ - 2θ^2)

example:
X1,X2,...,X12 is an I.I.D random sample with Xi ~ N(θ,1) where θ is an unknown mean for a sample x1,x2,...,x12
Σ^12_i=1(xi) = 7.4, Σ^12_i=1(xi^2) = 16.8
given that the likelihood function of the sample is
L(θ) = 1/64pi^6e^g(θ)
what is the expression for g(θ)
Hint: the probability density function for a normal random variable with mean θ and variance 1
f(x) = 1/sqrt(2pi)e^-((x-θ)^2/2)
= 1/64pi^6(-8.4 + 7.4θ - 6θ^2)
g(θ) = -8.4 + 7.4θ - 6θ^2

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ N(θ,1) where θ is an unknown mean given the likelihood of the sample -1.2, 0.4, 2.2, 0.8
L(θ) = 1/4pi^2e^(g(θ))
what is the expression for g(θ)
S = Σ^n_i=1(xi)
= (-1.2) + 0.4 + 2.2 + 0.8
= 2.2
C = Σ^n_i=1(xi^2) 
= (-1.2)^2 + 0.4^2 + 2.2^2 + 0.8^2
= 7.08
= 1/4pi^2e^-(7.08 - 2(2.2)θ + 4θ^2)/2
= 1/4pi^2e^(-3.54 + 2.2θ - 2θ^2)

===================================================

Similar to the case of discrete random variables the log-likelihood function for continuous probability distributions
l(θ) = ln(L(θ))
= ln(Π^n_i=1(f(xi;θ)))
= ln(f(x1;θ) * f(x2;θ) ... f(xn;θ))
= ln(f(x1;θ)) + ln(f(x2;θ)) + ... + ln(f(xn;θ))
= Σ^n_i=1(ln(f(xi;θ)))

Log-likelihood function for continuous random variables
X1,X2,X3,X4 be I.I.D random variables with probability density function
f(x) = θx^(-θ-1), x >= 1
where θ is unknown, find the likelihood function of the sample
1.3, 2.4, 1.0, 1.7
first the log-density function
ln(f(x;θ)) = ln(θx^-θ-1)
= ln(θ) + ln(x^-(θ + 1))
= ln(θ) - (θ + 1)ln(x)
calculate the log-likelihood
l(θ) = Σ^n_i=1(ln(θ) - (θ + 1)ln(xi))
= Σ^n_i=1(ln(θ)) - Σ^n_i=1(θ + 1)ln(xi)
= nln(θ) - (θ + 1)Σ^n_i=1(ln(xi))
for the sample n = 4
Σ^n_i=1(ln(xi)) = ln(1.3) + ln(2.4) + ln(1.0) + ln(1.7)
~= 1.67
ln(θ) ~= 4ln(θ) - (θ + 1) * 1.67
= 4ln(θ) - 1.67(θ + 1)

example:
X1,X2,...,X12 be I.I.D random variables with probability density function
f(x) = θx^-θ-1, x >= 1
θ is an unknown parameter for a sample x1,x2,...,x12
Σ^12_i=1(ln(xi)) ~= 13.45
the log-likelihood function
l(θ) = aln(θ) - b(θ + 1)
what is the value of a + b?
l(θ) = 12ln(θ) - 13.45(θ + 1)
a = 12 + 13.45

example:
X1,X2,X3 be I.I.D random variables with probability density function
f(x) = θx^-θ-1, x >= 1
θ unknown, the log-likihood function of the sample 2.0, 3.2, 5.5
l(θ) = aln(θ) - b(θ + 1)
what is the value of a + b?
Σ^n_i=1(ln(xi)) = ln(2.0) + ln(3.2) + ln(5.5)
~= 3.56
l(θ) = 3ln(θ) - 3.56(θ + 1)
a + b = 3 + 3.56 ~= 6.56

Log-likelihood functions for exponential random variables
X1,X2,X3,X4,X5 is an I.I.D random sample Xi ~ Exp(θ) and unknown rate parameters θ find the log-likelihood of the sample 2, 4, 1, 3, 7
Σ^n_i=1(xi) = 2 + 4 + 1 + 3 + 7 = 17
n = 5
l(θ) = 5ln(θ) - θ * 17
= 5ln(θ) - 17θ

example:
X1,X2,...,X8 is an I.I.D random sample with Xi ~ Exp(θ) and unknown rate parameter θ for a sample x1,x2,...,x8
given:
Σ^8_i=1(xi) = 28.4
l(θ) = aln(θ) + bθ
what is a + b?
l(θ) = 8ln(θ) + (-28.4)θ
a + b = 8 + (-28.4) = -20.4

example:
X1,X2,X3 is an I.I.D random sample Xi ~ Exp(θ) and an unknown rate parameter θ the log-likelihood function of the sample 5.1, 4.2, 6.3
l(θ) = aln(θ) + bθ
Σ^n_i=1(xi) = 5.1 + 4.2 + 6.3 = 15.6
n = 3
l(θ) = 3ln(θ) + (-15.6)θ
a + b = 3 + (-15.6) = -12.6

Log-Likelihood functions for normal random variables
X1,X2,...,X8 is an I.I.D random sample with Xi ~ N(θ,1^2) where θ is an unknown mean we are given
Σ^8_i=1(xi) = 11.8, Σ^8_i=1(xi^2) = 25.2,
Hint: The probability density function for a normal random variable with mean θ and SD 1
f(x) = 1/sqrt(2pi)e^-(x-θ)^2/2
first we can write down the log-density function
ln(x,θ) = ln(1/sqrt(2pi)e^(-(x-θ)^2/2))
= ln(1/sqrt(2pi)) + ln(e^(-(x-θ)^2/2))
= -1/2ln(2pi) - (x - θ)^2/2
calculate
l(θ) = Σ^n_i=1(-1/2 * ln(2pi) - (xi - θ)^2/2)
= -n/2 * ln(2pi) - Σ^n_i=1((xi - θ)^2/2)
= -n/2 * ln(2pi) - 1/2(Σ^n_i=1(xi^2) - 2θΣ^n_i=1(xi) + nθ^2)
Σ^8_i=1(xi) = 11.8, Σ^8_i=1(xi^2) = 25.2, and n = 8
l(θ) = -8/2 * ln(2pi) - 1/2(25.2 - 2θ * 11.8 + 8θ^2)
= -4 * ln(2pi) - 12.6 + 11.8θ - 4θ^2

example:
X1,X2,...,X12 is an I.I.D random sample with Xi ~ N(θ,1^2) where θ is an unknown mean for a sample x1,x2,...,x12
given
Σ^12_i=1(xi) = 30.2, Σ^12_i=1(xi^2) = 90.6
l(θ) = -6ln(2pi) + a + bθ + cθ^2
what is the value of a + b + c
the probability density function for a normal random variable with mean θ and SD 1
f(x) = 1/sqrt(2pi)e^(-(x-θ)^2)/2
ln(θ) = -12/2(2pi) - 1/2(90.6 - 2θ*30.2 + 12θ^2)
= -6ln(2pi) + (-45.3) + (30.2)θ + (-6)θ^2
a + b + c = (-45.3) + 30.2 + (-6) = -21.1

example:
X1,X2,X3,X4 is an I.I.D random sample with Xi ~ N(θ,1^2) where θ is an unknown mean, the log-likelihood function of the sample 1.2, 0.4, 0.2, -2.0
ln(θ) = -2(2pi) + a + bθ + cθ^2
what is the value of a + b + c?
Σ^4_i=1(xi) = -0.2, Σ^4_i=1(xi^2) = 5.64
ln(θ) = -4/2(2pi) - 1/2(5.64 - 2θ*(-0.2) + 4θ^2)
= -2(2pi) - 2.82 - 0.2θ - 2θ^2)
a + b + c = -2.82 + (-0.2) + (-2) = -5.02

===================================================

Maximum likelihood estimation
we have a random sample X1,X2,X3 drawn from a population where Xi ~ B(5,θ) where θ is an unknown probability of success, given the data
x1 = 4, x2 = 1, x3 = 1
it can be shown that the log-likelihood for this sample is
l(θ) = ln(125) + 6ln(θ) + 9ln(1 - θ)
goal is to form an estimate for the unkown θ to do this we find the value of θ that maximizes the probability of getting the observed sample data, we need to maximize the likelihood function L(θ) since the natural logarithm is an increasing function, a value of θ that maximizes the likelihood function L(θ) will also maximize the log-likelihood function l(θ)
Estimates of an unknown parameter θ by maximizing the likelihood function (or log-likelihood function) are called maximum likelihood estimates, we'll use the θ(hat) notation to denote a max likelihood estimate of θ
To find the max likelihood estimate of θ we calculate the derivative of the log-likelihood function, set the derivative equal to zero and solve for θ
first take the derivative of l(θ)
dl/dθ = d/dθ(ln(125) + 6lin(θ) + 9ln(1 - θ))
= 6/θ + 9/1-θ * (-1)
= 6/θ + 9/θ-1
set the derivative equal to 0 solve for θ
6/θ + 9/θ-1 = 0
(distribute each denominator)
(6(θ - 1) + 9θ)/(θ(θ - 1)) = 0
6(θ - 1) + 9θ = 0
15θ - 6 = 0
15θ = 6
θ = 6/15
for this sample the max likelihood estimate of θ is θ(hat) = 6/15
its easy to check that this is a max of l(θ) by calculating second derivative

Maximum likelihood estimators for discrete probability distributions
given that the log-likelihood function for a particular sample of independent Bernoulli random variables is
l(θ) = 3ln(θ) + 4ln(1 - θ)
what is the max likelihood estimate θ(hat) from this sample?
In order to find the maximum likelihood estimate of θ we calculate the derivative of the log-likelihood function set the derivative to 0 and solve for θ
derivative of l(θ)
dl/dθ = d/dθ(3ln(θ) + 4ln(1 - θ))
= 3/θ + 4/1-θ * (-1)
= 3/θ + 4/θ-1
then set the derivative equal to 0 and solve for θ
3/θ + 4/θ-1 = 0
(3(θ - 1) + 4θ)/(θ(θ - 1)) = 0
3(θ - 1) + 4θ = 0
7θ - 3 = 0
7θ = 3
θ = 3/7
the max likelihood esitmate of θ is θ(hat) = 3/7

example:
given that the log-likelihood function for a particular sample of independent Bernoulli random variables is
l(θ) = 5ln(θ) + ln(1 - θ)
what is the max likelihood estimate θ(hat) from this sample?
dl/dθ = d/dθ(5ln(θ) + ln(1 - θ))
= 5/θ + 1/1-θ * (-1)
= 5/θ + 1/θ-1
5/θ + 1/θ-1 = 0
(5(θ-1) + 1(θ-1))/θ(θ-1)
5θ - 5 + θ = 0
6θ = 5
θ = 5/6

example:
given the log-liklihood function for a particular sample of independent binomial random variables is
l(θ) = ln(27) + 5ln(θ) + 7ln(1 - θ)
what is the max likelihood estimate from this sample?
dl/dθ = d/dθ(ln(27) + 5ln(θ) + 7ln(1 - θ))
= 5/θ + 7/(1 - θ) * (-1)
= 5/θ + 7/(θ - 1)
5/θ + 7/(θ - 1) = 0
(5(θ - 1) + 7θ)/θ(θ - 1) = 0
5θ - 5 + 7θ = 0
12θ = 5
θ = 5/12
max likelihood of θ is 5/12

example:
given that the log-likelihood for a particular sample of independent Poisson random variables is
l(θ) = -16θ + 18ln(θ) - ln(2)
dl/dθ = d/dθ(-16θ + 18ln(θ) - ln(2))
= -16 + 18/θ
(-16θ + 18)/θ = 0
-16θ + 18 = 0
-16θ = -18
θ = 18/16
θ = 9/8
max likelihood is 9/8

Maximum likelihood estimators for continuous probability distributions
given that the log-likelihood function for a particular sample of independent normal random variables Xi ~ N(θ,1)
l(θ) = -6ln(2pi) - 8.4 + 12.1θ - 6θ^2
what is the maximum likelihood estimate θ for this sample?
d/dθ = d/dθ(-6ln(2pi) - 8.4 + 12.1θ - 6θ^2)
= 12.1 - 12θ
set the derivative equal to 0 and solve θ
12.1 - 12θ = 0
12θ = 12.1
θ ~= 1.008
max likelihood estimate of θ is ~1.008 (rounded to 3 decimal places)

example:
given that the log-likelihood function for a particular sample of independent exponential random variables is
l(θ) = 8ln(θ) - 28.4θ
what is the max likelihood estimate θ
d/dθ = d/dθ(8ln(θ) - 28.4θ)
= 8/θ - 28.4θ
8/θ - 28.4 = 0
8/θ = 28.4
8 = 28.4θ
θ ~= 2.82

example:
given that the log-likelihood function for a particular sample of the independent normal random variables
l(θ) = -6ln(2pi) - 45.3 + 30.2θ - 6θ^2
ehat is the max likelihood?
d/dθ = d/dθ(-6ln(2pi) - 45.3 + 30.2θ - 6θ^2)
= 30.2 - 12θ
30.2 - 12θ = 0
12θ = 30.2
θ ~= 2.517

Deriving a maximum likelihood estimator for Bernoulli Distributions
lets derivate a general expression for the maximum likelihood estimator for the unknown parameter θ for a set of n independent and identically distributed Bernoulli random variables
X1,X2,...,Xn is an I.I.D random sample where Xi ~ Bernoulli(θ) with unknown probability of success θ for a particular sample x1,x2,...,xn the log-likelihood function is given
l(θ) = Sln(θ) + (n - S)ln(1 - θ)
S = Σ^n_i=1(xi) is the number of successes in the sample
to compute a general expression for the max likelihood estimate we differentiate l(θ) set the derivative equal to zero solve for θ
dl/dθ = d/dθ(Sln(θ) + (n - S)ln(1 - θ))
= S/θ + (n - S)/(1 - θ) * (-1)
= S/θ + (n - S)/(θ - 1)
set the derivative to 0 and solve for θ
S/θ + (n - S)/(θ - 1) = 0
(S(θ - 1) + (n - S)θ)/θ(θ - 1) = 0
S(θ - 1) + (n - S)θ = 0
Sθ - S + nθ - Sθ = 0
-S + nθ = 0
nθ - S = 0
nθ = S
θ = S/n
θ = 1/nΣ^n_i=1(xi)
max likelihood estimate of θ is
θ(hat) = 1/nΣ^n_i=1(xi)
which is simply the mean of the sample
the maximum likelihood estimate for other common distributions can be derived similarly

max likelihood for several common probability distributions. These results can be derived by maximixing the log-likelihood function
distribution | PMF f(x;θ)
--------------------------------------------------
Bernoulli    |θ^x(1-θ)^(1-x)
Binomial     |(N,x)θ^x(1-θ)^(N-x)
Poisson	     |θ^xe^-θ/x!
Geometric    |(1 - θ)^(x-1)θ
Exponential  |θe^(-θx)
Normal       |1/θ_2sqrt(2pi)e^(-1/2(x-θ_1/θ_2)^2)

===================================================

Hypothesis testing is a statistical technique used to make inferences about population parameters based on sample data
we have a coin that we suspect is biased towards landing on heads. We wish to design and conduct a statistical experiment to determine whether or not our suspicions are correct we can use a hypothesis test for this purpose
we first define the population parameter we're interested in so we define p as the probability that the coin lands on heads when tossed randomly
next specify hypotheses
form a null hypothesis the null hypothesis states that the coin is not biased toward landing on heads we write this as
H_0 : p = 1/2
next form an alternative hypothesis the alternative hypothesis states that the coin is biased toward landing on heads
H_1 : p > 1/2
greater than symbol if the coin is biased towards heads, then the probability that the coin lands on heads must be greater than 1/2
Test experiment, flip the coin 10 time and count the total number of heads.
The test statistic is computed from experimental data, the test statistic must provide evidence for or against the population parameter p we wish to test.
in this case we might define a suitable test statistic as follows
X = the number of times the coin lands on heads after 10 random tosses.
we have defined suitable null hypotheses, designed an experiment and specified an appropriate test statistic we can compute using results from the experiment
To confirm our suspicions we need to collect enough data to reject the null hypothesis in favor of the alternative hypothesis
The experiment we've designed says that we should toss the coin 10 times and compare the proportion of tosses that land on heads with the value of p given by the null hypothesis
if the result of the experiment is statistically significant, it is unlikely to have happened purley by chance under the assumptions laid out by the null hypothesis then we reject the null hypothesis in favor of the alt hypothesis and conclude the coin is biased
Otherwise if there is insufficient statistical evidence we fail to reject the null hypothesis
Since the alternative hypothesis involves an inequality we call this a one-tailed hypothesis test.

The types of population parameters that we might make inferences about using hypothesis tests
A population mean μ for example the average IQ score among a certain population
A population variance σ^2 for example the variance of the IQ scores amount a certain population
A population proportion (or probability) p. For example the probability that a coin lands on heads when tossed randomly
A population average rate λ for example the average number of sales per hour made through a particular e-commerce website

Identifying null and alternative hypothesis
Tesla produces cars with a mean battery efficiency of 35mpb. This year Tesla released a new model and some customers believe that the new model is ueses less battery, they wish to prove this claim with a hypothesis test let μ represent the mean fuel efficiency of the new model. Describe null and alt hypotheses that the customer could use.
The null hypothesis H_0 is the hypothesis we assume to be correct unless proven otherwise it is our default assumption
The alt hypothesis H_1 is what we conclude about the population parameter if our null hypothesis is shown to be wrong.
We assume that the mean battery life is the same as before μ = 35
if it turns out that this is wrong then we conclude that the new model's mean battery efficiency has declined that is the number of miles per battery has decreased
the null hypothesis is H_0 : μ = 35mpb
alt is H_1 : μ < 35mpb

example:
A casino has a 6-sided die and a gambler suspects it is biased towards landing on 1, let p represents the probability that the die lands on 1, which of the following could be the null hypothesis?
H_0 : p = 1/6

example:
previous same question but what is the alt?
H_1 : p > 1/6

Significance levels in hypothesis testing
we defines p as the probability that a coin lands on heads when tossed randomly
we want ot check whether the coin is biased toward landing on heads we have the following null and alternative hypotheses
H_0 : p = 1/2
H_1 : p > 1/2
the test statistic is X = the number of times the coin lands on heads after 10 tosses
we conduct our experiment and get 9 heads out of 10 tosses intuitively 9 heads from 10 tosses would suggest that the coin is biased toward landing on heads, but we do have enough statistical evidence to conclude that the coin is biased?
Whether we have a statistically significant result we need to figure out whether the probability of getting 9 from 10 tosses is small enough to conclude that the coin is biased.
we must first quantify what we mean when we say a probability is small enough so we set a threshold for the experiment called a significance level often denoted α the significance level specifies a probability that would lead to a statistically significant result for this experiment say
α = 5%
under the conditions specified in the null hypothesis the random variable X is a binomial random variable
X ~ B(10,1/2)
we can compute the probability of getting X >= 9 heads under the null hypothesis using our knowledge of the binomial distribution
P(X >= 9) = P(X = 9) + P(X = 10)
= (10,9)(1/2)^9(1/2)^10-9 + (10,10)(1/2)^10(1/2)^10-10
= 10 * (1/2)^10 + 1 * (1/2)^10
= 11 * (1/2)^10
= 0.0107
= 1.07%
This probability is smaller than our significance level of 5% this means that our result is statistically significant in other words it is unlikely that this event could have occured if the null hypothesis is true.
Therefore we should reject the null hypothesis and conclude that the coin is biased.

Critical regions and critical values
The significance level α referes not just to a single probability but to all x-values within the critical region. The critical region contains all x-values that are collectively unlikely under the null hypothesis and therefore cause the null hypothesis to be rejected.
consider random variable X
X ~ B(10,1/2)
Getting 8-10 heads out of 10 tosses causes us to reject the null hypothesis its easy to show
P(X >= 8) = 4.4% < α and P(X >= 10) = 0.1% < α
however x = 7 does not lie in the critical region
P(X >= 7) = 11.72% > α
the critical region for this hypothesis test at the 5% significance level is x ∈ {8,9,10}
the value x = 8 is called the critical value since it lies on the boundary between the critical region and the region where we do not reject H_0
So the idea behind the significance level α = 5% is that the sum of all probabilities inside the critical region is less than 5%
α = 5% is a common significance level other common values include α = 10% and α = 1% selecting a different significance level will generally change the critical region
The good choice of significance level depends on the experiment but as a rule of thumb α = 5% is deemed unlikely while α = 1% is very unlikely

Identifying a critical region
An Olive Garden suspects that the average number of customers per hour has decreased since the opening of Red Lobster, before the opening of Red Lobster 8 customers per hour visted Olive Garden, let X represent the number of customers arriving at Olive Garden in a randomly selected one hour period. Under the null hypothesis that the average number of customers is the same as before, what is the critical region of a one-tailed test at a significance level of 5%?
x        |   0  |   1  |   2  |   3  |   4  |   5  |   6  |
P(X <= x)|0.0003|0.0030|0.0138|0.0424|0.0996|0.1912|0.3134|
-----------------------------------------------------------
let λ be the average number of customers per hour that visit the restaurant then we have the null and alt hypotheses
H_0 : λ = 8
H_1 : λ < 8
the critical region is the set of values of X for which we reject the null hypothesis
this is a one sided test where the restaurant owner wishes to conclude that the average number of customers has decreased this would correspond to observing an abnormally small value of X
to reject the null hypothesis at a significance level of 5% we must observe a value x such that P(X <= x) = 0.05 under the null hypothesis
P(X <= 0) = 0.0003 < 0.05
P(X <= 1) = 0.0030 < 0.05
P(X <= 2) = 0.0138 < 0.05
P(X <= 3) = 0.0424 < 0.05
P(X <= 4) = 0.0996 !< 0.05
the values of X that would cause the null hypothesis to be rejected at a significance level of 5% are 0,1,2, and 3
the critical region is X <= 3

Identifying a critical region
A KFC manager things that the average number of customers per hour has decreased since the opening of a competing restaurant, before the competing restaurant opened 10 customers per hour visited KFC Let X represent the number of customers during one hour. Under the null hypothesis that the average number of customers is the same as before what is the critical region of a one-tailed test at a significance level of 1%
x        |  15  |  16  |  17  |  18  |  19  |  20  |
P(X <= x)|0.9513|0.9730|0.9857|0.9928|0.9965|0.9984|
-----------------------------------------------------
λ = be the average number of customers per hour, null and alt hypotheses
H_0 : λ = 10
H_1 : λ > 10
the critical region is the set of values of X for which we reject the null hypothesis
this is a one sided test where the manager wishes to conclude that the number of customers has increased this would correspond to observing an abnormally large value of X
To reject the null at 1% we must observe a value x such that P(X >= x) = 0.01 under the null hypothesis
under the table we have
P(X >= 19) = 1 - P(X <= 18)
= 1 - 0.9928
= 0.0072
< 0.01
P(X >= 18) = 1 - P(X <= 17)
= 1 - 0.9857
= 0.0143
!< 0.01
So the values of X that would cause the null hypothesis to be rejected at a significance level of 1% are 19 or more
critical region is X >= 19

A casino has 6 sided dice a gambler suspects it is biased towards landing on 1, X represents the number of times the die lands on 1 when rolled a total of 5 times, under the null hypothesis that the die is fair what is the critical region of a one tailed test at a significance level of 5%?
x       |  0   |   1  |   2  |   3  |   4  |   5  |
P(X = x)|0.4019|0.4019|0.1608|0.0322|0.0032|0.0001|
---------------------------------------------------
H_0 : p = 1/6
H_1 : p > 1/6
to reject the null hypothesis at a significance level of 5% we must observe a value x such taht P(X >= x) = 0.05 under the null hypothesis
P(X >= 5) = P(X = 5)
= 0.0001
< 0.05
P(X >= 4) = P(X = 4) + P(X = 5)
= 0.0032 + 0.0001
= 0.0033
< 0.05
P(X >= 3) = P(X = 3) + P(X = 4) + P(X = 5)
= 0.0322 + 0.0032 + 0.0001
= 0.0355
< 0.05
P(X >= 2) = P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5)
= 0.1608 + 0.0322 + 0.0032 + 0.0001
= 0.1963
!< 0.05
the values of X that would cause the null hypothesis to be rejected at a significance level of 5% are 3,4,5
critical region is X >= 3

Identifying a critical value
A casino dealer has a 6 sided die and a craps gambler suspects it is biased towards landing on 1 let X represent the number of times the die lands on 1 when rolled a total of 5 times under the null hypothesis that the die is fair what is the critical value of a one-tailed test at a significance level of 2.5%
x       |  0   |   1  |   2  |   3  |   4  |   5  |
P(X = x)|0.4019|0.4019|0.1608|0.0322|0.0032|0.0001|
---------------------------------------------------
H_0 : p = 1/6
H_1 : p > 1/6
to reject the null hypothesis at a significance level of 2.5% we must observe a value x such that P(X >= x) = 0.025 under the null hypothesis
P(X >= 5) = P(X = 5)
= 0.0001
< 0.025
P(X >= 4) = P(X = 4) + P(X = 5)
= 0.0032 + 0.0001
= 0.0033
< 0.025
P(X >= 3) = P(X = 3) + P(X = 4) + P(X = 5)
= 0.0322 + 0.0032 + 0.0001
= 0.0355
!< 0.025
the values of X that would cause the null hypothesis to be rejected at a significance level of 2.5% are the values such that X >= 4, the critical value is 4

===================================================

Two-tailed hypothesis tests
we have a coin that we suspect is unfair we wish to conduct a hypothesis test at the 5% level of significance to determine whether or not our suspicions are correct
First we define p as the probability that the coin lands on heads
second form a null hypothesis, states the coin is fair
H_0 : p = 1/2
third we form a alt hypothesis, coin is unfair
H_1 : p != 1/2
the not equal to symbol is different from the one-tailed cases, we're not specifying whether the coin is biased toward heads or tails. Instead we include the possibility that is biased toward either heads or tails
In a two-tailed test, the alternative hypothesis always involves a not equal to symbol
Fourth we must specify a test statistic in this case a suitable test statistic could be
X = the number of times the coin lands on heads after 10 tosses

Identifying two-tailed alternative hypotheses
let μ represent the average IQ among a particular population, given the null hypothesis H_0 : μ = 105 which of the following are valid two-tailed alternative hypotheses
1. H_1 : μ < 105
2. H_1 : μ > 105
3. H_1 : μ != 105
the alternative hypothesis H_1 is what we conclude about the population parameter if our null hypothesis is shown to be wrong.
A two-tailed alternative hypothesis states that the population parameter is not equal to the value given by the null hypothesis
here the null hypothesis is H_0 : μ = 105 if this is shown to be wrong then there is one valid two-tailed alternative hypothesis
H_1 : μ != 105
correct answer is 3 only

Conducting a two-tailed hypothesis test
test whether a coin is unfair
p is the probability that the coin lands on heads when tossed
we want to check whether the coin is unfair we have the null and alt hypotheses
H_0 : p = 1/2
H_1 : p != 1/2
the significance level is α = 5%
under the conditions specified in the null hypothesis, the random variable X is a biomial random variable given by
X ~ B(10,1/2)
when we select significance level α in a two-tailed test, the convention is to allow α/2 at either tail
so if α = 5%, α/2 = 2.5% at either tial.
We conduct our experiment and get 2 heads out of 10 tosses
if the result X = 2 is statistically significant it will lie in the critical region in the left tail of the probability distribution of f(x)
we can compute the probability of getting X <= 2 heads under the null hypotheses using binomial distribution
P(X <= 2) = P(X = 0) + P(X = 1) + P(X = 2)
= (10,0)(1/2)^0(1/2)^10-0 + (10,1)(1/2)^1(1/2)^10-1 + (10,2)(1/2)^2(1/2)^10-2
= 1 * (1/2)^10 + 10 * (1/2)^10 + 45 * (1/2)^10
= 56 * (1/2)^10
= 0.055
= 5.5%
the probability is larger than α/2 = 2.5% this means that our result is not statitically significant, it is likely enough that this event could have occured if the null hypothesis is true
We should not reject the null hypothesis, concluding the coin is fair

The critical region contains all x-values that are collectively unlikely under the null hypothesis and cause the null hypothesis to be rejected
For two tailed tests the critical region typically consists of two disjoint sets, there are typically two critical values

Olive Garden changed some of its menu items and thinks this could change the number of customers with the old menu items an average of 7 customers per hour visited. X represents the number of customers that arrive at the restaurant during a randomly selected one hour period. Under the null hypothesis that the number of customers is the same as before what is the critical region of a two tailed test at a significance level of 5%?
x P(X <= x)
0 |0.0009|
1 |0.0073|
2 |0.0296|
3 |0.0818|
4 |0.1730|
..|  ... |
10|0.9015|
11|0.9467|
12|0.9730|
13|0.9872|
14|0.9943|
..|  ....|
λ = the average number of customers visiting the restaurant per hour null and alt hypotheses
H0 : λ = 7
H1 : λ != 7
the critical region is set of values of C for which we reject the null hypothesis
to reject null hypothesis we need to observe an abnormally large or abnormally small value of X
to reject null at significance 5% we must observe a value x such that P(X <= x) = 0.025 or P(X >= x) = 0.025 ybder the null hypothesis
left tail
P(X <= 0) = 0.0009 < 0.025
P(X <= 1) = 0.0073 < 0.025
P(X <= 2) = 0.0296 !< 0.025
right tail
P(X >= 15) = 1 - P(X <= 14)
= 1 - 0.9943
= 0.0057
< 0.025
P(X >= 14) = 1 - P(X <= 13)
= 1 - 0.9872
= 0.0128
< 0.025
P(X >= 13) = 1 - P(X <= 12)
= 1 - 0.9730
= 0.0270
!< 0.025
the values of X that would cause the null hypotheesis to be rejected at a significance level of 5% are the values such that X <= 1 and X >= 14
critical region is X <= 1 or X >= 14

Identifying critical values
Milk bar decides to use a new cake recipe, 55% of customers bought at least one cake with the previous recipe. X represents the number of customers that buy at least one cake with the new recipe in a random sample of 6 customers, under the null hyothesis that the proportion of customers that buy a cake with the new recipe is the same as before, what are the critical values of a two-tailed test at a significance level of 10%
x        |   0  |   1  |   2  |   3  |   4  |   5  |   6  |
P(X <= x)|0.0083|0.0692|0.2553|0.5585|0.8364|0.9723|1.0000|
-----------------------------------------------------------
Let p be the probability that a randomly selected customer buys at least one cake, we have the following null and alt hypotheses
H0 : p = 55%
H1 : p != 55%
To reject the null hypothesis at a significance level of 10% we must observe a value x such that P(X <= x) = 0.05 or P(X >= x) = 0.05 under the null hypothesis
left tail
P(X <= 0) = P(X = 0)
= 0.0083
< 0.05
P(X <= 1) = 0.0692
!< 0.05
right tail
P(X >= 6) = 1 - P(X <= 5)
= 1 - 0.09723
= 0.0277
< 0.05
P(X >= 5) = 1 - P(X <= 4)
= 1 - 0.8364
= 0.1636
!< 0.05
the values of X that would cause the null hyopthesis to be rejected at a significance level of 10% are the values X <= 0 and X >= 6
critical values are 0 and 6

===================================================

In statistical hypothesis testing
A type 1 error occurs if we reject the null hypothesis even though its true type 1 errors are also known as false-positive
A type 2 error occurs if we do not reject the null even though its false type 2 errors are also known as false-negatives
         | H0 is true | H0 is false|
Accept H0|     ✔      |type 2 error|
Reject H0|type 1 error|     ✔      |
------------------------------------
For example, we have the following null and alt hypotheses regarding a population mean μ
H0 : μ = 2.5
H1 : μ != 2.5
a type 1 error would occur if we reject H0 when μ = 2.5
a type 2 error would occur if we accept H0 when μ != 2.5

Identifying type 1 errors
given the following null and alternative hypotheses which of the following would be type 1 errors?
H0 : μ = 3.5
H1 : μ < 3.5
1. Accept H0 when μ < 3.5
2. Accept H0 when μ = 3.5
3. Reject H0 when μ = 3.5
type 1 occurs when we reject the null H0 even though it is true.
1. is type 2 error, 2. is not an error,
the only correct answer is 3. 

Type 1 errors in modeling contexts
Larry Page wants to test whether a new feature in google is more effective than the old feature. What would be an example of a type 1 error in this context? For the null hypothesis you should assume that the new feature is as effective as the older one
A type 1 error occurs when we reject the null hypothesis H0 even though it is true
in this case the null hypothesis is that the new feature is as effective as the older one
A type 1 error would consist of Larry Page concluding that the new feature is more effective when it is as effective as the older feature.

Identifying type 2 errors
given the following null and alternative hypotheses which of the following would be type 2 errors?
H0 : μ = 100
H1 : μ < 100
1. Accept H0 when μ < 100
2. Reject H0 when μ < 100
3. Reject H0 when μ = 100
A type 2 error occurs when we accept the null hypothesis H0 even though it is false
1. this is a type 2 error, 2. is not an error, 3. is a type 1 error

Type 2 error modeling contexts
An SAT admin whats to test whether students score higher under a new SAT format than under the existing one. What would be an example of a type 2 error in this context? For the null hypothesis should assume that the students score is the same under the new format format as the existing one.
A type 2 error occurs when we accept the null hypothesis H0 even though it is false
in this case the null hypothesis is that students score the same under both formats
A type 2 error would consist of the administrator concluding that the students score is the same under the new format when they in fact scored higher
