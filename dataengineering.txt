LAB workbooks in a different repo (just notes on lab here)
(using datagrip)

Lecture 1
Complex data types and Cumulation
--------------------------------------------

Dimensions are attributes of an entity (e.g. user's bday, fav food)
some may identify an entity (user's ID)
Dimensions can be slow changing (e.g. time dependent)
Dimensions can be fixed, like your bday, a phone manufacture

Knowing your customer for data (how is your data being used)
- Data analysts / data scientists
should be easy to query, this is probabily flat
- Other data engineers
should be compact and might be harder to query, nested types are ok
- ML models
depends on the model and how its trained
- Customers
should be very easy to interpret (good annotations)

OLTP vs OLAP vs Master data
OLTP (online transaction processing)
optimizes for low-latency, low-volume queries. (3rd normal form)
MySQL, Postegres, performing a lot of joins to get the data we want, looks at one user (already filtered to one entity)
OLAP (online analytical processing)
optimizes for large volume, GROUPBY queries, minimizes, JOINS
looks at the entire dataset or a subset 
Master data
optimizes for completeness of entity definitions, deduped
(sites in the middle of OLTP and OLAP)

OLTP & OLAP is a continuum
Production db snapshots -> master data -> OLAP cubes -> metrics
Master data should be one table with all the joins of the transactions so were not using 40 different joins on a query.
OLAP cubes: slice and dice (flatten the data, can have multiple rows per entity, data scientists/analystis like this) can easily do GROUP BY methods on this
metrics: distill the data from all tables

Cumulative Table design
Core Components consists of: 
- 2 dataframes (yesterday and today)
- FULL OUTER JOIN the two data frames together
- COALESCE values to keep everything around
- Hang onto all of history
Usage:
- Growth analytics @ Facebook (dim_all_users)
- state transition tracking
(if a user is active yesterday but not today they are considered "churned", but if not active yesterday but active today "resurrected", theres lots of transitions we can classify here from yesterday to today, another one is "new" modeling patterns)
some filtering can occur because the data is bigger each day (maybe users that haven't logged in 180 days/deleted users)
cumulative output of today will become yesterdays input
strengths:
- historical analysis without shuffle
- easy transition analysis
drawbacks:
- can only be backfilled sequentially
- handing PII data can be a mess since deleted/inactive users get carried forward

Compactness vs usability tradeoffs
Most usable tales:
- have no complex data types
- easily can be manipulated with WHERE and GROUP BY
Most compact tables (not human readable):
- compressed so small they can't be queired directly until decoded
(this can be useful because Network I/O is slow! AirBnB does this for pricing/availability sends down compact data and decodes on the app)
Middle-ground tables:
- use complex data types (array, map, struct), making querying trickier but also compacting more.

When would you use compact vs usability tables?
Most compact:
- online systems where latency and data volumns matter
Middle:
- upstream staging/master data where most consumers are data engineers
Most usable:
- when analytics is the main consumer (they are less technical)

Struct vs Array vs Map
Struct
- keys are rigidly defined (compression is good)
- values can be any type
Map
- keys are loosely defined (compression is ok)
- values all have to be same type
Array
- Ordinal
- List of values that all have to be the same type

Temporal Cardinality, explosions of dimensions
- when you add temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude
example:
airbnb has ~6 million listings, if we want to know the nightly pricing and available of each night for the next year thats 365 * 6 million or about ~2 billion nights
Should this be a dataset of:
- listing level with an array of nights?
- listing night level with 2 billion rows?
if you do the sorting, Parquet will keep these two about the same size
(encoding will remove duplicated rows)

Run length encoding can be very useful for duplicates (be careful spark may mix up ordering after a join), you could re-sort this but sorting should only be done once. We should instead explode out the data that different between rows.

LAB1
This will show the advantages of cumulative data, and how we can create queries/CTEs to high performant operations, and to easily/powerful way to aggregate the data.
--------------------------------------------

We need to dedup the table so the differences between rows are the season stats/year so we will make a new type (ignoring age):
SELECT * FROM player_seasons;
CREATE TYPE season_stats AS (
                                season INTEGER,
                                gp INTEGER,
                                pts REAL,
                                reb REAL,
                                ast REAL
                            );

now we can create a new table that should represent the data with our new type
CREATE TABLE players (
    player_name TEXT,
    height TEXT,
    college TEXT,
    country TEXT,
    draft_year TEXT,
    draft_round TEXT,
    draft_number TEXT,
    season_stats season_stats[],
    current_season INTEGER,
    PRIMARY KEY(player_name, current_season)
)
next we can create a CTE (common table expression) to combine the two tables where the player_name matches:

WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 1996
    )
SELECT * FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name


This becomes our "seed query", since our minimum season is 1996, so this is really NULL, but this won't be the case as we accumulate more and more.
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),

We can COALESCE if the values don't change
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name

After Coalescing we can cocatenate todays with yesterdays season stats:
CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULLL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        COALESCE(t.season, y.current_season + 1) as current_season

|| is concatenation
:: we can cast it to the season_stats type

WHEN t.season IS NOT NULLL THEN y.season_stats ||
this is so we don't keep adding data in case the player has been retired

ELSE y.season_stats
so we dont add a bunch of NULLs

use current season otherwise use yesterdays + 1
COALESCE(t.season, y.current_season + 1)

We can INSERT our new data into our players table:
INSERT INTO players
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 1995
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 1996
    )
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.country, y.country) AS country,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number,
        CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        COALESCE(t.season, y.current_season + 1) as current_season
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name;

We run this query and the main column to focus on is the season_stats:
first stat from A.C. Green:
{(1996,83,7.2,7.9,0.8)}
Next we increase WHERE current_season = 1995, and WHERE season = 1996 by one WHERE current_season = 1996, and WHERE season = 1997, and re-run query:
SELECT * FROM players WHERE current_season = 1997;
{(1996,83,7.2,7.9,0.8),(1997,82,7.3,8.1,1.5)}
We do this a few times until and current_season = 2000 season = 2001

SELECT * FROM players WHERE current_season = 2001
(example from A.C. Green)
{(1996,83,7.2,7.9,0.8),(1997,82,7.3,8.1,1.5),(1998,50,4.9,4.6,0.5),(1999,82,5,5.9,1),(2000,82,4.5,3.8,0.5)}

Theres a gap in Michael Jordan:
{(1996,82,29.6,5.9,4.3),(1997,82,28.7,5.8,3.5),(2001,60,22.9,5.7,5.2)}

You can do some powerful stuff with the new table we can view these as 3 seperate records for Michael Jordan (UNNEST):
SELECT player_name,
	UNNEST(season_stats)::season_stats AS season_stats
FROM players
WHERE current_season = 2001
AND player_name = 'Michael Jordan'

With UNNEST the sorting will stay intack

we can wrap this in a CTE and perform actions on the CTE
WITH unnested AS (
	... (above query)
)
This uses the unnested and puts them in there corresponding columns
SELECT player_name
	(season_stats::season_stats).*
FROM unnested

This allows us to go back and forth between viewing the data compact and unraveling and we don't need to use so many joins we can use the CTE we've created.

Next we create a scoring system:
CREATE TYPE scoring_class AS ENUM ('star', 'good', 'average', 'bad');
DROP players, and re-create it with two new fields:
scoring_class scoring_class,
years_since_last_season INTEGER,
We can add to our cumlative query:

Scoring (ELSE y.scoring_class it just pulls from previous year if none this year):
CASE
    WHEN t.season IS NOT NULL THEN
        CASE WHEN t.pts > 20 THEN 'star'
            WHEN t.pts > 15 THEN 'good'
            WHEN t.pts > 10 THEN 'average'
        ELSE 'bad'
    END::scoring_class
    ELSE y.scoring_class
END,

last season (how many years off a player has had like Michael Jordan in 2000)
CASE WHEN t.season IS NOT NULL THEN 0
	ELSE y.years_since_last_season + 1
END AS years_since_last_season

We can aggregrate some data say we want to know whose made the biggest improvement in pts from their first season to their last season:
SELECT
    player_name,
    (season_stats[1]::season_stats).pts AS first_season,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts AS latest_season
FROM players
WHERE current_season = 2001

SELECT
    player_name,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts/ 
    CASE WHEN (season_stats[1]::season_stats).pts = 0 THEN 1 ELSE (season_stats[1]::season_stats).pts END,
    latest_season
FROM players
WHERE current_season = 2001

COMPLETE DUMP of the queries ran (commented out some just to save in case I needed to run them again)

-- SELECT * FROM player_seasons;
-- CREATE TYPE season_stats AS (
--                                 season INTEGER,
--                                 gp INTEGER,
--                                 pts REAL,
--                                 reb REAL,
--                                 ast REAL
--                             )
-- CREATE TYPE scoring_class AS ENUM ('star', 'good', 'average', 'bad');
-- DROP TABLE players;
-- CREATE TABLE players (
--     player_name TEXT,
--     height TEXT,
--     college TEXT,
--     country TEXT,
--     draft_year TEXT,
--     draft_round TEXT,
--     draft_number TEXT,
--     season_stats season_stats[],
--     scoring_class scoring_class,
--     years_since_last_season INTEGER,
--     current_season INTEGER,
--     PRIMARY KEY(player_name, current_season)
-- );
-- SELECT MIN(season) FROM player_seasons;
INSERT INTO players
WITH yesterday AS (
    SELECT * FROM players
             WHERE current_season = 2000
),
    today AS (
        SELECT * FROM player_seasons
                 WHERE season = 2001
    )
SELECT
        COALESCE(t.player_name, y.player_name) AS player_name,
        COALESCE(t.height, y.height) AS height,
        COALESCE(t.college, y.college) AS college,
        COALESCE(t.country, y.country) AS country,
        COALESCE(t.draft_year, y.draft_year) AS draft_year,
        COALESCE(t.draft_round, y.draft_round) AS draft_round,
        COALESCE(t.draft_number, y.draft_number) AS draft_number,
        CASE WHEN y.season_stats IS NULL
            THEN ARRAY[ROW(
                    t.season,
                    t.gp,
                    t.pts,
                    t.reb,
                    t.ast
                )::season_stats]
        WHEN t.season IS NOT NULL THEN y.season_stats || ARRAY[ROW(
                t.season,
                t.gp,
                t.pts,
                t.reb,
                t.ast
            )::season_stats]
        ELSE y.season_stats
        END as season_stats,
        CASE
            WHEN t.season IS NOT NULL THEN
                CASE WHEN t.pts > 20 THEN 'star'
                    WHEN t.pts > 15 THEN 'good'
                    WHEN t.pts > 10 THEN 'average'
                ELSE 'bad'
            END::scoring_class
            ELSE y.scoring_class
        END,
        CASE WHEN t.season IS NOT NULL THEN 0
            ELSE y.years_since_last_season + 1
        END AS years_since_last_season,
        COALESCE(t.season, y.current_season + 1) as current_season
    FROM today t FULL OUTER JOIN yesterday y
    ON t.player_name = y.player_name;

-- SELECT player_name,
--        UNNEST(season_stats)::season_stats AS season_stats
-- FROM players
-- WHERE current_season = 2001
--   AND player_name = 'Michael Jordan'

-- SELECT * FROM players WHERE current_season = 2001;

SELECT
    player_name,
    (season_stats[CARDINALITY(season_stats)]::season_stats).pts/
    CASE WHEN (season_stats[1]::season_stats).pts = 0 THEN 1 ELSE (season_stats[1]::season_stats).pts END
FROM players
WHERE current_season = 2001
AND scoring_class = 'star'
-- ORDER BY 2 DESC

The final output to continue to Lab 2:


CREATE TABLE players (
    player_name TEXT,
    height TEXT,
    college TEXT,
    country TEXT,
    draft_year TEXT,
    draft_round TEXT,
    draft_number TEXT,
    season_stats season_stats[],
    scoring_class scoring_class,
    years_since_last_active INTEGER,
    current_season INTEGER,
    is_active BOOLEAN,
    PRIMARY KEY (player_name, current_season)
);

INSERT INTO players
WITH years AS (
    SELECT *
    from generate_series(1996, 2022) as season
), p AS (
    SELECT player_name , MIN(season) as first_season
    FROM player_seasons
    GROUP BY player_name
 ), players_and_seasons as (
         SELECT *
         FROM p
            JOIN years y ON p.first_season <= y.season
), windowed AS (
         SELECT
             ps.player_name, ps.season,
             array_remove(
                             array_agg(CASE
                                           WHEN p1.season is not null then
                                               cast(row(p1.season, p1.gp, p1.pts, p1.reb, p1.ast) AS season_stats)
                         end
                                      )
                             OVER (PARTITION by ps.player_name ORDER BY COALESCE(p1.season, ps.season))
                 ,null
             )
                 as seasons
         FROM players_and_seasons ps
                  LEFT JOIN player_seasons p1
                            ON ps.player_name = p1.player_name AND ps.season = p1.season
         ORDER BY ps.player_name, ps.season
     )
        ,static AS (
    SELECT player_name,
           max(height) AS height,
           max(college) AS college,
           max(country) AS country,
           max(draft_year) AS draft_year,
           max(draft_round) AS draft_round,
           max(draft_number) AS draft_number
    FROM player_seasons ps
    GROUP BY player_name
)

SELECT
    w.player_name,
    s.height,
    s.college,
    s.country,
    s.draft_year,
    s.draft_number,
    s.draft_round,
    seasons as season_stats
        ,CASE
             WHEN (seasons[cardinality(seasons)]).pts > 20 THEN 'star'
             WHEN (seasons[cardinality(seasons)]).pts > 15 THEN 'good'
             WHEN (seasons[cardinality(seasons)]).pts > 10 THEN 'average'
             else 'bad'
        end :: scoring_class as scorring_class
        ,w.season - (seasons[cardinality(seasons)]).season as years_since_last_season
        ,w.season as current_season
        ,(seasons[cardinality(seasons)]).season = w.season as is_active
FROM windowed w
         join static s
              on w.player_name = s.player_name;


Lecture 2
Slowly Changing Dimensions and Idempotency
--------------------------------------------

Idempotent:
denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself 

pipelines should product the same result
regardless of the day
regardless of how many times
regardless of the hour

If you have data that not idempotent the data will be inconsistent
(which is bad!)

What can make a pipeline not idempotent?
- INERT INTO without TRUNCATE
	use MERGE or INSERT OVERWRITE everytime!
- Using Start_date > without a corresponding end_date <
- Not using a full set of partition sensors
	pipeline might run when there is no/partial data
- Not using depends_on_past for cumulative pipelines
(INERT INTO without TRUNCATE if you run this twice you have twice the amount of data, you'll duplicate the data, youll VIOLATE a idemotent rule)
MERGE will notice that data matches and won't do anything
INSERT OVERWRITE just overwrites the data
If you have a date greater than yesterday and you run the pipline today fine but you run the pipline tomorrow you'll have an extra day of data etc. violates the when clause of idempotentency.
Not using a full set of partition sensors: could happen when you don't have all the data at that moment (i.e. runs at different times in different all data input might not be ready)
depends_on_past (sequencial processing). This happens when you need to use cumulative pipeline and cannot run parallel processing say because yesterdays data is not ready yet (backfill and production behavior of the pipeline should be the same)

More causes that could make a pipleline not idempotent
- Relying on the "latest" partition of a not properly modeled SCD table
	SCD (slowly changing dimensions) if you have a properly modeled SCD table and your backfilling (not in production) that is the only exception
- Relying on the "latest" partition of anything else
if you have a non idempotent pipeline that introduces a bug because it gives the incorrect data (inconsistent data) and you have a cumalitve pipeline that depends on that data then it just carrys those bugs forward everyday

The pains of not having idempotent pipelines
- Backfilling causes inconsistencies between the old and restated data
- Very hard to troubleshoot bugs
- Unit testing cannot replicate the production behavior
- silent failures

Should you model as Slowly Changing dimensions?
- what are the options here?
	latest snapshot
	daily/monthly/yearly
	SCD
SCD: a way to collapse daily snapshots, based on whether the data changed day over day.

Why do dimentions change?
- Someone decides they hate Windows and want MacOS now
- Someone decides to move to another country
- you're fav food over time
etc

How can you model dimensions that change?
- Singular snapshots
	Becareful since these are not idempotent
- Daily partitioned snapshots
- SCD types 1,2,3
Dont backfill data with only latest snapshot of dimensions, which might not be correct for the older values because you might need the old dimensions of the data and not the new version

Types of SCD
Type 0
	- Aren't actually slowly changing (eye color)
	(no temporal component, data doesn't change)
Type 1
	Only care about the latest value
	- NEVER use this type because it makes your pipelines not idempontent anymore
	(when your dealing with daily online transactions this might be okay, but not for a consumer such as for data analytics)
Type 2
	- You care about what the value was from start_date to end_date
	- Current values usually have either an end_date that is:
		NULL
		Far into the future (9999-12-31)
	- Hard to use
		Since theres more than 1 row per dimension, you need to be careful about filtering on time
	- The only type of SCD that is purley idempotent
Type 3
	- You only care about "original" and "current"
		Benfits: you only have 1 row per dimension
		Drawbacks: you lose the history in between original and current
		is this idempotent: partially (which means its not)

Which types are idempotent?
	- Type 0 and Type 2 and idempotent
		type 0 values are unchanging
		type 2 need to be careful how start_date and end_date is used
	- Type 1 is not idempotent
		if the data is backfilled you'll get the dimension as it is now not as it was then.
	- Type 2 isn't idempotent
		if you backfill with this dataset, its impossible to know when to pick original vs current

SCD2 (slowly changing dimensions type 2) Loading
	- Load the entire history in one query
		inefficient but nimble
		1 query and you're done
	- Incrementally load the data after the previous SCD is generated
		has the same "depends_on_past" constraint
		efficient but cumbersome

Its good to make your data small and effecient and you can spend a lot of time doing this but whats the value? Sometimes its important to work on new things!

Lab 2
Building SCD Type 2 (purely idempotent)
--------------------------------------------
we're going to be tracking in our scd table:
scoring_class, and scoring_class
current_season is last (you can think of it as a date partition)

CREATE TABLE players_scd (
    player_name TEXT,
    scoring_class scoring_class,
    is_active BOOLEAN,
    start_season INTEGER,
    end_season INTEGER,
    current_season INTEGER,
    PRIMARY KEY(player_name, start_season)
);

LAG() is a window function that lets us use the previous row
AS with_previous (
	SELECT
	    player_name,
	    current_season,
	    scoring_class,
	    is_active,
	    LAG(scoring_class, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_scoring_class,
	    LAG(is_active, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_is_active
	FROM players
)


This is a good way to look at data and compare players based on previous seasons (<> does not equal)
SELECT *,
        CASE
            WHEN scoring_class <> previous_scoring_class THEN 1
            ELSE 0
        END AS scoring_class_change_indicator,
        CASE
            WHEN is_active <> previous_is_active THEN 1
            ELSE 0
        END AS is_active_change_indicator
FROM with_previous

We can combine the two change columns from the above query into one to analyse the values better
with_indicators AS (
    SELECT *,
            CASE
                WHEN scoring_class <> previous_scoring_class THEN 1
                WHEN is_active <> previous_is_active THEN 1
                ELSE 0
            END AS change_indicator
    FROM with_previous
)

Next we can sum the rows with the change_indicator, so we can aggregate the data a little better like finding min/max on a season on a streak_identifier.
 with_streaks AS (
    SELECT *,
           SUM(change_indicator)
           OVER (PARTITION BY player_name ORDER BY current_season) AS streak_identifier
    FROM with_indicators
)

Now we can easily see changes in players over time (although some common practices don't include the streak_identifier in the SELECT clause, and have an ORDER BY player_name):
SELECT player_name,
       streak_identifier,
       is_active,
       scoring_class,
       MIN(current_season) AS start_season,
       MAX(current_season) AS end_season
FROM with_streaks
GROUP BY player_name, streak_identifier, is_active, scoring_class

then we can just wrap everything with an INSERT INTO player_scd, this is powerful cause we have 2 window functions and an aggregate over the whole data set. Most of the time this is enough, but can be prone to out of memory exceptions.

Complete Postegres dump from Lab 2:

-- CREATE TABLE players_scd (
--     player_name TEXT,
--     scoring_class scoring_class,
--     is_active BOOLEAN,
--     start_season INTEGER,
--     end_season INTEGER,
--     current_season INTEGER,
--     PRIMARY KEY(player_name, start_season)
-- );
--
-- SELECT player_name, scoring_class, is_active
-- FROM players
-- WHERE current_season = 2022;
-- DROP TABLE players_scd;
-- INSERT INTO players_scd
-- WITH with_previous AS (
--     SELECT
--         player_name,
--         current_season,
--         scoring_class,
--         is_active,
--         LAG(scoring_class, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_scoring_class,
--         LAG(is_active, 1) OVER (PARTITION BY player_name ORDER BY current_season) AS previous_is_active
--     FROM players
--     WHERE current_season <= 2021
-- ),
-- with_indicators AS (SELECT *,
--                            CASE
--                                WHEN scoring_class <> previous_scoring_class THEN 1
--                                WHEN is_active <> previous_is_active THEN 1
--                                ELSE 0
--                                END AS change_indicator
--                     FROM with_previous
-- ),with_streaks AS (
--     SELECT *,
--            SUM(change_indicator)
--            OVER (PARTITION BY player_name ORDER BY current_season) AS streak_identifier
--     FROM with_indicators
-- )
--
-- SELECT player_name,
--        scoring_class,
--        is_active,
--        MIN(current_season) AS start_season,
--        MAX(current_season) AS end_season,
--        2021 AS current_season
-- FROM with_streaks
-- GROUP BY player_name, streak_identifier, is_active, scoring_class
-- ORDER BY player_name, streak_identifier
-- SELECT * FROM players_scd;
--
-- CREATE TYPE scd_type AS (
--     scoring_class scoring_class,
--     is_active BOOLEAN,
--     start_season INTEGER,
--     end_season INTEGER
-- );
WITH last_season_scd AS (
    SELECT * FROM players_scd
    WHERE current_season = 2021
    AND end_season = 2021
), historical_scd AS (
    SELECT
        player_name,
        scoring_class,
        is_active,
        start_season,
        end_season
        FROM players_scd
    WHERE current_season = 2021
    AND end_season < 2021
), this_season_data AS (SELECT *
                        FROM players
                        WHERE current_season = 2022
-- SELECT * FROM last_season_scd;
), unchanged_records AS (
    SELECT
        ts.player_name,
        ts.scoring_class,
        ts.is_active,
        ls.start_season,
        ts.current_season as end_season
    FROM this_season_data ts
        JOIN last_season_scd ls
        ON ls.player_name = ts.player_name
            WHERE ts.scoring_class = ls.scoring_class
            AND ts.is_active = ls.is_active
), changed_records AS (
    SELECT
        ts.player_name,
        UNNEST(ARRAY[
            ROW(
                ls.scoring_class,
                ls.is_active,
                ls.start_season,
                ls.end_season
                )::scd_type,
            ROW(
                ts.scoring_class,
                ts.is_active,
                ts.current_season,
                ts.current_season
            )::scd_type
        ]) AS records
    FROM this_season_data ts
             LEFT JOIN last_season_scd ls
                  ON ls.player_name = ts.player_name
    WHERE (ts.scoring_class <> ls.scoring_class
        OR ts.is_active <> ls.is_active)
), unnested_changed_records AS (
    SELECT player_name,
           (records::scd_type).scoring_class,
           (records::scd_type).is_active,
           (records::scd_type).start_season,
           (records::scd_type).end_season
           FROM changed_records
), new_records AS (
    SELECT
        ts.player_name,
        ts.scoring_class,
        ts.is_active,
        ts.current_season AS start_season,
        ts.current_season AS end_season
    FROM this_season_data ts
    LEFT JOIN last_season_scd ls
        ON ts.player_name = ls.player_name
    WHERE ls.player_name IS NULL
)

SELECT * FROM historical_scd

UNION ALL

SELECT * FROM unchanged_records

UNION ALL

SELECT * FROM unnested_changed_records

UNION ALL

SELECT * FROM new_records

Lecture 3
Graph db & Additive dimensions
--------------------------------------------

What makes a dimension additive?
Additive dimensions mean that you don't "double count"
Age is additive
	- the population is equal to 20 + 30 + 40 year olds ...
Application interface is NOT additive
	- the number of active users != # of users on web + # of users on Android + # users on iphone
	(I can have two different types of phones!)
Counting drivers by cars is NOT additive
	The number of Honda drivers != # of Civic drivers + # of Odyssey driver + # of Accord driver
	(a driver can drive two different cars the same day)

The essential nature of additivity
	A dimension is additive over a specific window of time, iff the grain of data over that window can only ever be one value at a time

How does additivity help?
	You don't need to use COUNT(DISTINCT) on preaggregated dimensions
	Non-additive dimensions are usually non-additive with respect to COUNT aggregations but not SUM aggregations
can a user be two of these at the same time?

When should you use Enums?
	Great for low-to-medium cardinality (usually ~50 is limit)
	Country is where enums start to struggle

Why enums?
	built in data quality
	built in static fields
	built in documentation

Enumerations and subpartitions
	Enumerations make superb subpartitions:
		you have an exhaustive list
		they chunk up the big data problem into manageable pieces

Little book of enums is a pattern used in pipelines
if theres a lot of datasets (say 50) how can they all be managed? we can group them in a numerated group.
all Data quality checks
customized data quality checks on each partition
if you need a new source you just add another value to the enum
qualtiy goes up and you have documentation
The little book of enums is generated by an enumeration defined in Scala or Python and then a job that turns the list into a table with ~20-30 rows thats how you can share it between DQ-checks and source functions

What type of use cases is this enum pattern useful?
Whenever you have tons of sources mapping to a shared schema
	- unit economics (fees, coupons, credits, insurance, infrastructure cost, taxes, etc)
	- infrastructure graph (applications, dbs, servers, code bases, CI/CD jobs)
	- family of apps (oculus, instagram, facebook, messenger, etc)

How do you model data from disparate sources into a shared schema?
With a flexable schema
	Benifits:
		- you don't have to run ALTER TABLE commands
		- you can manage a lot more columns
		- your schemas don't have a ton of NULL columns
		- "Other_properties" column is great for rarely-used but needed columns
	Downsides:
		- compression is usually worse (especially if you use JSON)
		- readability, queryability

Graph data modeling is different
Shifting the focus on from what the data is to how is the data connected
it is a RELATIONSHIP focused not ENTITY focused
because of this you can do a very poor job at modeling the entities
	- usually the model looks like
		Identifier: STRING
		Type: STRING
		Properties: MAP<STRING, STRING>
(Graph data models usually have the same schema always)
The relationships are modeled a little bit more in depth:
usually the model looks like:
	- subject_identifier: STRING
	- subject_type: VERTEX_TYPE
	- object_identifier: STRING
	- object_type: VERTEX_TYPE
	- edge_type: EDGE_TYPE
	- properties: MAP<STRING, STRING>

Lab 3
Network Graph
--------------------------------------------
This will be from the same data we've been using (basketball)
the vertices, and edges table will almost always be the same schema.

CREATE TYPE vertex_type AS ENUM('player', 'team', 'game');

CREATE TABLE vertices (
    identifier TEXT,
    type vertex_type,
    properties JSON,
    PRIMARY KEY (identifier, type)
)

CREATE TYPE edge_type AS ENUM('plays_against', 'shares_team', 'plays_in', 'plays_on')

CREATE TABLE edges (
    subject_identifier TEXT,
    subject_type vertex_type,
    object_identifier TEXT,
    object_type vertex_type,
    edge_type edge_type,
    properties JSON,
    PRIMARY KEY (subject_identifier,
                subject_identifier,
                object_identifier,
                object_type,
                edge_type)
)
Sometimes theres an edge_id in the edges table it acts as:
	A global identifier for the edge (idenpendent of primary key)
	Facilitates easier querying and referencing
	referential integrity across related tables
	enables better graph traversal, optimization, and maintenance

Creating a vertex specific to games
INSERT INTO vertices
SELECT
    game_id AS identifier,
    'game'::vertex_type AS type,
    json_build_object(
        'pts_home', pts_home,
        'pts_away', pts_away,
        'winning_team', CASE WHEN home_team_wins = 1 THEN home_team_id ELSE visitor_team_id END
        ) as properties
FROM games;

ARRAY_AGG will collapse rows down by putting a multiple values in a single row usually with a GROUP BY
this gives us a list of teams a player is associated with

SELECT
    player_id AS identifier,
    MAX(player_name) AS player_name,
    COUNT(1) AS number_of_games,
    SUM(pts) AS total_points,
    ARRAY_AGG(DISTINCT team_id) AS teams
FROM game_details
GROUP BY player_id

A nice way to visualize everything, this might be how data is structured to form a JSON response:

INSERT INTO vertices
WITH players_agg AS (
    SELECT
        player_id AS identifier,
        MAX(player_name) AS player_name,
        COUNT(1) AS number_of_games,
        SUM(pts) AS total_points,
        ARRAY_AGG(DISTINCT team_id) AS teams
    FROM game_details
    GROUP BY player_id
)
SELECT
    identifier,
    'player'::vertex_type,
    json_build_object(
        'player_name', player_name,
        'number_of_games', number_of_games,
        'total_points', total_points,
        'teams', teams
    )
FROM players_agg 

Next we can create our next vertex for teams
INSERT INTO vertices
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams;

Since we have duplicated data in the teams, we need to make sure its cleaned up by de-duping the data:
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT * FROM teams_deduped WHERE row_num = 1;

so the above team insert becomes:
INSERT INTO vertices
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams_deduped
WHERE row_num = 1

We can view all of our verticies:
SELECT type, COUNT(1)
FROM vertices
GROUP BY 1

Next we can build our edges (here we de-dupe the player_id and game_id)
INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
)
SELECT
    player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    game_id AS object_identifier,
    'game'::vertex_type AS object_type,
    'plays_in'::edge_type AS edge_type,
    json_build_object(
        'start_position', start_position,
        'pts', pts,
        'team_id', team_id,
        'team_abbreviation', team_abbreviation
    ) as properties
FROM deduped
WHERE row_num = 1;

MAX(CAST(e.properties->>'pts' AS INTEGER)) casts this as an integer and gets the property out:
SELECT
    v.properties->>'player_name',
    MAX(e.properties->>'pts')
    FROM vertices v JOIN edges e
    ON e.subject_identifier = v.identifier
    AND e.subject_type = v.type
GROUP BY 1 

Creating a double edge on our vertex (a join on itself), this will show us 2 players playing with/against each other if on same team (with) or different (against):
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
), filtered AS (
    SELECT * FROM deduped
    WHERE row_num = 1
)
SELECT
    f1.player_name,
    f2.player_name,
    f1.team_abbreviation,
    f2.team_abbreviation
    FROM filtered f1
    JOIN filtered f2
    ON f1.game_id = f2.game_id
    AND f1.player_name <> f2.player_name

if we want to easily see teams with/against we can just use our enum and build a CASE clause:
CASE WHEN f1.team_abbreviation = f2.team_abbreviation
    THEN 'shares_team'::edge_type
    ELSE 'plays_against'::edge_type
END

We can now do some aggreations on the data:
COUNT(1) AS num_games,
SUM(f1.pts) AS left_points,
SUM(f2.pts) AS right_points

When creating network graphs we can end up with double edges (sometimes this is needed other times not) in our dataset we are comparing whos playing against who and we don't need to know if tim plays against bob AND bob plays against tim this is data that is redundant:
WHERE f1.player_id > f2.player_id

altogether in a CTE form:
, aggregated AS (
    SELECT
        f1.player_id,
        f1.player_name,
        f2.player_id,
        f2.player_name,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END,
        COUNT(1) AS num_games,
        SUM(f1.pts) AS left_points,
        SUM(f2.pts) AS right_points
    FROM filtered f1
             JOIN filtered f2
                  ON f1.game_id = f2.game_id
                      AND f1.player_name <> f2.player_name
    WHERE f1.player_id > f2.player_id
    GROUP BY
        f1.player_id,
        f1.player_name,
        f2.player_id,
        f2.player_name,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END
)

We have to be careful with data sometimes we can get a duplicate error when joining or aggregating, for instance a player can have the same id but different name (a player can change there name):
MAX(f1.player_name) AS subject_player_name
MAX(f2.player_name) AS object_player_name

Total numbers of points divided by games, we can get all different types of connections, and aggregate the data at a much higher level!
SELECT
    v.properties->>'player_name',
    e.object_identifier,
    CAST(v.properties->>'number_of_games' AS REAL)/
    CASE WHEN CAST(v.properties->>'total_points' AS REAL) = 0 THEN 1
            ELSE CAST(v.properties->>'total_points' AS REAL)
    END,
    e.properties->>'subject_points',
    e.properties->>'num_games'
    FROM vertices v JOIN edges e
    ON v.identifier = e.subject_identifier
    AND v.type = e.subject_type
WHERE e.object_type = 'player'::vertex_type


Complete Postegres dump from Lab 3:

CONSOLE 1:

CREATE TYPE vertex_type AS ENUM('player', 'team', 'game');

CREATE TABLE vertices (
    identifier TEXT,
    type vertex_type,
    properties JSON,
    PRIMARY KEY (identifier, type)
)

CREATE TYPE edge_type AS ENUM('plays_against', 'shares_team', 'plays_in', 'plays_on')

CREATE TABLE edges (
    subject_identifier TEXT,
    subject_type vertex_type,
    object_identifier TEXT,
    object_type vertex_type,
    edge_type edge_type,
    properties JSON,
    PRIMARY KEY (subject_identifier,
                subject_type,
                object_identifier,
                object_type,
                edge_type)
)

CONSOLE 2:

INSERT INTO vertices
SELECT
    game_id AS identifier,
    'game'::vertex_type AS type,
    json_build_object(
        'pts_home', pts_home,
        'pts_away', pts_away,
        'winning_team', CASE WHEN home_team_wins = 1 THEN home_team_id ELSE visitor_team_id END
        ) as properties
FROM games;

INSERT INTO vertices
WITH players_agg AS (
    SELECT
        player_id AS identifier,
        MAX(player_name) AS player_name,
        COUNT(1) AS number_of_games,
        SUM(pts) AS total_points,
        ARRAY_AGG(DISTINCT team_id) AS teams
    FROM game_details
    GROUP BY player_id
)
SELECT
    identifier,
    'player'::vertex_type,
    json_build_object(
        'player_name', player_name,
        'number_of_games', number_of_games,
        'total_points', total_points,
        'teams', teams
    )
FROM players_agg;

SELECT * FROM teams;

INSERT INTO vertices
WITH teams_deduped AS (
    SELECT *, ROW_NUMBER() OVER(PARTITION BY team_id) AS row_num
    FROM teams
)
SELECT
    team_id AS identifier,
    'team'::vertex_type AS type,
    json_build_object(
        'abbreviation', abbreviation,
        'nickname', nickname,
        'city', city,
        'arena', arena,
        'year_founded', yearfounded
    )
FROM teams_deduped
WHERE row_num = 1

CONSOLE 3:

SELECT
    v.properties->>'player_name',
    e.object_identifier,
    CAST(v.properties->>'number_of_games' AS REAL)/
    CASE WHEN CAST(v.properties->>'total_points' AS REAL) = 0 THEN 1
            ELSE CAST(v.properties->>'total_points' AS REAL)
    END,
    e.properties->>'subject_points',
    e.properties->>'num_games'
    FROM vertices v JOIN edges e
    ON v.identifier = e.subject_identifier
    AND v.type = e.subject_type
WHERE e.object_type = 'player'::vertex_type

SELECT
    v.properties->>'player_name',
    MAX(CAST(e.properties->>'pts' AS INTEGER))
    FROM vertices v JOIN edges e
    ON e.subject_identifier = v.identifier
    AND e.subject_type = v.type
GROUP BY 1
ORDER BY 2 DESC

INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
), filtered AS (
    SELECT * FROM deduped
    WHERE row_num = 1
), aggregated AS (
    SELECT
        f1.player_id AS subject_player_id,
        f2.player_id AS object_player_id,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
        END AS edge_type,
        MAX(f1.player_name) AS subject_player_name,
        MAX(f2.player_name) AS object_player_name,
        COUNT(1) AS num_games,
        SUM(f1.pts) AS subject_points,
        SUM(f2.pts) AS object_points
    FROM filtered f1
             JOIN filtered f2
                  ON f1.game_id = f2.game_id
                      AND f1.player_name <> f2.player_name
    WHERE f1.player_id > f2.player_id
    GROUP BY
        f1.player_id,
        f2.player_id,
        CASE WHEN f1.team_abbreviation = f2.team_abbreviation
                 THEN 'shares_team'::edge_type
             ELSE 'plays_against'::edge_type
            END
)
SELECT
    subject_player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    object_player_id AS object_identifier,
    'player'::vertex_type AS object_type,
    edge_type AS edge_type,
    json_build_object(
        'num_games', num_games,
        'subject_points', subject_points,
        'object_points', object_points
    )
FROM aggregated

INSERT INTO edges
WITH deduped AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, game_id) AS row_num
    FROM game_details
)
SELECT
    player_id AS subject_identifier,
    'player'::vertex_type AS subject_type,
    game_id AS object_identifier,
    'game'::vertex_type AS object_type,
    'plays_in'::edge_type AS edge_type,
    json_build_object(
        'start_position', start_position,
        'pts', pts,
        'team_id', team_id,
        'team_abbreviation', team_abbreviation
    ) as properties
FROM deduped
WHERE row_num = 1;

SELECT type, COUNT(1)
FROM vertices
GROUP BY 1
